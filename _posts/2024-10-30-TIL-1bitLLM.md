---
layout: post
title: "ðŸ’¡ TIL: 1.58-bit LLMs Match Full Performance @ 98.6% Energy Reduction"
date: 2024-10-30
tags: [til, llm, performance, energy-reduction]
---
<!--more-->

## Introduction
Back in February 2024, a preprint titled [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764) was released. Lots of people picked up on it, simply search for _1.58-bits_ on YouTube for instance, however it escaped me due to [a busy time at work](https://xcancel.com/AdamMGrant/status/1851348990589354464). It was only when I stumbled across this preprint again recently, that I realised what a fantastic idea it is to [substitute multiplication with addition or subtraction](https://www.youtube.com/watch?v=wCDGiys-nLA). 

## Contributions
The TL;DR is that all LLM weights can be ternary i.e. {-1, 0, 1}. Ternary weights are 1.58-bits. Activations are 8-bits. This highly quantised model matches full-precision performance at 3B parameter scale.  
This highly quantised model exhibits 2.71x faster inference, 2.55x lower memory usage at 3B scale, 71.4x lower energy consumption for matrix multiplication operations. Benefits increase with model scale e.g. 4.1x speed-up at 70B parameters, 8.9x higher throughput, 11x larger batch size. 

## What Does 1.58-bits Mean?
Carnegie Mellon University has a great reference on the [basics of Information theory](https://www.cs.cmu.edu/~dst/Tutorials/Info-Theory/). Learning how to measure information content for a ternary system {-1, 0, 1}, we notice that:  
Each value {-1, 0, 1} has an equal probability $P = \frac{1}{3}$ for each state. 
The information content is $-(P \log_2{P})$ summed over all states 

$$
-(\frac{1}{3} \log_2(\frac{1}{3}) + \frac{1}{3} \log_2(\frac{1}{3}) + \frac{1}{3} \log_2(\frac{1}{3}))  
= -(3 Ã— (\frac{1}{3} \log_2(\frac{1}{3})))  
= -\log_2(\frac{1}{3})
\approx 1.58496... bits  
$$  

## Conclusion
LLMs can achieve comparable performance to full-precision models while using only three weight values {-1, 0, 1}, achieving up to 71.4x lower energy consumption for matrix operations and 3.55x lower memory usage at 3B scale. This breakthrough suggests a new direction for efficient LLM deployment, particularly promising for edge devices and mobile applications, while also opening opportunities for specialized hardware optimized for 1-bit operations.
