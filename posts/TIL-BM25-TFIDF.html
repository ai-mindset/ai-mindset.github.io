<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ðŸ’¡ TIL: TF-IDF vs BM25 - Just-in-Time Learning</title>
  <link rel="stylesheet" href="/style.css">
</head>
<body>
  <header>
    <div class="container">
      <h1><a href="/">Just-in-Time Learning</a></h1>
    </div>
  </header>

  <main class="post-container">
    <article class="post">
      <header class="post-header">
        <h1>ðŸ’¡ TIL: TF-IDF vs BM25</h1>
        <span class="post-date">November 20, 2024</span>
        <div class="post-tags">
          <span class="post-tag">til</span><span class="post-tag">tf-idf</span><span class="post-tag">bm25</span><span class="post-tag">text-ranking</span><span class="post-tag">nlp</span>
        </div>
      </header>
      
      <div class="post-content">
        <p><strong>TL;DR:</strong> While TF-IDF ranks documents based on term frequency weighted by rarity across a corpus, BM25 improves upon this foundation by adding term frequency saturation and document length normalisation. Choose TF-IDF for simpler tasks with uniformly-sized documents, but prefer BM25 for search engines handling varied document lengths where its sophisticated algorithm delivers superior retrieval performance despite requiring more complex implementation and parameter tuning.</p>
<!--more-->

<h2>Introduction</h2>
<p>When building search engines or document retrieval systems, two algorithms often come up: <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a> and <a href="https://en.wikipedia.org/wiki/Okapi_BM25">Okapi BM25</a>. While both aim to rank documents by relevance, they differ significantly in their approach and effectiveness. Today, I learned the key differences between these techniques and when to use each one.</p>
<h2>TF-IDF: The Classic Approach</h2>
<p>TF-IDF (Term Frequency-Inverse Document Frequency) ranks documents based on how frequently terms appear in a document, weighted by how rare those terms are across all documents. It&#39;s straightforward: if a word appears often in a document but is rare across the corpus, it&#39;s probably important[^1]. $idf$ is calculated as follows:</p>
<p>$idf(t) = \log\frac{N}{n_t}$</p>
<p>where:\ $N$ : Total number of documents in corpus\ $n_t$ : Number of documents containing term $t$</p>
<p>TF-IDF is derived by the following calculation:</p>
<p>$TF\text{-}IDF(t,d) = tf(t,d) \cdot idf(t)$</p>
<p>where:\ $tf(t,d)$ : Frequency of term $t$ in document $d$</p>
<h3>Advantages</h3>
<ul>
<li>Simple to understand and implement</li>
<li>Computationally efficient</li>
<li>Works well for documents of similar length</li>
<li>Great for basic document classification</li>
</ul>
<h3>Disadvantages</h3>
<ul>
<li>No term frequency saturation (more occurrences always mean higher scores)</li>
<li>Doesn&#39;t handle varying document lengths well</li>
<li>Can overemphasise common terms in long documents</li>
</ul>
<h2>BM25: The Modern Evolution</h2>
<p>BM25 (Best Match 25) builds upon TF-IDF&#39;s foundation but adds two crucial improvements: term frequency saturation and document length normalisation. Note how the $idf_{BM25}$ component differs from TF-IDF&#39;s:</p>
<p>$idf_{BM25}(t) = \log\frac{N- n_t + 0.5}{n_t + 0.5}$</p>
<p>This modification provides smoother IDF weights and better handles edge cases.</p>
<p>$BM25(t,d) = \frac{tf(t,d) \cdot (k_1 + 1)}{tf(t,d) + k_1 \cdot (1- b + b \cdot \frac{|d|}{avgdl})} \cdot idf_{BM25}$</p>
<p>where:\ $tf(t,d)$ : Frequency of term $t$ in document $d$\ $|d|$ : Length of document $d$ (in words)\ $avgdl$ : Average document length in corpus\ $k_1$ : Term frequency saturation parameter (typically 1.2-2.0)\ $b$ : Length normalisation parameter (typically 0.75)\ $N$ : Total number of documents in corpus\ $n_t$ : Number of documents containing term $t$</p>
<h3>Advantages</h3>
<ul>
<li>Better handles varying document lengths</li>
<li>Prevents term frequency from dominating scores</li>
<li>More nuanced relevance rankings</li>
<li>Industry standard for search engines</li>
</ul>
<h3>Disadvantages</h3>
<ul>
<li>More complex implementation</li>
<li>Requires parameter tuning</li>
<li>Slightly higher computational cost</li>
<li>Less interpretable than TF-IDF</li>
</ul>
<h2>Which to Choose?</h2>
<h3>Choose TF-IDF when:</h3>
<ul>
<li>Building basic document classification systems</li>
<li>Working with uniformly-sized documents</li>
<li>Needing interpretable results</li>
<li>Prioritising implementation simplicity</li>
</ul>
<h3>Choose BM25 when:</h3>
<ul>
<li>Building a search engine</li>
<li>Handling documents of varying lengths</li>
<li>Requiring state-of-the-art retrieval performance</li>
<li>Working with user queries</li>
</ul>
<h2>Conclusion</h2>
<p>While TF-IDF remains valuable for simpler tasks and educational purposes, BM25 is generally superior for serious search applications. The choice between them often comes down to the trade-off between simplicity and sophistication. For modern search engines, BM25 is the clear winner, but TF-IDF&#39;s simplicity makes it perfect for learning and basic applications.</p>
<p>Remember: the best algorithm is the one that meets your specific needs. Don&#39;t automatically reach for BM25 just because it&#39;s more advanced â€“ sometimes, simpler is better.</p>
<p>[^1]: This is why TF-IDF is effective at identifying characteristic terms in     documents. It automatically downweights common words like &quot;the&quot;, &quot;and&quot;, &quot;is&quot;     while highlighting distinctive terms that appear frequently in specific     documents.</p>

      </div>
    </article>
  </main>

  <footer>
    <div class="container">
      <p>Created with <a href="https://github.com/ai-mindset/init.vim">Neovim</a>, using <a href="https://ai-mindset.github.io/dialogue-engineering">AI</a> to help process and curate content âœ¨</p>
    </div>
  </footer>

  <script src="/script.js"></script>
</body>
</html>