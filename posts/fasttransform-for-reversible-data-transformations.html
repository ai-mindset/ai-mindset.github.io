<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>⏪ Making Data Transformations Reversible with fasttransform - Just-in-Time Learning</title>
  <link rel="stylesheet" href="/style.css">
</head>
<body>
  <header>
    <div class="container">
      <h1><a href="/">Just-in-Time Learning</a></h1>
    </div>
  </header>

  <main class="post-container">
    <article class="post">
      <header class="post-header">
        <h1>⏪ Making Data Transformations Reversible with fasttransform</h1>
        <span class="post-date">March 22, 2025</span>
        <div class="post-tags">
          <span class="post-tag">machine-learning</span><span class="post-tag">data-processing</span><span class="post-tag">fast-ai</span><span class="post-tag">python</span><span class="post-tag">data-science</span><span class="post-tag">optimisation</span><span class="post-tag">best-practices</span><span class="post-tag">interpretability</span>
        </div>
      </header>
      
      <div class="post-content">
        <p><strong>TL;DR:</strong> Fast.ai&#39;s fasttransform library makes machine learning data pipelines reversible by pairing each transformation with its inverse, enabling visualisation of transformed data for debugging and utilising multiple dispatch to handle different data types intelligently - crucial for understanding model behaviour and identifying spurious correlations. </p>
<!--more-->

<h2 id="introduction">Introduction</h2>
<p>Machine learning practitioners face a common problem: after applying multiple transformations to prepare data for training, it becomes difficult to visualise what the model actually sees. This visualisation gap makes debugging challenging and often leads to missing critical insights about model behaviour.<br>For example, consider a model built to distinguish wolves from huskies that performs poorly on certain images. Without the ability to easily inspect how transformations affect the input data, one might miss that the model is actually detecting snow (common in wolf photos) rather than the animals themselves.<br>Fast.ai&#39;s solution to this problem is <a href="https://github.com/AnswerDotAI/fasttransform">fasttransform</a>[^1], a library that ensures any transformation applied to data can be easily reversed. Let&#39;s explore how it works and why it matters.</p>
<h2 id="reversible-pipelines-made-simple">Reversible Pipelines Made Simple</h2>
<h3 id="the-problem-with-one-way-transforms">The Problem with One-Way Transforms</h3>
<p>Traditional data transformation pipelines in libraries like PyTorch are one-way streets. Consider this simple example of normalising an image:</p>
<pre><code class="language-python">from torchvision import transforms as T
transforms_pt = T.Compose([
    T.Resize(256),
    T.CenterCrop(224),
    T.ToTensor(),
    T.Normalize(*imagenet_stats)
])

img = Image.open(&quot;husky.jpeg&quot;)
img_transformed = transforms_pt(img)
</code></pre>
<p>Attempting to visualise <code>img_transformed</code> results in a mess of pixel values outside the displayable range. To see what the model sees, one needs to manually write an inverse transform function:</p>
<pre><code class="language-python">def decode_pt(tensor, mean, std):
    out = tensor.clone()
    for t, m, s in zip(out, mean, std): t.mul_(s).add_(m)
    out = out.mul(255).clamp(0, 255).byte()
    return out
</code></pre>
<p>This is tedious and error-prone, especially as your transformation pipeline grows more complex.</p>
<h3 id="an-elegant-solution">An Elegant Solution</h3>
<p>fasttransform takes a fundamentally different approach by pairing each transformation with its inverse. Here&#39;s the same pipeline using fasttransform:</p>
<pre><code class="language-python">from fastai.vision.all import *

transforms_ft = Pipeline([
   PILImage.create,
   Resize(256, method=&quot;squish&quot;),
   Resize(224, method=&quot;crop&quot;),
   ToTensor(),
   IntToFloatTensor(),
   Normalize.from_stats(*imagenet_stats)
])

# Transform our image
fpath = Path(&quot;./huskies_vs_wolves/train/husky/husky_0.jpeg&quot;)
img_transformed = transforms_ft(fpath)
# To reverse the transformations:
img_decoded = transforms_ft.decode(img_transformed)
</code></pre>
<p>The magic lies in how each transform defines both forward and reverse operations:</p>
<pre><code class="language-python">class Normalize(Transform):
    def __init__(self, mean=None, std=None):
        self.mean = mean
        self.std = std
        
    def encodes(self, x): return (x-self.mean) / self.std  # forward transform
    def decodes(self, x): return x*self.std + self.mean    # inverse transform
</code></pre>
<p>By defining both <code>encodes</code> and <code>decodes</code> methods, fasttransform automatically knows how to reverse your transformations. This is particularly valuable when working with fast.ai v2, where this kind of visualisation capability is built directly into core functions like <code>show_batch</code> and <code>show_results</code>.</p>
<h3 id="multiple-dispatch-the-secret-sauce">Multiple Dispatch: The Secret Sauce</h3>
<p>Another powerful feature of fasttransform is how it handles different types of data. Using a concept called <a href="https://www.youtube.com/watch?v=kc9HwsxE1OY">multiple dispatch</a>[^2], transformations can apply differently based on the type of data they receive.</p>
<p>This becomes particularly valuable when dealing with images and their labels, allowing a single pipeline to handle both:</p>
<pre><code class="language-python"># Function that loads both image and its label
def load_img_and_label(fp): return PILImage.create(fp), parent_label(fp)

transforms_ft = Pipeline([
   load_img_and_label,  # Loads both image and label as a tuple
   Resize(256, method=&quot;squish&quot;),
   Resize(224, method=&quot;crop&quot;),
   ToTensor(),
   IntToFloatTensor(),
   Normalize.from_stats(*imagenet_stats)
])
</code></pre>
<p>The pipeline intelligently applies each transform only to the appropriate data types, eliminating the need for separate transformation pipelines.</p>
<h3 id="connections-to-julias-multiple-dispatch">Connections to Julia&#39;s Multiple Dispatch</h3>
<p>Interestingly, the concept of multiple dispatch that fasttransform leverages is a core feature of the Julia programming language. In Julia, which method of a function gets called depends on the types of all arguments, not just the first one (as in traditional object-oriented programming).<br>As explained in Julia&#39;s documentation: &quot;<em>Using all of a function&#39;s arguments to choose which method should be invoked, rather than just the first, is known as multiple dispatch. Multiple dispatch is particularly useful for mathematical code, where it makes little sense to artificially deem the operations to &#39;belong&#39; to one argument more than any of the others</em>&quot;.<br>The connection to Julia is particularly illuminating, as it demonstrates how concepts from one language can inspire powerful design patterns in another. Just as Julia&#39;s multiple dispatch enables elegant mathematical code, fasttransform&#39;s implementation of this concept allows for cleaner, more intuitive data pipelines in Python.</p>
<h2 id="conclusion">Conclusion</h2>
<p>fasttransform represents a significant step forward in making machine learning workflows more intuitive and debugging more accessible. By making transformations reversible through paired encode/decode methods and leveraging multiple dispatch to handle different data types intelligently, it solves two fundamental problems in data processing pipelines: the inability to easily reverse transformations to inspect data, and the need for separate transformation pipelines for different types of data.<br>The ability to easily visualise transformed data isn&#39;t just convenient -it&#39;s essential for understanding model behaviour and catching issues like the wolf/husky example, where models learn spurious correlations rather than intended features.<br>As machine learning systems grow more complex, tools like fasttransform that improve transparency and the ability to debug become increasingly valuable. Whether working with images, text, time series, or other data types, being able to see what a model sees provides critical insights that might otherwise be missed.<br>Returning to our wolf/husky example, the ability to easily visualise transformed data allows researchers to immediately identify that their model is learning to detect snow backgrounds rather than animal features -a crucial insight for building more robust models.<br>Those interested in trying fasttransform can install it with <code>pip install fasttransform</code> and check out the <a href="https://github.com/AnswerDotAI/fasttransform">official fasttransform documentation</a> for more examples and detailed API references. The library offers these capabilities with minimal performance overhead, as the paired transformation approach adds negligible computational cost while providing significant benefits for debugging and understanding model behaviour.</p>
<hr>
<p>[^1]: Rens Dimmendaal, Hamel Husain, &amp; Jeremy Howard. &quot;<a href="https://www.fast.ai/posts/2025-02-20-fasttransform.html">fasttransform: Reversible Pipelines Made Simple</a>&quot; fast.ai blog, February 20, 2025.
[^2]: &quot;<a href="https://docs.julialang.org/en/v1/manual/methods/">Methods · The Julia Language</a>&quot; Julia Documentation, docs.julialang.org.</p>

      </div>
    </article>
  </main>

  <footer>
    <div class="container">
      <p>Created with <a href="https://github.com/ai-mindset/init.vim">Neovim</a>, using <a href="https://ai-mindset.github.io/dialogue-engineering">AI</a> to help process and curate content ✨</p>
    </div>
  </footer>

  <script src="/script.js"></script>
</body>
</html>