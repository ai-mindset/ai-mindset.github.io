<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ðŸ’¡ TIL: Test-Driven Development Is Key to Better LLM System Prompts - Just-in-Time Learning</title>
  <link rel="stylesheet" href="/style.css">
</head>
<body>
  <header>
    <div class="container">
      <h1><a href="/">Just-in-Time Learning</a></h1>
    </div>
  </header>

  <main class="post-container">
    <article class="post">
      <header class="post-header">
        <h1>ðŸ’¡ TIL: Test-Driven Development Is Key to Better LLM System Prompts</h1>
        <span class="post-date">January 2, 2025</span>
        <div class="post-tags">
          <span class="post-tag">ai</span><span class="post-tag">llm</span><span class="post-tag">til</span><span class="post-tag">prompt-engineering</span><span class="post-tag">testing</span><span class="post-tag">best-practices</span><span class="post-tag">evaluation</span><span class="post-tag">machine-learning</span><span class="post-tag">system-prompts</span>
        </div>
      </header>
      
      <div class="post-content">
        <p><strong>TL;DR:</strong> Anthropic&#39;s approach to system prompt development parallels test-driven development-first creating test cases where default model behaviour fails, then developing prompts that pass these tests, followed by iterative refinement-highlighting how robust automated evaluation is not merely a quality check but the foundation for building reliable LLM applications.</p>
<!--more-->

<h2 id="introduction">Introduction</h2>
<p>2024 has made clear that writing good automated evaluations for LLM-powered systems is the most critical skill for building useful applications. This insight parallels Anthropic&#39;s internal approach to system prompt development. As usual, Simon Willison&#39;s <a href="https://simonwillison.net/2024/Dec/31/llms-in-2024/#evals-really-matter">recent insightful 2024 LLM overview</a> was a treasure trove. One item I picked up on was evaluating system prompts using a test-driven approach.</p>
<h2 id="the-evaluation-first-approach">The Evaluation-First Approach</h2>
<p><a href="https://askell.io/">Amanda Askell</a>, leading fine-tuning at Anthropic, <a href="https://xcancel.com/amandaaskell/status/1866207266761760812">outlines a test-driven process</a> for system prompts:</p>
<ol>
<li>Create a test set of messages where the model&#39;s default behaviour fails to</li>
<li>Develop a system prompt that passes these tests</li>
<li>Identify cases where the system prompt is misapplied and refine it</li>
<li>Expand the test set and repeat
meet requirements</li>
</ol>
<p>This methodology&#39;s importance extends beyond prompt engineering. Companies with strong evaluation suites can adopt new models faster and build more reliable features than competitors. As <a href="https://xcancel.com/cramforce/status/1860436022347075667">Vercel&#39;s experience demonstrates</a>, moving from complex prompt protection to robust testing enables rapid iteration and development.</p>
<h2 id="conclusion">Conclusion</h2>
<p>While everyone acknowledges evals&#39; importance, implementing them effectively remains challenging. The key insight is clear: robust automated evaluation isn&#39;t just a quality check, it&#39;s the foundation for building reliable LLM-powered systems.</p>

      </div>
    </article>
  </main>

  <footer>
    <div class="container">
      <p>Created with <a href="https://github.com/ai-mindset/init.vim">Neovim</a>, using <a href="https://ai-mindset.github.io/dialogue-engineering">AI</a> to help process and curate content âœ¨</p>
    </div>
  </footer>

  <script src="/script.js"></script>
</body>
</html>