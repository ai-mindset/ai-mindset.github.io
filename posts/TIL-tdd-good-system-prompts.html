<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>üí° TIL: Test-Driven Development Is Key to Better LLM System Prompts - Just-in-Time Learning</title>
  <link rel="stylesheet" href="/style.css">
</head>
<body>
  <header>
    <div class="container">
      <h1>Just-in-Time Learning</h1>
      <p class="subtitle">Inquisitive. Learning. Sharing. Simplicity = Reliability</p>
      <nav>
        <a href="/" class="back-link">‚Üê Back to Home</a>
      </nav>
    </div>
  </header>

  <main class="post-container">
    <article class="post">
      <header class="post-header">
        <h1>üí° TIL: Test-Driven Development Is Key to Better LLM System Prompts</h1>
        <span class="post-date">January 2, 2025</span>
        <div class="post-tags">
          <span class="post-tag">ai</span><span class="post-tag">llm</span><span class="post-tag">til</span><span class="post-tag">prompt-engineering</span><span class="post-tag">testing</span><span class="post-tag">best-practices</span><span class="post-tag">evaluation</span><span class="post-tag">machine-learning</span><span class="post-tag">system-prompts</span>
        </div>
      </header>
      
      <div class="post-content">
        <p>
<strong>TL;DR:</strong> Anthropic‚Äôs approach to system prompt development parallels test-driven development-first creating test cases where default model behaviour fails, then developing prompts that pass these tests, followed by iterative refinement-highlighting how robust automated evaluation is not merely a quality check but the foundation for building reliable LLM applications.</p>
<!--more-->
<h2>
Introduction</h2>
<p>
2024 has made clear that writing good automated evaluations for LLM-powered systems is the most critical skill for building useful applications. This insight parallels Anthropic‚Äôs internal approach to system prompt development. As usual, Simon Willison‚Äôs <a href="https://simonwillison.net/2024/Dec/31/llms-in-2024/#evals-really-matter">recent insightful 2024 LLM overview</a> was a treasure trove. One item I picked up on was evaluating system prompts using a test-driven approach.</p>
<h2>
The Evaluation-First Approach</h2>
<p>
<a href="https://askell.io/">Amanda Askell</a>, leading fine-tuning at Anthropic, <a href="https://xcancel.com/amandaaskell/status/1866207266761760812">outlines a test-driven process</a> for system prompts:</p>
<ol>
  <li>
Create a test set of messages where the model‚Äôs default behaviour fails to  </li>
  <li>
Develop a system prompt that passes these tests  </li>
  <li>
Identify cases where the system prompt is misapplied and refine it  </li>
  <li>
Expand the test set and repeat    <br>
meet requirements  </li>
</ol>
<p>
This methodology‚Äôs importance extends beyond prompt engineering. Companies with strong evaluation suites can adopt new models faster and build more reliable features than competitors. As <a href="https://xcancel.com/cramforce/status/1860436022347075667">Vercel‚Äôs experience demonstrates</a>, moving from complex prompt protection to robust testing enables rapid iteration and development.</p>
<h2>
Conclusion</h2>
<p>
While everyone acknowledges evals‚Äô importance, implementing them effectively remains challenging. The key insight is clear: robust automated evaluation isn‚Äôt just a quality check, it‚Äôs the foundation for building reliable LLM-powered systems.</p>

      </div>
    </article>
  </main>

  <footer>
    <div class="container">
      <p>Created with <a href="https://github.com/ai-mindset/init.vim">Neovim</a>, using <a href="https://ai-mindset.github.io/dialogue-engineering">AI</a> to help process and curate content ‚ú®</p>
    </div>
  </footer>

  <script src="/script.js"></script>
</body>
</html>