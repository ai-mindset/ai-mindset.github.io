<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ðŸ”§ A 5-Minute Guide to Engineering Machine Learning Systems - Just-in-Time Learning</title>
  <link rel="stylesheet" href="/style.css">
</head>
<body>
  <header>
    <div class="container">
      <h1><a href="/">Just-in-Time Learning</a></h1>
    </div>
  </header>

  <main class="post-container">
    <article class="post">
      <header class="post-header">
        <h1>ðŸ”§ A 5-Minute Guide to Engineering Machine Learning Systems</h1>
        <span class="post-date">January 21, 2025</span>
        <div class="post-tags">
          <span class="post-tag">machine-learning</span><span class="post-tag">best-practices</span><span class="post-tag">mlops</span><span class="post-tag">monitoring</span><span class="post-tag">production</span><span class="post-tag">quality-assurance</span><span class="post-tag">data-science</span><span class="post-tag">decision-making</span>
        </div>
      </header>
      
      <div class="post-content">
        <p><strong>TL;DR:</strong> This concise guide distils Google&#39;s 43 machine learning best practices into essential principles across four phases: starting with simple heuristics before ML, building robust data pipelines, prioritising feature engineering over complex algorithms, and gradually introducing complexity only after monitoring systems are established - emphasising engineering excellence over ML expertise. </p>
<!--more-->

<h2 id="introduction">Introduction</h2>
<p>This is a concise reference guide distilling Martin Zinkevich&#39;s <a href="https://developers.google.com/machine-learning/guides/rules-of-ml">influential Google article on machine learning best practices</a>. While the original spans 43 detailed rules, this 10-minute summary captures the essential principles for building production ML systems. Whether you&#39;re starting a new project or reviewing an existing one, this summary can be used as a practical checklist for engineering-focused machine learning.</p>
<h2 id="core-philosophy">Core Philosophy</h2>
<blockquote>
<p>Do machine learning like the great engineer you are, not like the great machine learning expert you aren&#39;t.</p>
</blockquote>
<p>Most ML gains come from great features, not algorithms. The basic approach should be:</p>
<ol>
<li>Ensure solid end-to-end pipeline</li>
<li>Start with reasonable objective</li>
<li>Add common-sense features simply</li>
<li>Maintain pipeline integrity</li>
</ol>
<h2 id="phase-i-before-machine-learning-rules-1-3">Phase I: Before Machine Learning (Rules #1-3)</h2>
<ol>
<li><p><strong>Don&#39;t be afraid to launch without ML</strong></p>
<ul>
<li>Simple heuristics get you 50% of the way</li>
<li>Launch with heuristics when data is insufficient</li>
<li>Example: Use install rate for app ranking</li>
</ul>
</li>
<li><p><strong>First, design and implement metrics</strong></p>
<ul>
<li>Track everything possible in current system</li>
<li>Get early permission from users</li>
<li>Design systems with metric instrumentation</li>
<li>Implement experiment framework</li>
</ul>
</li>
<li><p><strong>Choose ML over complex heuristics</strong></p>
<ul>
<li>Simple heuristics for launching</li>
<li>Complex heuristics become unmaintainable</li>
<li>ML models are easier to maintain long-term</li>
</ul>
</li>
</ol>
<h2 id="phase-ii-first-pipeline-rules-4-11">Phase II: First Pipeline (Rules #4-11)</h2>
<ol>
<li><p><strong>Keep first model simple, get infrastructure right</strong></p>
<ul>
<li>Focus on data pipeline integrity</li>
<li>Define clear evaluation metrics</li>
<li>Plan model integration carefully</li>
</ul>
</li>
<li><p><strong>Pipeline Health is Critical</strong></p>
<ul>
<li>Test infrastructure independently</li>
<li>Monitor freshness requirements</li>
<li>Watch for silent failures</li>
<li>Give feature columns owners</li>
<li>Document feature expectations</li>
</ul>
</li>
<li><p><strong>Starting Your ML System</strong></p>
<ul>
<li>Test getting data into algorithm</li>
<li>Test getting models out correctly</li>
<li>Monitor data statistics continuously</li>
<li>Build alerting system</li>
</ul>
</li>
</ol>
<h2 id="your-first-objective-rules-12-15">Your First Objective (Rules #12-15)</h2>
<ol>
<li><p><strong>Choose Objectives Wisely</strong></p>
<ul>
<li>Don&#39;t overthink initial objective choice</li>
<li>Start with simple, observable metrics</li>
<li>Use directly observed user behaviours</li>
<li>Example: clicks, downloads, shares</li>
</ul>
</li>
<li><p><strong>Model Selection Guidelines</strong></p>
<ul>
<li>Start with interpretable models</li>
<li>Separate spam filtering from quality ranking</li>
<li>Use simple linear models initially</li>
<li>Make debugging easier</li>
</ul>
</li>
</ol>
<h2 id="phase-iii-feature-engineering-rules-16-22">Phase III: Feature Engineering (Rules #16-22)</h2>
<ol>
<li><p><strong>Plan to launch and iterate</strong></p>
<ul>
<li>Expect regular model updates</li>
<li>Design for feature flexibility</li>
<li>Keep infrastructure clean</li>
</ul>
</li>
<li><p><strong>Feature Engineering Principles</strong></p>
<ul>
<li>Start with directly observed features</li>
<li>Use cross-product features wisely</li>
<li>Clean up unused features</li>
<li>Scale feature complexity with data</li>
</ul>
</li>
<li><p><strong>Feature Coverage and Quality</strong></p>
<ul>
<li>Features that generalise across contexts</li>
<li>Monitor feature coverage</li>
<li>Document feature ownership</li>
<li>Regular feature clean-up</li>
</ul>
</li>
</ol>
<h2 id="human-analysis-rules-23-28">Human Analysis (Rules #23-28)</h2>
<ol>
<li><p><strong>Testing and Validation</strong></p>
<ul>
<li>Use crowdsourcing or live experiments</li>
<li>Measure model deltas explicitly</li>
<li>Look for error patterns</li>
<li>Consider long-term effects</li>
</ul>
</li>
<li><p><strong>Common Pitfalls</strong></p>
<ul>
<li>Engineers aren&#39;t typical users</li>
<li>Beware of confirmation bias</li>
<li>Quantify undesirable behaviours</li>
</ul>
</li>
</ol>
<h2 id="training-serving-skew-rules-29-37">Training-Serving Skew (Rules #29-37)</h2>
<ol>
<li><p><strong>Prevent Skew</strong></p>
<ul>
<li>Save serving-time features</li>
<li>Weight sampled data properly</li>
<li>Reuse code between training/serving</li>
<li>Test on future data</li>
</ul>
</li>
<li><p><strong>Monitor Everything</strong></p>
<ul>
<li>Track performance metrics</li>
<li>Watch data distributions</li>
<li>Monitor feature coverage</li>
<li>Check prediction bias</li>
</ul>
</li>
</ol>
<h2 id="phase-iv-optimisation-and-complex-models-rules-38-43">Phase IV: Optimisation and Complex Models (Rules #38-43)</h2>
<ol>
<li><p><strong>When to Add Complexity</strong></p>
<ul>
<li>After simple approaches plateau</li>
<li>When objectives are well-aligned</li>
<li>If maintenance cost justifies gains</li>
</ul>
</li>
<li><p><strong>Advanced Techniques</strong></p>
<ul>
<li>Keep ensembles simple</li>
<li>Look for new information sources</li>
<li>Balance complexity vs. benefits</li>
</ul>
</li>
</ol>
<h2 id="final-recommendations">Final Recommendations</h2>
<ol>
<li><p><strong>Launch Decisions</strong></p>
<ul>
<li>Consider multiple metrics</li>
<li>Use proxies for long-term goals</li>
<li>Balance simple vs. complex</li>
</ul>
</li>
<li><p><strong>System Evolution</strong></p>
<ul>
<li>Start simple, add complexity gradually</li>
<li>Monitor consistently</li>
<li>Keep infrastructure clean</li>
<li>Document everything</li>
</ul>
</li>
</ol>

      </div>
    </article>
  </main>

  <footer>
    <div class="container">
      <p>Created with <a href="https://github.com/ai-mindset/init.vim">Neovim</a>, using <a href="https://ai-mindset.github.io/dialogue-engineering">AI</a> to help process and curate content âœ¨</p>
    </div>
  </footer>

  <script src="/script.js"></script>
</body>
</html>