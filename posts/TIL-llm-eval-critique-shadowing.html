<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ðŸ’¡ TIL: LLM Evaluation using Critique Shadowing - Just-in-Time Learning</title>
  <link rel="stylesheet" href="/style.css">
</head>
<body>
  <header>
    <div class="container">
      <h1><a href="/">Just-in-Time Learning</a></h1>
    </div>
  </header>

  <main class="post-container">
    <article class="post">
      <header class="post-header">
        <h1>ðŸ’¡ TIL: LLM Evaluation using Critique Shadowing</h1>
        <span class="post-date">December 5, 2024</span>
        <div class="post-tags">
          <span class="post-tag">til</span><span class="post-tag">llm</span><span class="post-tag">ai</span><span class="post-tag">machine-learning</span><span class="post-tag">mlops</span><span class="post-tag">best-practices</span><span class="post-tag">production</span><span class="post-tag">model-governance</span><span class="post-tag">evaluation</span><span class="post-tag">observability</span><span class="post-tag">monitoring</span><span class="post-tag">quality-assurance</span><span class="post-tag">iterative-refinement</span>
        </div>
      </header>
      
      <div class="post-content">
        <p><strong>TL;DR:</strong> Critique Shadowing offers an expert-centered approach to LLM evaluation by starting with binary pass/fail judgments and detailed critiques before building automated systems. This iterative methodologyâ€”reminiscent of 1970s knowledge engineeringâ€”prioritizes domain expertise over complex metrics, revealing valuable insights about products and users while developing reliable evaluation systems that capture nuanced quality standards.</p>
<!--more-->

<h2 id="introduction">Introduction</h2>
<p>As LLMs increasingly drive critical business decisions, ensuring their reliability becomes paramount. Many teams struggle with complex metrics and scoring systems that lead to confusion rather than clarity. <a href="https://hamel.dev/">Hamel Husain</a>&#39;s Critique Shadowing methodology[^1] offers a systematic path from drowning in metrics to developing reliable evaluation systems.</p>
<h2 id="the-critique-shadowing-method">The Critique Shadowing Method</h2>
<p>The key insight behind Critique Shadowing is deceptively simple: start with binary (pass/fail) expert judgements and detailed critiques before building automated evaluation systems. This approach solves two critical challenges: capturing domain expertise and scaling evaluation processes.</p>
<p>This expert-centric approach echoes <a href="https://en.wikipedia.org/wiki/Knowledge_engineering">knowledge engineering</a> practices from the 1970-80s, when AI researchers first recognised the necessity of systematically capturing domain expertise. Just as <a href="https://en.wikipedia.org/wiki/Mycin">MYCIN</a>&#39;s creators worked closely with medical doctors to encode diagnostic knowledge, Critique Shadowing similarly structures the process of extracting expert judgement for LLM evaluation. While the technology has evolved from rule-based systems to large language models, the fundamental challenge of effectively capturing and operationalising expert knowledge remains central.</p>
<h3 id="implementation-process">Implementation Process</h3>
<p>The methodology follows a structured, iterative process:</p>
<center>
    <img src="https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/refs/heads/master/images/Critique%20Framework%20Hamel%20Husain.png" width="80%" height="80%"/>
</center>

<ol>
<li>Identify a principal domain expert as the arbiter of quality</li>
<li>Create a diverse dataset covering different scenarios and user types</li>
<li>Expert conducts binary pass/fail judgements with detailed critiques</li>
<li>Address discovered issues and verify fixes</li>
<li>Develop LLM-based judges using expert critiques as few-shot examples</li>
<li>Analyse error patterns and root causes</li>
<li>Create specialised judges for persistent issues</li>
</ol>
<p>The process is continuous, repeating periodically or when material changes occur. For simpler applications or when manual review is feasible, teams can adapt or streamline these steps while maintaining the core principle of systematic data examination.</p>
<h2 id="beyond-automation">Beyond Automation</h2>
<p>Husain&#39;s most striking observation is that the process of developing evaluation systems often provides more value than the resulting automated judges. The systematic collection of expert feedback reveals product insights, user needs, and failure modes that might otherwise remain hidden. This understanding drives improvements in the core system, not just its evaluation.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The Critique Shadowing methodology succeeds by prioritising expert knowledge and systematic data collection over premature automation. For teams building LLM applications, this approach offers a clear path to reliable evaluation systems while simultaneously deepening their understanding of their product and users.\ LLM evaluation is an active area of interest and research both in academia and industry. Here is a short list of resources to look into:</p>
<ul>
<li><a href="https://www.ibm.com/think/topics/llm-evaluation">IBM LLM Evaluation</a></li>
<li><a href="https://docs.mistral.ai/guides/evaluation/">Mistral AI - Evaluation</a></li>
<li><a href="https://github.com/mistralai/mistral-evals">Mistral Evals</a></li>
<li><a href="https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool">Anthropic - Using the Evaluation Tool</a></li>
<li><a href="https://dev.to/guybuildingai/-top-5-open-source-llm-evaluation-frameworks-in-2024-98m">Top 5 Open-Source LLM Evaluation Frameworks in 2024</a></li>
</ul>
<hr>
<p>[^1]: Husain, H. (2024). &quot;Creating a LLM-as-a-Judge That Drives Business
    Results&quot; <a href="https://hamel.dev/blog/posts/llm-judge/">https://hamel.dev/blog/posts/llm-judge/</a></p>

      </div>
    </article>
  </main>

  <footer>
    <div class="container">
      <p>Created with <a href="https://github.com/ai-mindset/init.vim">Neovim</a>, using <a href="https://ai-mindset.github.io/dialogue-engineering">AI</a> to help process and curate content âœ¨</p>
    </div>
  </footer>

  <script src="/script.js"></script>
</body>
</html>