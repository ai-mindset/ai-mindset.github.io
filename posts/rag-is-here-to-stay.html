<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>üóÉÔ∏è RAG Is Here To Stay - Just-in-Time Learning</title>
  <link rel="stylesheet" href="/style.css">
</head>
<body>
  <header>
    <div class="container">
      <h1><a href="/">Just-in-Time Learning</a></h1>
    </div>
  </header>

  <main class="post-container">
    <article class="post">
      <header class="post-header">
        <h1>üóÉÔ∏è RAG Is Here To Stay</h1>
        <span class="post-date">October 29, 2024</span>
        <div class="post-tags">
          <span class="post-tag">rag</span><span class="post-tag">llm</span><span class="post-tag">ai</span><span class="post-tag">performance</span>
        </div>
      </header>
      
      <div class="post-content">
        <p><strong>TL;DR:</strong> Despite larger LLM context windows, Retrieval-Augmented Generation (RAG) remains essential for information curation, data provenance, and overcoming the &quot;lost in the middle&quot; effect where models struggle with information placed centrally in long contexts-making careful retrieval strategies more valuable than simply dumping large amounts of text into expanded context windows.</p>
<!--more-->

<h2>Introduction</h2>
<p>This morning I noticed that <a href="https://xcancel.com/simonw/status/1850928417363149049">Simon Willison shared some views on RAG</a>, <a href="https://xcancel.com/burkov/status/1851159933913280647">Andryi Burkov criticised</a> people who claim that RAG is obsolete, and other RAG-related discussions taking place sparked by recent longer LLM context windows. Below I&#39;m sharing some thoughts based on personal experience.</p>
<h2>RAG</h2>
<p>RAG is not simply a workaround to context limits, it&#39;s a way to carefully curate information and data. It enables provenance and visibility of the data flowing through an LLM pipeline -compared to fine-tuning which bakes knowledge into the model itself. Importantly, RAG is not a synonym of embeddings. Embedding text is a fantastic way to enable semantic search, especially if it is done in a smart way (word, sentence, paragraph, or document) given project needs.\ I have successfully reused existing infrastructure to provide one of the largest companies in the world with the ability to quickly retrieve information through Q &amp; A. To achieve this, in the context of simplicity and leveraging existing infrastructure, I opted against adding moving parts like a Vector DB. Instead, I used plain JSON objects and an agentic system to meet the client&#39;s needs. It worked very well, with feedback from higher management being &quot;<em><strong>thank you</strong>, this is mind-blowing</em>&quot;.\ A nice overview of RAG comes from <a href="https://www.latent.space/p/llamaindex">Jerry Liu&#39;s interview on Latent Space</a>.\ <em>Update: a useful open-source tool for <a href="https://github.com/Brandon-c-tech/RAG-logger">RAGLogging</a> just came out.</em></p>
<h2>U-Shaped Performance</h2>
<p>One LLM behaviour that should be considered, before regarding RAG obsolete, is their tendency to attend to information from the beginning and end of the context window. See <a href="https://arxiv.org/abs/2307.03172">Lost in the Middle: How Language Models Use Long Contexts</a> for an empirical analysis.\ The paper concludes</p>
<blockquote>
<p>We empirically study how language models use long input contexts via a series &gt; of controlled experiments. We show that language model performance degrades &gt; significantly when changing the position of relevant information, indicating &gt; that models struggle to robustly access and use information in long input &gt; contexts. In particular, performance is often lowest when models must use &gt; information in the middle of long input contexts.We conduct a preliminary &gt; investigation of the role of (i) model architecture, (ii) query-aware &gt; contextualisation, and (iii) instruction fine-tuning to better understand how &gt; they affect how language models use context. Finally, we conclude with a &gt; practical case study of open-domain question answering,finding that the &gt; performance of language model readers saturates far before retriever recall. &gt; Our results and analysis provide a better understanding of how language models &gt; use their input context and provides new evaluation protocols for future &gt; long-context models.\ &gt; In other words, simply dumping loads of text or embeddings into an LLM with a &gt; big context window -say 2M tokens- won&#39;t yield great results. There&#39;s more to &gt; it than brute forcing.</p>
</blockquote>
<h2>Conclusion</h2>
<p>Extending context length, as appealing as it may sound, neither simplifies nor solves the issue of creating a good quality AI system that is enriched by large text corpora. It seems that when it comes to larger data volumes, <a href="https://www.youtube.com/watch?v=5e1Wzbr8wGU">semantic search augmented with Graph search</a> could be a more robust, albeit more involved, approach. Solid prompt engineering approaches, including <a href="https://www.promptingguide.ai/techniques/cot">Chain-of-Thought</a>, <a href="https://www.promptingguide.ai/techniques/fewshot">Few-shot prompting</a> etc. are also powerful tools to keep in our toolbox.</p>

      </div>
    </article>
  </main>

  <footer>
    <div class="container">
      <p>Created with <a href="https://github.com/ai-mindset/init.vim">Neovim</a>, using <a href="https://ai-mindset.github.io/dialogue-engineering">AI</a> to help process and curate content ‚ú®</p>
    </div>
  </footer>

  <script src="/script.js"></script>
</body>
</html>