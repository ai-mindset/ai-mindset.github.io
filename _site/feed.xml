<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2025-03-21T11:03:56+00:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Just-in-Time learning</title><subtitle>Inquisitive. Learning. Sharing. Simplicity = Reliability</subtitle><entry><title type="html">üè† Why Companies and Individuals Are Moving Back from the Cloud</title><link href="http://0.0.0.0:4000/cloud-repatriation-trends-implications/" rel="alternate" type="text/html" title="üè† Why Companies and Individuals Are Moving Back from the Cloud" /><published>2025-03-20T00:00:00+00:00</published><updated>2025-03-20T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cloud-repatriation-trends-implications</id><content type="html" xml:base="http://0.0.0.0:4000/cloud-repatriation-trends-implications/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>The last decade has witnessed the meteoric rise of cloud computing, with organisations large and small migrating their data, applications, and infrastructure to public cloud environments. The promises were compelling: reduced capital expenditure, unlimited scalability, enhanced flexibility, and access to cutting-edge technologies without the overhead of managing physical infrastructure. However, a notable countertrend has emerged in recent years ‚Äì cloud repatriation. This phenomenon, sometimes referred to as ‚Äúreverse cloud migration,‚Äù involves moving workloads, applications, and data back from public cloud environments to on-premises data centres, private clouds, or hybrid setups (<a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International, 2023</a>). I‚Äôve previously explored this topic in my article <a href="/cloud-repatriation/">The On-Prem Comeback (aka Cloud Repatriation)</a>, where I introduced the basic concepts and early examples of this trend.<br />
This article explores the growing cloud repatriation movement, examining why organisations and individuals are reconsidering their cloud-first strategies, the key drivers behind these decisions, and how they‚Äôre implementing these transitions to achieve more balanced and optimised IT infrastructures.</p>

<h2 id="the-scale-of-the-cloud-repatriation-movement">The Scale of the Cloud Repatriation Movement</h2>

<p>The repatriation trend is not isolated but represents a significant shift in how organisations approach their IT infrastructure strategy. According to a 2021 survey by IDC cited by <a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International</a>, 80% of organisations reported repatriating workloads or data from public cloud environments. More recent data from the end of 2024 showed that 86% of CIOs planned to move some public cloud workloads back to private cloud or on-premises ‚Äì the highest on record for the Barclays CIO Survey (<a href="https://www.puppet.com/blog/cloud-repatriation">Puppet, 2025</a>).<br />
A recent survey by Rackspace found that nearly seven in 10 companies (69%) have moved at least some applications off the cloud and back to on-premise systems or private clouds (<a href="https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/">ZDNet, 2025</a>).<br />
It‚Äôs important to note that this doesn‚Äôt represent a wholesale abandonment of cloud computing. Only about 8% of organisations are moving their entire workloads off the cloud, according to an October 2024 IDC survey (<a href="https://www.puppet.com/blog/cloud-repatriation">Puppet, 2025</a>). Most are selectively repatriating specific workloads while maintaining others in the cloud, resulting in more nuanced, hybrid approaches to IT infrastructure.</p>

<h2 id="key-drivers-of-cloud-repatriation">Key Drivers of Cloud Repatriation</h2>

<h3 id="cost-optimisation">Cost Optimisation</h3>

<p>While the cloud initially promised cost savings through reduced capital expenditure and operational flexibility, many organisations have experienced what industry experts call ‚Äúbill shock‚Äù as their cloud usage scales. According to <a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International</a>, ‚Äúa Gartner study predicts that through 2024, 60% of infrastructure and operations leaders will encounter public cloud cost overruns that negatively impact their on-premises budgets‚Äù.<br />
This cost concern is particularly relevant for organisations with predictable, high-volume workloads. According to <a href="https://www.rsa.com/resources/blog/identity-governance-and-administration/cloud-repatriation-why-enterprise-it-is-returning-from-the-cloud/">RSA</a>, the company 37Signals announced that its ‚Äòcloud exit‚Äô would save more than $10 million over five years. Similarly, a 2022 report by Andreessen Horowitz found that repatriation of cloud workloads could reduce cloud bills by 50% or more for some companies (<a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International, 2023</a>).<br />
David Linthicum, a leading consultant and former CTO with Deloitte, attributes much of this cost issue to technical debt: ‚ÄúThey didn‚Äôt refactor the applications to make them more efficient in running on the public cloud providers. So the public cloud providers, much like if we‚Äôre pulling too much electricity off the grid, just hit them with huge bills to support the computational and storage needs of those under-optimized applications‚Äù (<a href="https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/">ZDNet, 2025</a>).</p>

<h3 id="performance-and-latency">Performance and Latency</h3>

<p>Performance requirements are driving many repatriation decisions, particularly for applications requiring ultra-low latency. According to <a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International</a>, ‚Äúa study by the IEEE found that for certain AI workloads, on-premises GPU clusters outperformed cloud-based solutions by up to 30% in terms of performance per dollar‚Äù.<br />
This performance concern is especially critical in fields like financial trading, scientific research, and manufacturing where latency can significantly impact outcomes. As <a href="https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully">ComputerWeekly</a> notes, ‚Äútime-sensitive data includes information that users need to access as rapidly as possible ‚Äì think financial trading feeds ‚Äì or where the application is sensitive to latency‚Äù.</p>

<h3 id="security-and-compliance">Security and Compliance</h3>

<p>Security concerns and regulatory compliance requirements are powerful motivators for cloud repatriation. According to the Rackspace survey cited by <a href="https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/">ZDNet</a>, data security and compliance concerns were the most common reason for repatriation, cited by 50% of respondents. <br />
The implementation of stringent regulations like GDPR has compelled many organisations to keep certain data within specific geographic boundaries. As <a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International</a> highlights, ‚ÄúThe Data Protection Commission reported a 59% increase in GDPR complaints in 2022, underscoring the importance of data sovereignty‚Äù.<br />
Despite cloud providers‚Äô significant security investments, many organisations prefer to maintain direct control over their most sensitive data. According to <a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International</a>, ‚Äúa 2023 Thales Cloud Security Study found that 45% of businesses have experienced a cloud-based data breach or failed audit in the past 12 months, highlighting ongoing security concerns‚Äù.</p>

<h3 id="control-and-vendor-lock-in">Control and Vendor Lock-in</h3>

<p>The desire for greater control over hardware and software configurations, along with concerns about vendor lock-in, are also driving repatriation efforts. On-premises infrastructure offers more customisation possibilities that may not be available in public cloud environments.<br />
Richard Robbins, founder of TheTechnologyVault.com, observes that ‚Äúenterprises don‚Äôt like being dependent upon someone else‚Äôs cloud infrastructure‚Äù (<a href="https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/">ZDNet, 2025</a>). This concern is particularly acute among regulated industries such as financial institutions, which are ‚Äúmoving some or all of their web apps from the cloud back to on-prem or to hybrid setups‚Äù due to ‚Äúvulnerability and downsides to cloud hosting‚Äù that make ‚Äúexecutives feel nervous about not having more control‚Äù.</p>

<h2 id="the-emergence-of-balanced-approaches">The Emergence of Balanced Approaches</h2>

<p>Rather than a binary choice between cloud and on-premises, organisations are increasingly adopting hybrid and multi-cloud approaches that offer the best of both worlds. This trend allows organisations to:</p>

<ul>
  <li>Keep sensitive or high-performance workloads on-premises</li>
  <li>Leverage cloud services for scalability and innovation</li>
  <li>Maintain flexibility to adapt to changing business needs</li>
</ul>

<p>According to <a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International</a>, ‚ÄúThe hybrid cloud market is expected to grow from $85.3 billion in 2022 to $262.4 billion by 2027, according to MarketsandMarkets research‚Äù. Similarly, ‚ÄúFlexera‚Äôs 2023 State of the Cloud Report revealed that 71% of enterprises are pursuing a hybrid cloud strategy, combining public cloud, private cloud, and on-premises infrastructure‚Äù.</p>

<h2 id="personal-cloud-repatriation">Personal Cloud Repatriation</h2>

<p>The repatriation trend isn‚Äôt limited to enterprises. Individuals are also exploring self-hosting options for personal data.<br />
For example, <a href="https://hachyderm.io/@Jeffrey04/114175854454606516">a fediverse user</a> recently posted about developing a self-hosted photo album application when faced with cloud storage limitations: ‚ÄúBeing an enthusiastic photographer, my partner captured moments of us together. However, the increasing stack of photos is accelerating the imminent explosion of my cloud storage‚Äù (<a href="https://kitfucoda.medium.com/a-love-story-in-code-building-my-self-hosted-photo-album-b56a4e89ebdd">KitFu Coda, 2023</a>). This personal project highlights how individuals with technical skills can leverage idle hardware to create cost-effective alternatives to cloud storage services.<br />
As <a href="https://kitfucoda.medium.com/a-love-story-in-code-building-my-self-hosted-photo-album-b56a4e89ebdd">they note</a>, ‚ÄúSelf-hosting your own data is becoming a trend these days, and it is really not hard to get started‚Äù. This trend parallels the enterprise movement, with individuals seeking greater control, cost savings, and privacy for their personal data.</p>

<h2 id="planning-for-successful-repatriation">Planning for Successful Repatriation</h2>

<p>For organisations considering cloud repatriation, careful planning is essential. Key considerations include:</p>

<ol>
  <li><strong>Workload Assessment</strong>: Not all workloads benefit equally from repatriation. <a href="https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully">ComputerWeekly</a> advises that ‚Äúbroadly, repatriation might be the best option where data is sensitive, time sensitive or expensive to store in the cloud‚Äù.</li>
  <li><strong>Infrastructure Preparation</strong>: Organisations must ensure they have the physical capacity, networking, power, and cooling capabilities to support repatriated workloads. According to <a href="https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully">ComputerWeekly</a>, ‚Äúa large repatriation project might be a prompt to reorganise the datacentre, perhaps by moving to newer equipment that can pack more storage into a single rack or that consumes less power‚Äù.</li>
  <li><strong>Skills Assessment</strong>: <a href="https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully">ComputerWeekly</a> notes the importance of having ‚Äúenough staff to provision and manage a larger system‚Äù with the necessary ‚Äúsecurity and privacy skills needed to handle sensitive data‚Äù and ‚Äútechnical know-how to handle mission-critical, latency sensitive applications‚Äù.</li>
  <li><strong>Future-Proofing</strong>: <a href="https://www.rsa.com/resources/blog/identity-governance-and-administration/cloud-repatriation-why-enterprise-it-is-returning-from-the-cloud/">RSA</a> emphasises the importance of maintaining flexibility: ‚ÄúOrganizations should consider the long-term implications of repatriation for their overall IT strategy. This includes planning for future scalability, considering how repatriation fits into the broader digital transformation initiatives, and ensuring that the new infrastructure aligns with long-term business goals‚Äù.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>Cloud repatriation represents a maturing perspective on IT infrastructure strategy rather than a rejection of cloud computing. As organisations gain experience with cloud environments, they‚Äôre becoming more strategic about which workloads belong where, based on factors like cost, performance, security, and control.<br />
The future likely belongs to balanced, hybrid approaches that leverage the strengths of both cloud and on-premises infrastructure. As <a href="https://www.puppet.com/blog/cloud-repatriation">Puppet</a> notes, ‚ÄúCloud repatriation is not an endpoint, but rather a strategic tool in the ongoing evolution of enterprise IT. It empowers organizations to take control of their digital assets, enhance their security posture, and align their technology infrastructure with their business objectives‚Äù.<br />
For both organisations and individuals, the key is making informed decisions about where and how to deploy IT resources based on specific needs rather than following blanket ‚Äúcloud-first‚Äù or ‚Äúon-premises-first‚Äù policies. This nuanced approach to infrastructure strategy will likely characterise the next phase of digital transformation as the industry moves beyond the initial hype cycles of cloud adoption.</p>]]></content><author><name></name></author><category term="cloud" /><category term="on-prem" /><category term="performance" /><category term="security" /><category term="mlops" /><category term="deployment" /><category term="best-practices" /><category term="data-science" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">‚öôÔ∏è Turning Data Science into Real-World Value with The Drivetrain Framework</title><link href="http://0.0.0.0:4000/the-drivetrain-method/" rel="alternate" type="text/html" title="‚öôÔ∏è Turning Data Science into Real-World Value with The Drivetrain Framework" /><published>2025-03-20T00:00:00+00:00</published><updated>2025-03-20T00:00:00+00:00</updated><id>http://0.0.0.0:4000/the-drivetrain-method</id><content type="html" xml:base="http://0.0.0.0:4000/the-drivetrain-method/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>Most data science initiatives fail to deliver meaningful impact. Why? Because they focus on prediction rather than action. Organisations spend millions building sophisticated prediction models that tell them what <em>might</em> happen, but provide no clear path to influencing outcomes.<br />
This gap between prediction and value creation is what Jeremy Howard, data scientist and entrepreneur, addressed in his transformative ‚Äú<a href="https://www.youtube.com/watch?v=vYrWTDxoeGg">Drivetrain Framework</a>‚Äù back in 2012. Having successfully applied this approach to revolutionise insurance pricing, Howard outlines a systematic method for connecting data science to tangible business results.<br />
The framework isn‚Äôt about building more complex algorithms -it‚Äôs about constructing systems that link predictions to decisions that drive value. If you‚Äôre struggling to translate advanced analytics into bottom-line results or finding your data science investments yield interesting insights but limited action, this framework offers a practical solution to bridge that gap.</p>

<h2 id="the-four-critical-components">The Four Critical Components</h2>

<p>The Drivetrain Framework consists of four interconnected steps that bridge the gap between data and value:</p>

<h3 id="1-define-your-objective">1. Define Your Objective</h3>

<p>Begin with absolute clarity about what you‚Äôre trying to achieve. In Howard‚Äôs insurance example, the objective was straightforward: maximise profit from each customer based on price. For Google‚Äôs search engine, it was finding the most relevant web page based on a query. For a marketing team, it might be maximising customer lifetime value.<br />
Without a clear objective, data science becomes an academic exercise. With one, it becomes a targeted tool for value creation.</p>

<h3 id="2-identify-your-levers">2. Identify Your Levers</h3>

<p>Next, determine what variables you can actually control. These are your ‚Äúlevers‚Äù -the actions you can take to influence outcomes:</p>

<ul>
  <li>For Google, the key lever was the ordering of search results</li>
  <li>For insurers, it was the price offered to each customer</li>
  <li>For marketers, levers include product recommendations, discount offers, and communication timing</li>
</ul>

<p>The insight here is focusing not on what you can predict, but on what you can change.</p>

<h3 id="3-collect-causal-data">3. Collect Causal Data</h3>

<p>Howard emphasises a crucial distinction: most organisations have plenty of observational data showing correlations, but lack causal data showing what happens when you pull different levers.<br />
This requires intentional experimentation:</p>

<ul>
  <li>The insurance company randomly varied prices to understand true price-response relationships</li>
  <li>A marketing team might randomly test diverse recommendations rather than showing what customers already like</li>
</ul>

<p>The counterintuitive insight: You must sometimes sacrifice short-term optimisation to collect data that enables superior long-term results. Howard convinced insurance executives to randomise pricing for six months -initially accepting potentially lower profits -to build models that later significantly increased their profitability and transformed how the entire industry approached pricing.</p>

<h3 id="4-build-an-integrated-system">4. Build an Integrated System</h3>

<p>The final step combines three elements to connect levers to objectives:</p>

<ul>
  <li><strong>Modeller</strong>: Build predictive models for key relationships (e.g. how price affects purchase probability)</li>
  <li><strong>Simulator</strong>: Combine models to predict outcomes of actions (e.g. how price changes affect profit across customer segments)</li>
  <li><strong>Optimizer</strong>: Find the best lever settings to achieve objectives (e.g. optimal price for each customer)</li>
</ul>

<p>This integrated approach replaces the need for complex ‚ÄúPageRank-like‚Äù algorithms with systems that combine simpler models to optimise real-world outcomes.</p>

<h2 id="application-revolutionising-marketing">Application: Revolutionising Marketing</h2>

<p>Howard suggests marketing analytics remains in the ‚ÄúDark Ages‚Äù and ready for transformation through the Drivetrain approach:<br />
Consider Amazon‚Äôs recommendation system. Rather than simply suggesting more books by authors you‚Äôve already read, a Drivetrain-based system would:</p>

<ol>
  <li>Define the objective as maximising customer lifetime value</li>
  <li>Identify recommendation content as a key lever</li>
  <li>Collect causal data by testing diverse recommendations, including unexpected ones</li>
  <li>Build an integrated system that models what customers might enjoy but don‚Äôt yet know about, and optimises for long-term value</li>
</ol>

<p>In Howard‚Äôs experience, companies implementing this approach have seen substantial improvements in customer engagement and retention while achieving meaningful reductions in marketing costs.</p>

<h2 id="drawing-from-engineering">Drawing from Engineering</h2>

<p>Howard notes that many solutions already exist in engineering disciplines, which data scientists would benefit from studying.<br />
Aircraft designers have used integrated models and optimisation for decades, combining aerodynamic models, structural analysis, and optimisation techniques to create planes that safely fly millions of passengers daily.<br />
Building construction similarly relies on systems that integrate architectural models, structural engineering, and materials science to optimize for safety, cost, and aesthetics.<br />
The most advanced example might be Google‚Äôs self-driving car, which integrates multiple predictive models (how the car responds to controls, what sensors detect) with optimisation to safely navigate real-world environments, significantly improving safety in testing environments.<br />
These engineering successes demonstrate how combining relatively simple models into integrated systems can solve extraordinarily complex problems.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The Drivetrain Framework represents a fundamental shift in how we should approach data science:</p>

<ol>
  <li>Move beyond building better predictive models in isolation</li>
  <li>Focus on connecting predictions to actions that drive real value</li>
  <li>Invest in collecting causal data through deliberate experimentation</li>
  <li>Integrate modelling, simulation, and optimisation into coherent systems</li>
</ol>

<p>By adopting this framework, organisations can bridge the gap between sophisticated analytics and meaningful results. The companies that will gain competitive advantage aren‚Äôt those with marginally better algorithms, but those that build integrated systems connecting data to decisions that create value.</p>

<h2 id="getting-started">Getting Started</h2>

<p>To begin implementing the Drivetrain approach:</p>

<ol>
  <li>Identify one high-value business objective with measurable outcomes</li>
  <li>Map the specific levers your team can control that influence this objective</li>
  <li>Design small-scale experiments to collect causal data about these relationships</li>
  <li>Start simple -build basic models for key relationships, then integrate them before attempting sophisticated optimisation</li>
</ol>

<p>The most important step is shifting your thinking from ‚Äú<em>what can we predict?</em>‚Äù to ‚Äú<em>what actions can we take to create value?</em>‚Äù -the essence of the Drivetrain Framework.</p>]]></content><author><name></name></author><category term="data-science" /><category term="decision-making" /><category term="machine-learning" /><category term="modelling-mindsets" /><category term="optimisation" /><category term="fast-ai" /><category term="advantage" /><category term="best-practices" /><category term="design-principles" /><category term="causal-inference" /><category term="business-value" /><category term="predictive-modelling" /><category term="integration" /><category term="deliberate-experimentation" /><category term="real-value" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üì¶ From Compilation to Containerisation and Back Again</title><link href="http://0.0.0.0:4000/compilation-going-back-full-circle/" rel="alternate" type="text/html" title="üì¶ From Compilation to Containerisation and Back Again" /><published>2025-03-19T00:00:00+00:00</published><updated>2025-03-19T00:00:00+00:00</updated><id>http://0.0.0.0:4000/compilation-going-back-full-circle</id><content type="html" xml:base="http://0.0.0.0:4000/compilation-going-back-full-circle/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>Over the years, I‚Äôve experimented with numerous programming languages and deployment strategies. Python has been my domain‚Äôs lingua franca -with its vast ecosystem for data science and AI applications. However, its deployment complexities have consistently been a pain point: managing dependencies, configuring containers, and setting up build pipelines.<br />
This search for a better alternative has led me through statically compiled languages like Go and Rust; JIT-compiled languages like Julia; and hosted languages like Clojure and Scala. Yet most failed to provide a good balance between ecosystem richness and deployment simplicity. Recently, however, Deno 2.0 has emerged as a compelling solution -particularly with its ability to compile TypeScript (TS) / JavaScript (JS) to standalone executables.</p>

<h2 id="the-circular-evolution-of-programming-languages">The Circular Evolution of Programming Languages</h2>

<p>Programming languages have undergone a fascinating evolution. In the beginning (the late 1950s and 1960s), languages like Fortran, COBOL, and C were ahead-of-time compiled -transformed directly into machine code executables that could run without additional dependencies.<br />
As computing evolved, the pendulum swung toward higher-level languages -interpreted languages like Python and hosted environments like the JVM- prioritising readability and developer productivity over raw performance. These languages abstracted away machine-level concerns, allowing developers to focus on solving business problems.<br />
Yet this shift introduced new challenges. Python applications often require managing complex dependency trees, virtual environments, and platform-specific configurations. The infamous ‚Äú<em>works on my machine</em>‚Äù problem became so pervasive that containerisation emerged as a solution.<br />
While effective, containerisation introduces its own complexities: orchestration, image management, and networking configurations. What began as a solution to simplify deployment has become a complex system requiring specialised knowledge.</p>

<h2 id="deno-compilation-makes-a-comeback">Deno: Compilation Makes a Comeback</h2>

<p>Deno 2.0 represents a return to first principles. As highlighted in the <a href="https://youtube.com/watch?v=ZsDqTQs3_G0">Run JavaScript Anywhere</a> video, its <code class="language-plaintext highlighter-rouge">compile</code> command enables developers to transform JS and TS programs into standalone binaries that run across major platforms -no runtime installation or dependencies required.</p>

<div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// sample.ts</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">open</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">https://deno.land/x/open/index.ts</span><span class="dl">"</span><span class="p">;</span>

<span class="c1">// Open a URL in the default browser</span>
<span class="k">await</span> <span class="nx">open</span><span class="p">(</span><span class="dl">"</span><span class="s2">https://example.com</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div>

<p>With a simple <code class="language-plaintext highlighter-rouge">deno compile sample.ts</code> command, this code becomes a standalone executable that works on any machine without requiring Deno to be installed.<br />
This compilation process isn‚Äôt traditional transpilation to machine code -it embeds your JS and TS code into a specialized Deno runtime binary (denort). Your script and dependencies are bundled as an EZIP file and injected into the runtime binary, creating a self-contained executable that can be code-signed for distribution.</p>

<p>The key benefits include:</p>
<ol>
  <li><strong>Cross-platform compatibility</strong> without runtime requirements</li>
  <li><strong>Simplified deployment</strong> with single-binary distribution</li>
  <li><strong>Bundled assets</strong> for complete portability</li>
  <li><strong>Improved startup times</strong> compared to interpreter-based approaches</li>
</ol>

<p>Deno 2.0 enhances these capabilities further with support for npm packages, web workers, cross-compilation, smaller binary sizes, and code signing with custom icons‚Äîmaking it viable for complete applications, not just scripts.</p>

<h2 id="the-single-language-advantage">The Single Language Advantage</h2>

<p>Beyond deployment simplicity, using a single language across an entire project stack creates significant organisational benefits. I‚Äôve experienced first-hand how using different languages for front-end, back-end, and data science work can create silos within teams.<br />
<a href="https://dockyard.com/blog/2024/02/06/5-benefts-amplified-saw-switching-to-elixir">Amplified‚Äôs case study</a> demonstrates this point clearly. After switching from a React/JS front-end and Phoenix/Elixir back-end to an all-Elixir approach with LiveView, they reported:</p>

<ol>
  <li><strong>Halved server costs</strong> through more efficient resource utilisation</li>
  <li><strong>Dramatically increased development speed</strong> by eliminating cross-language silos</li>
  <li><strong>Improved team cohesion</strong> with shared tooling and knowledge</li>
  <li><strong>Enhanced maintainability</strong> through code reuse</li>
  <li><strong>Reduced team size requirements</strong> from 12 developers to just 2</li>
</ol>

<p>TS with Deno provides a similar single language opportunity -allowing teams to build front-end interfaces, back-end services, and data processing workflows with the same toolchain. The JS/TS ecosystem is rapidly maturing for AI, ML, and data science applications, as I noted in my previous article on <a href="/deno/">Modern Data Science and AI Engineering with Deno 2.0</a>.<br />
One often overlooked benefit is the reduced cognitive load when developers don‚Äôt need to context-switch between different language paradigms, package managers, testing frameworks, and debugging approaches.</p>

<h2 id="practical-applications">Practical Applications</h2>

<p>Deno‚Äôs compilation capabilities shine in several real-world scenarios:</p>

<ol>
  <li><strong>CLI Tools</strong>: Creating self-contained executables that ‚Äújust work‚Äù across platforms without complex installation instructions</li>
  <li><strong>Offline Environments</strong>: Deploying to systems without internet access, where package resolution at runtime isn‚Äôt possible</li>
  <li><strong>Cross-Platform Applications</strong>: Building desktop applications that leverage web technologies without requiring a browser runtime</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>We‚Äôve come full circle in programming language evolution -from compiled languages like Fortran in the 1950s, to interpreted languages for improved developer experience, to containerisation for managing deployment complexities, and now back to compilation with Deno<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.<br />
Deno‚Äôs approach represents a compelling blend‚Äîcombining deployment simplicity with the ecosystem richness of modern TS/JS. For AI engineering, this addresses many pain points of Python deployment while maintaining access to growing ecosystem of data science tools.<br />
While Elixir offers similar single language benefits, its distribution story remains a work in progress with projects like <a href="https://github.com/burrito-elixir/burrito">Burrito</a> showing promise but not yet fully mature. Until then, Deno stands out as a viable alternative for simplified deployment without sacrificing ecosystem benefits.<br />
The future of deployment may look surprisingly like its past, just with better languages and tools at our disposal -offering a path toward more cohesive, efficient software development that reduces complexity without sacrificing capability.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Go, Zig, Rust, C/C++ D, Nim, Common Lisp are some prominent examples of ahead-of-time compiled languages that -with the exception of Common Lisp- excel in systems programming. However, Deno allows a ubiquitous, higher-level language like JS and its superset TS to join the club of languages that can easily package code to a cross-platform single binary.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="deno" /><category term="typescript" /><category term="deployment" /><category term="cross-platform" /><category term="evolution" /><category term="toolchain" /><category term="best-practices" /><category term="code-quality" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üß† RAG vs CAG: Understanding Knowledge Augmentation in LLMs</title><link href="http://0.0.0.0:4000/rag-or-cag/" rel="alternate" type="text/html" title="üß† RAG vs CAG: Understanding Knowledge Augmentation in LLMs" /><published>2025-03-18T00:00:00+00:00</published><updated>2025-03-18T00:00:00+00:00</updated><id>http://0.0.0.0:4000/rag-or-cag</id><content type="html" xml:base="http://0.0.0.0:4000/rag-or-cag/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>Large Language Models (LLMs) face a fundamental knowledge problem: they‚Äôre limited to information present in their training data. This creates challenges when dealing with recent events that occurred after training or proprietary information specific to an organization.<br />
To address these limitations, two primary augmentation techniques have emerged: Retrieval Augmented Generation (RAG) and Cache Augmented Generation (CAG). This article breaks down both approaches based on  <a href="https://www.youtube.com/channel/UCKWaEZ-_VweaEx1j62do_vQ">IBM Technology</a>‚Äôs comprehensive explanation from their <a href="https://youtube.com/watch?v=HdafI0t3sEY">video on RAG vs CAG</a>, examining how they work, their capabilities, and when to use each one.</p>

<h2 id="understanding-rag-and-cag">Understanding RAG and CAG</h2>

<h3 id="retrieval-augmented-generation-rag">Retrieval Augmented Generation (RAG)</h3>

<p>RAG operates through a two-phase system:</p>

<ol>
  <li><strong>Offline Phase (Preparation)</strong>
    <ul>
      <li>Documents are broken into manageable chunks.</li>
      <li>Vector embeddings are created for each chunk using an embedding model.</li>
      <li>These embeddings are stored in a vector database, creating a searchable knowledge index.</li>
    </ul>
  </li>
  <li><strong>Online Phase (Query &amp; Response)</strong>
    <ul>
      <li>The user submits a query.</li>
      <li>The RAG retriever converts this query to a vector using the same embedding model.</li>
      <li>The system performs a similarity search in the vector database.</li>
      <li>It retrieves the most relevant document chunks (typically 3-5 passages).</li>
      <li>These chunks and the user‚Äôs query are placed in the LLM‚Äôs context window.</li>
      <li>The LLM generates an answer based on both the query and the retrieved context.</li>
    </ul>
  </li>
</ol>

<p>For example, if asked <em>‚ÄúWhat film won Best Picture this year?‚Äù</em>, the system might retrieve information about <em>‚ÄúAnora‚Äù</em> winning the award, even if this occurred after the model‚Äôs original training.</p>

<p>A key advantage of RAG is its modularity‚Äîcomponents like the vector database, embedding model, or LLM can be swapped independently without rebuilding the entire system.</p>

<h3 id="cache-augmented-generation-cag">Cache Augmented Generation (CAG)</h3>

<p>CAG takes a fundamentally different approach:</p>

<ul>
  <li>Instead of retrieving knowledge on demand, CAG preloads all available information into the model‚Äôs context window</li>
  <li>The entire knowledge corpus is formatted into one massive prompt that fits within the model‚Äôs context limits</li>
  <li>The LLM processes this extensive input in a single forward pass</li>
  <li>The model‚Äôs internal state is captured in what‚Äôs called a ‚ÄúKV cache‚Äù (key-value cache)</li>
  <li>When a user query arrives, it‚Äôs added to this pre-existing KV cache</li>
  <li>The model can access any relevant information from the cache without reprocessing the entire knowledge base</li>
</ul>

<p>The fundamental distinction: RAG fetches only what it predicts is needed, while CAG loads everything upfront and remembers it for later use.</p>

<h2 id="comparing-capabilities">Comparing Capabilities</h2>

<h3 id="accuracy">Accuracy</h3>
<ul>
  <li><strong>RAG</strong>: Accuracy depends heavily on the retriever component. If the retriever fails to fetch relevant documents, the LLM won‚Äôt have the facts needed to answer correctly.</li>
  <li><strong>CAG</strong>: Guarantees that all information is available (assuming it exists in the knowledge base), but places the burden on the LLM to extract the right information from a large context.</li>
</ul>

<h3 id="latency">Latency</h3>
<ul>
  <li><strong>RAG</strong>: Higher latency due to additional steps of embedding the query, searching the index, and processing retrieved text.</li>
  <li><strong>CAG</strong>: Lower latency once knowledge is cached, as answering queries requires only one forward pass without retrieval lookup time.</li>
</ul>

<h3 id="scalability">Scalability</h3>
<ul>
  <li><strong>RAG</strong>: Can scale to millions of documents as only a small portion is retrieved per query.</li>
  <li><strong>CAG</strong>: Limited by the model‚Äôs context window size (typically ~32k-100k tokens), restricting it to a few hundred documents at most.</li>
</ul>

<h3 id="data-freshness">Data Freshness</h3>
<ul>
  <li><strong>RAG</strong>: Easy to update incrementally as you add new document embeddings or remove outdated ones.</li>
  <li><strong>CAG</strong>: Requires recomputation when data changes, making it less suitable for frequently updated information.</li>
</ul>

<h2 id="when-to-use-each-approach">When to Use Each Approach</h2>

<p>The video presents several scenarios to illustrate when each approach is more appropriate:</p>

<ol>
  <li><strong>IT Help Desk Bot with Static Manual (200 pages, rarely updated)</strong>
    <ul>
      <li><strong>Best Choice</strong>: CAG</li>
      <li><strong>Rationale</strong>: Knowledge base is small enough to fit in most LLM context windows, information is static, and caching enables faster query responses.</li>
    </ul>
  </li>
  <li><strong>Legal Research Assistant (Thousands of constantly updated documents)</strong>
    <ul>
      <li><strong>Best Choice</strong>: RAG</li>
      <li><strong>Rationale</strong>: Knowledge base is massive and dynamic, precise citations are required, and incremental updates are essential.</li>
    </ul>
  </li>
  <li><strong>Clinical Decision Support System (Patient records, treatment guides, drug interactions)</strong>
    <ul>
      <li><strong>Best Choice</strong>: Hybrid Approach</li>
      <li><strong>Rationale</strong>: Use RAG to retrieve relevant subsets from the massive knowledge base, then load that retrieved content into a long-context model using CAG for follow-up questions.</li>
    </ul>
  </li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>The choice between RAG and CAG ultimately depends on your specific use case. Consider RAG when dealing with large or frequently updated knowledge sources, when citations are necessary, or when resources for running long-context models are limited. CAG is preferable when working with a fixed knowledge set that fits within your model‚Äôs context window, when low latency is crucial, or when you want to simplify deployment.<br />
As LLM technology evolves with expanding context windows and improved retrieval mechanisms, we may see these approaches converge or new hybrid solutions emerge. For now, understanding the strengths and limitations of both RAG and CAG allows AI engineers to make informed decisions about knowledge augmentation strategies that best suit their specific applications.</p>]]></content><author><name></name></author><category term="rag" /><category term="llm" /><category term="ai" /><category term="machine-learning" /><category term="prompt-engineering" /><category term="nlp" /><category term="data-processing" /><category term="best-practices" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">ü§ñ The State of AI Agents in 2025</title><link href="http://0.0.0.0:4000/navigating-ais-frontier-2025/" rel="alternate" type="text/html" title="ü§ñ The State of AI Agents in 2025" /><published>2025-03-15T00:00:00+00:00</published><updated>2025-03-15T00:00:00+00:00</updated><id>http://0.0.0.0:4000/navigating-ais-frontier-2025</id><content type="html" xml:base="http://0.0.0.0:4000/navigating-ais-frontier-2025/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>The AI landscape has evolved at a breathtaking pace over the past few years, with autonomous AI agents being positioned as the next revolutionary frontier. At the 2025 AI Engineer Summit, Grace Isford, a partner at Lux Capital, delivered an <a href="https://www.youtube.com/watch?v=HS5a8VIKsvA">insightful keynote</a> on ‚ÄúThe State of the AI Frontier‚Äù that challenged the prevailing narrative about AI agents. While many industry players proclaim that 2025 marks the ‚Äúperfect storm‚Äù for AI agents, Isford‚Äôs presentation offered a more nuanced view, highlighting both the tremendous progress and the significant challenges that remain. This article summarises the key insights from her keynote, examining the current state of AI agents and the strategies developers can employ to overcome persistent limitations.</p>

<h2 id="the-perfect-storm-for-ai-agents">The Perfect Storm for AI Agents</h2>

<p>The speaker began by acknowledging the remarkable progress in AI over the past two and a half years. The industry has seen exponential advancements since the release of Stable Diffusion in August 2022, with the pace of innovation only accelerating. 2025 has already witnessed several landmark developments:</p>

<ul>
  <li>The announcement of the $500 billion Stargate project collaboration between the U.S. government, OpenAI, SoftBank, and Oracle</li>
  <li>OpenAI‚Äôs o3 model exceeding human performance in the Arc AGI challenge</li>
  <li>DeepSeq‚Äôs R1 model launch causing market disruptions and reaching the top of the App Store</li>
  <li>France‚Äôs new AI initiative announced at the France AI Summit, bringing Europe back into the global AI race</li>
</ul>

<p>These developments, alongside other factors, have created what many call the ‚Äúperfect storm‚Äù for AI agents:</p>

<ol>
  <li>Reasoning models (like OpenAI‚Äôs o1 and o3, DeepSeq‚Äôs R1, and Grok‚Äôs latest offering) now outperform humans in various benchmarks</li>
  <li>Increased test-time compute (more resources allocated to inference rather than just training)</li>
  <li>Engineering and hardware optimisations driving efficiency</li>
  <li>Cheaper inference and hardware costs</li>
  <li>A narrowing gap between open-source and closed-source models</li>
  <li>Massive infrastructure investments from governments and corporations worldwide</li>
</ol>

<h2 id="the-reality-gap-why-ai-agents-arent-quite-working-yet">The Reality Gap: Why AI Agents Aren‚Äôt Quite Working Yet</h2>

<p>Despite this promising landscape, Isford argued that truly autonomous AI agents aren‚Äôt functioning as seamlessly as industry hype suggests. To illustrate this point, she shared a real-world example of trying to use OpenAI‚Äôs operator to book a flight from New York to San Francisco with specific requirements. Despite seemingly straightforward criteria (departure time after 3 PM, avoiding rush hour, specific airlines, budget constraints, seat preferences), the agent failed to deliver a satisfactory result.</p>

<p>The presenter identified five categories of cumulative errors that prevent AI agents from delivering consistent, reliable results:</p>

<ol>
  <li><strong>Decision Errors</strong>: Choosing incorrect facts or overthinking/exaggerating scenarios</li>
  <li><strong>Implementation Errors</strong>: Encountering access issues or integration failures (like CAPTCHA challenges)</li>
  <li><strong>Heuristic Errors</strong>: Applying wrong criteria or missing critical contextual information</li>
  <li><strong>Taste Errors</strong>: Failing to account for personal preferences not explicitly stated</li>
  <li><strong>Perfection Paradox</strong>: User expectations heightened by AI‚Äôs capabilities in some areas lead to frustration when agents perform at merely human speed or make basic errors</li>
</ol>

<p>These errors compound dramatically in complex multi-agent systems with multi-step tasks. Isford presented a compelling visual example showing how even agents with impressive 99% and 95% accuracy rates drop to 60% and 8% reliability respectively after just 50 consecutive steps.</p>

<h2 id="five-strategies-for-building-better-ai-agents">Five Strategies for Building Better AI Agents</h2>

<p>The keynote then shifted to offering concrete strategies for mitigating these challenges and building more effective AI agents:</p>

<h3 id="1-data-curation">1. Data Curation</h3>
<ul>
  <li>Recognise that data is increasingly diverse (text, images, video, audio, sensor data)</li>
  <li>Curate proprietary data, including data generated by the agent itself</li>
  <li>Design ‚Äúdata flywheels‚Äù that automatically improve agent performance through user interactions</li>
  <li>Recycle and adapt to user preferences in real-time</li>
</ul>

<h3 id="2-robust-evaluation-systems">2. Robust Evaluation Systems</h3>
<ul>
  <li>Move beyond evaluations for verifiable domains (math, science) to develop frameworks for subjective assessments</li>
  <li>Collect signals about human preferences</li>
  <li>Build personalised evaluation systems that reflect actual user needs</li>
  <li>Sometimes the best evaluation is direct human testing rather than relying solely on benchmarks</li>
</ul>

<h3 id="3-scaffolding-systems">3. Scaffolding Systems</h3>
<ul>
  <li>Implement safeguards to prevent cascading failures when errors occur</li>
  <li>Build complex compound systems that can work together harmoniously</li>
  <li>Incorporate human intervention at critical junctures</li>
  <li>Develop self-healing agents that can recognise their own mistakes and correct course</li>
</ul>

<h3 id="4-user-experience-as-a-competitive-moat">4. User Experience as a Competitive Moat</h3>
<ul>
  <li>Recognise that UX differentiation is crucial when most applications are using the same foundation models</li>
  <li>Deeply understand user workflows to create elegant human-machine collaboration</li>
  <li>Integrate seamlessly with existing systems to deliver tangible ROI</li>
  <li>Focus on industries with proprietary data sources and specialised workflows (robotics, manufacturing, life sciences)</li>
</ul>

<h3 id="5-multimodal-approaches">5. Multimodal Approaches</h3>
<ul>
  <li>Move beyond basic chatbot interfaces to create more human-like experiences</li>
  <li>Incorporate multiple sensory capabilities (vision, voice, and potentially touch or smell)</li>
  <li>Build personal memory systems that understand users on a deeper level</li>
  <li>Transform inconsistent but visionary products into experiences that exceed expectations through novel interfaces</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>While 2025 has created what appears to be a perfect storm for AI agents with advanced reasoning models, increased compute efficiency, and massive infrastructure investments, the reality is that autonomous AI agents still face significant challenges. The cumulative effect of small errors across decision-making, implementation, heuristics, and user preferences creates substantial reliability issues in complex agent systems.</p>

<p>However, as this keynote emphasised, these challenges are not insurmountable. By focusing on meticulous data curation, developing sophisticated evaluation frameworks, implementing robust scaffolding systems, prioritising distinctive user experiences, and embracing multimodal approaches, developers can build AI agents that deliver on their transformative potential. The lightning strike of truly autonomous, reliable AI agents may not have happened yet, but with these strategies, the industry is moving steadily toward that breakthrough moment.</p>]]></content><author><name></name></author><category term="ai" /><category term="machine-learning" /><category term="llm" /><category term="ai-alignment" /><category term="best-practices" /><category term="evaluation" /><category term="prompt-engineering" /><category term="decision-making" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üóÑÔ∏è SQLite: The Minimalist Database for AI Engineering</title><link href="http://0.0.0.0:4000/sqlite-minimalist-choice-for-ai-engineering/" rel="alternate" type="text/html" title="üóÑÔ∏è SQLite: The Minimalist Database for AI Engineering" /><published>2025-02-11T00:00:00+00:00</published><updated>2025-02-11T00:00:00+00:00</updated><id>http://0.0.0.0:4000/sqlite-minimalist-choice-for-ai-engineering</id><content type="html" xml:base="http://0.0.0.0:4000/sqlite-minimalist-choice-for-ai-engineering/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>In today‚Äôs AI engineering landscape, choosing the right database can feel overwhelming. While specialised solutions like <a href="https://qdrant.tech/">Qdrant</a> (vectors), <a href="https://neo4j.com/">Neo4j</a> (graphs), and <a href="https://www.mongodb.com/">MongoDB</a> (documents) excel in their niches, there‚Äôs a compelling case for <a href="https://www.sqlite.org/index.html">SQLite</a> as a versatile, minimalist solution that comes pre-installed on most systems and supports multiple data structures effectively. Speaking of minimalism, <a href="https://github.com/tconbeer/harlequin">Harlequin</a> (named after a <a href="https://en.wikipedia.org/wiki/Harlequin_duck">sea ü¶Ü</a>) makes data exploration very enjoyable. 
Credit for the SQLite idea goes to <a href="https://bsky.app/profile/simonwillison.net">Simon Willison</a>, a prolific AI researcher among others, who has been posting <a href="https://simonwillison.net/tags/sqlite/">blog articles</a> and <a href="https://til.simonwillison.net/sqlite">TILs</a> (Today I Learned) about it since 2003!</p>

<h2 id="the-power-of-pre-installation">The Power of Pre-installation</h2>

<p>SQLite‚Äôs ubiquity is remarkable. It comes pre-installed on:</p>
<ul>
  <li>macOS</li>
  <li>Most Linux distributions (including Ubuntu, as evidenced by its <a href="https://releases.ubuntu.com/24.10/ubuntu-24.10-desktop-amd64.manifest">manifest</a>)</li>
  <li>Python‚Äôs standard library</li>
  <li>Android devices</li>
  <li>iOS devices</li>
</ul>

<p>This universal availability means you can start developing immediately without additional setup or installation steps.</p>

<h2 id="modern-data-structure-support">Modern Data Structure Support</h2>

<p>Despite its lightweight nature, SQLite handles modern data structures surprisingly well:</p>

<ol>
  <li><strong>Vector Storage</strong><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>
    <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">CREATE</span> <span class="n">VIRTUAL</span> <span class="k">TABLE</span> <span class="n">vec_items</span> <span class="k">USING</span> <span class="n">vec0</span><span class="p">(</span><span class="n">embedding</span> <span class="nb">float</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
</code></pre></div>    </div>
    <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- vectors can be provided as JSON or in a compact binary format</span>
<span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">vec_items</span><span class="p">(</span><span class="n">rowid</span><span class="p">,</span> <span class="n">embedding</span><span class="p">)</span>
  <span class="k">VALUES</span>
 <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">'[-0.200, 0.250, 0.341, -0.211, 0.645, 0.935, -0.316, -0.924]'</span><span class="p">),</span>
 <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s1">'[0.443, -0.501, 0.355, -0.771, 0.707, -0.708, -0.185, 0.362]'</span><span class="p">),</span>
 <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s1">'[0.716, -0.927, 0.134, 0.052, -0.669, 0.793, -0.634, -0.162]'</span><span class="p">),</span>
 <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="s1">'[-0.710, 0.330, 0.656, 0.041, -0.990, 0.726, 0.385, -0.958]'</span><span class="p">);</span>
</code></pre></div>    </div>
    <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- KNN-style query</span>
<span class="k">SELECT</span>
  <span class="n">rowid</span><span class="p">,</span>
  <span class="n">distance</span>
<span class="k">FROM</span> <span class="n">vec_items</span>
<span class="k">WHERE</span> <span class="n">embedding</span> <span class="k">MATCH</span> <span class="s1">'[0.890, 0.544, 0.825, 0.961, 0.358, 0.0196, 0.521, 0.175]'</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">distance</span>
<span class="k">LIMIT</span> <span class="mi">3</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Graph Relationships</strong><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>
    <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- Create table `nodes`</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">IF</span> <span class="k">NOT</span> <span class="k">EXISTS</span> <span class="n">nodes</span> <span class="p">(</span>
 <span class="n">id</span> <span class="nb">TEXT</span> <span class="k">PRIMARY</span> <span class="k">KEY</span><span class="p">,</span>
 <span class="n">properties</span> <span class="nb">TEXT</span>
<span class="p">)</span>
</code></pre></div>    </div>
    <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- Create table `edges`</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">IF</span> <span class="k">NOT</span> <span class="k">EXISTS</span> <span class="n">edges</span> <span class="p">(</span>
 <span class="k">source</span> <span class="nb">TEXT</span><span class="p">,</span>
 <span class="n">target</span> <span class="nb">TEXT</span><span class="p">,</span>
 <span class="n">relationship</span> <span class="nb">TEXT</span><span class="p">,</span>
 <span class="n">weight</span> <span class="nb">REAL</span><span class="p">,</span>
 <span class="k">PRIMARY</span> <span class="k">KEY</span> <span class="p">(</span><span class="k">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">relationship</span><span class="p">),</span>
 <span class="k">FOREIGN</span> <span class="k">KEY</span> <span class="p">(</span><span class="k">source</span><span class="p">)</span> <span class="k">REFERENCES</span> <span class="n">nodes</span><span class="p">(</span><span class="n">id</span><span class="p">),</span>
 <span class="k">FOREIGN</span> <span class="k">KEY</span> <span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="k">REFERENCES</span> <span class="n">nodes</span><span class="p">(</span><span class="n">id</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div>    </div>
    <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- Create indices of the `edges` between `source` and `target`, for improved performance</span>
<span class="k">CREATE</span> <span class="k">INDEX</span> <span class="n">IF</span> <span class="k">NOT</span> <span class="k">EXISTS</span> <span class="n">source_idx</span> <span class="k">ON</span> <span class="n">edges</span><span class="p">(</span><span class="k">source</span><span class="p">)</span>
<span class="k">CREATE</span> <span class="k">INDEX</span> <span class="n">IF</span> <span class="k">NOT</span> <span class="k">EXISTS</span> <span class="n">target_idx</span> <span class="k">ON</span> <span class="n">edges</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
</code></pre></div>    </div>
    <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- Count the no. of incoming and outgoing edges per node, known as 'degree centrality' </span>
<span class="k">SELECT</span> <span class="n">id</span><span class="p">,</span>
    <span class="p">(</span><span class="k">SELECT</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">FROM</span> <span class="n">edges</span> <span class="k">WHERE</span> <span class="k">source</span> <span class="o">=</span> <span class="n">nodes</span><span class="p">.</span><span class="n">id</span><span class="p">)</span> <span class="o">+</span>
    <span class="p">(</span><span class="k">SELECT</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">FROM</span> <span class="n">edges</span> <span class="k">WHERE</span> <span class="n">target</span> <span class="o">=</span> <span class="n">nodes</span><span class="p">.</span><span class="n">id</span><span class="p">)</span> <span class="k">as</span> <span class="n">degree</span>
<span class="k">FROM</span> <span class="n">nodes</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">degree</span> <span class="k">DESC</span>
<span class="k">LIMIT</span> <span class="mi">10</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Document Storage</strong>
    <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">documents</span> <span class="p">(</span>
 <span class="n">id</span> <span class="nb">INTEGER</span> <span class="k">PRIMARY</span> <span class="k">KEY</span><span class="p">,</span>
 <span class="n">content</span> <span class="n">JSON</span><span class="p">,</span>
 <span class="n">metadata</span> <span class="n">JSON</span>
<span class="p">);</span>
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="portability-and-simplicity">Portability and Simplicity</h2>

<p>One of SQLite‚Äôs strongest features is its <a href="https://www.sqlite.org/onefile.html">single-file</a> nature. Your entire database exists in one file that can be:</p>
<ul>
  <li>Backed up with a simple copy operation</li>
  <li>Easily version controlled (for smaller databases)</li>
  <li>Moved between systems effortlessly</li>
  <li>Examined with standard SQLite tools</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>While specialised databases have their place, SQLite offers a compelling combination of features that make it ideal for many AI engineering projects:</p>
<ul>
  <li>Zero configuration</li>
  <li>Pre-installed availability</li>
  <li>Support for multiple data structures</li>
  <li>Single-file portability</li>
  <li>Wide language support, especially in Python and Go</li>
  <li>ACID<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> compliance</li>
</ul>

<p><strong>TL;DR</strong>: When you need a lightweight, self-contained database that can handle documents, vectors, and graphs without the complexity of a full database server, SQLite is often an excellent choice.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Example from <a href="https://alexgarcia.xyz/sqlite-vec/python.html">sqlite-vec with Python</a>¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Examples from <a href="https://dev.to/stephenc222/how-to-build-lightweight-graphrag-with-sqlite-53le">How to Build Lightweight GraphRAG with SQLite</a>¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Atomicity, Consistency, Isolation, Durability (<a href="https://en.wikipedia.org/wiki/ACID">ACID</a>), per Wikipedia, ‚Äú<em>is a set of properties of database transactions intended to guarantee data validity despite errors, power failures, and other mishaps. In the context of databases, a sequence of database operations that satisfies the ACID properties (which can be perceived as a single logical operation on the data) is called a transaction. For example, a transfer of funds from one bank account to another, even involving multiple changes such as debiting one account and crediting another, is a single transaction.</em>‚Äù¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="ai" /><category term="data-modeling" /><category term="data-processing" /><category term="data-science" /><category term="go" /><category term="minimal" /><category term="production" /><category term="python" /><category term="zero-config" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üí° TIL: To Prepare for AI, Study History‚Äôs Tech Cycles</title><link href="http://0.0.0.0:4000/TIL-prepare-for-ai/" rel="alternate" type="text/html" title="üí° TIL: To Prepare for AI, Study History‚Äôs Tech Cycles" /><published>2025-02-09T00:00:00+00:00</published><updated>2025-02-09T00:00:00+00:00</updated><id>http://0.0.0.0:4000/TIL-prepare-for-ai</id><content type="html" xml:base="http://0.0.0.0:4000/TIL-prepare-for-ai/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p><a href="https://jeremy.fast.ai/">Jeremy Howard</a> isn‚Äôt just another voice in the AI conversation. As the creator of <a href="https://towardsdatascience.com/understanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664">ULMFiT</a> (the algorithm that modern LLMs like ChatGPT are based on), founding researcher at <a href="https://course.fast.ai/">fast.ai</a>, and <a href="https://www.answer.ai/">Answer.AI</a>, Howard brings a unique perspective shaped by decades at the forefront of AI development. Recently, when <a href="https://xcancel.com/chrisbarber/status/1888037803566747942">asked about preparing for AI</a>, his response wasn‚Äôt about futuristic predictions or doomsday scenarios. Instead, he offered something more valuable: practical wisdom drawn from historical patterns.</p>

<h2 id="why-this-matters-now">Why This Matters Now</h2>
<p>We‚Äôre at a critical juncture with AI, similar to where we were with the internet in 1990. Just as the internet transformed every aspect of our lives, AI is poised to do the same. The difference? We can learn from history this time. Howard‚Äôs insights are particularly valuable because they come from someone who has not only observed but shaped these technological transitions.</p>

<h2 id="key-insights-on-technology-evolution">Key Insights on Technology Evolution</h2>
<p>Howard emphasises a crucial pattern: technology doesn‚Äôt just grow linearly. Each innovation follows a ‚Äúhockey stick‚Äù growth curve before flattening into a sigmoid.</p>

<center>
    <figure>
        <img src="https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/0a6eced3bce4c70b7ba715fe7873d1659ce2e9a9/images/hockey-stick-growth.png" width="80%" height="80%" />
    <figcaption>Hockey stick growth</figcaption>
    </figure>
</center>

<p>More importantly, new ‚Äúhockey sticks‚Äù emerge unexpectedly in different areas. This pattern repeats ‚Äúlike clockwork‚Äù making historical understanding more valuable than future predictions.</p>

<h2 id="practical-preparation-strategy">Practical Preparation Strategy</h2>
<p>Rather than trying to predict AI‚Äôs future, Howard advocates for:</p>
<ul>
  <li>Embracing uncertainty while avoiding both dismissive fear and blind hype</li>
  <li>Taking a counter-cyclical approach: pursuing opportunities others overlook</li>
  <li>Investing months in mastering AI tools, accepting initial poor results as part of the learning process</li>
  <li>Combining AI capabilities with deep domain expertise</li>
  <li>Building practical knowledge through side projects and community engagement</li>
</ul>

<h2 id="the-education-perspective">The Education Perspective</h2>
<p>Howard challenges traditional educational paths, suggesting alternatives:</p>
<ul>
  <li>Self-directed learning through resources like <a href="https://course.fast.ai/">fast.ai</a></li>
  <li>Multiple side hustles to build practical experience</li>
  <li>Community building with like-minded innovators</li>
  <li>Using AI itself to learn technical skills</li>
  <li>Developing both technical and human skills as a generalist</li>
</ul>

<h2 id="conclusion">Conclusion</h2>
<p>The key takeaway isn‚Äôt about predicting AI‚Äôs future -it‚Äôs about preparing for it intelligently. Howard‚Äôs message is: success in the AI era won‚Äôt come from perfect predictions or traditional career paths. Instead, it will come from practical engagement, continuous learning, and the ability to combine domain expertise with AI capabilities. As he puts it, those who master this combination will have ‚Äúsuperpowers‚Äù compared to those who don‚Äôt adapt.<br />
The most valuable insight? Even AI experts can‚Äôt predict AI‚Äôs future reliably. The best strategy is to engage deeply with the technology while maintaining a grounded, practical approach to learning and application. The future belongs to the tinkerers, the experimenters, and those willing to learn from both past and present.</p>]]></content><author><name></name></author><category term="til" /><category term="ai" /><category term="fast-ai" /><category term="llm" /><category term="machine-learning" /><category term="best-practices" /><category term="decision-making" /><category term="evolution" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üöÄ A Minimal, Pragmatic Approach to Production-Ready AI &amp;amp; ML with Go</title><link href="http://0.0.0.0:4000/go-pragmatic-modern-development/" rel="alternate" type="text/html" title="üöÄ A Minimal, Pragmatic Approach to Production-Ready AI &amp;amp; ML with Go" /><published>2025-01-26T00:00:00+00:00</published><updated>2025-01-26T00:00:00+00:00</updated><id>http://0.0.0.0:4000/go-pragmatic-modern-development</id><content type="html" xml:base="http://0.0.0.0:4000/go-pragmatic-modern-development/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>Modern software development often involves navigating complex toolchains, opinionated frameworks, and resource-heavy development environments. Many languages require extensive configuration, multiple runtime dependencies, and introduce significant cognitive overhead through their vast feature sets and multiple approaches to solving the same problem. Node.js, JVM languages, and even Python with its extensive ecosystem can lead to analysis paralysis, code inhomogeneity and team disagreements over tooling and style.<br />
Go offers a refreshing alternative. With a language specification under 50 pages, a consolidated toolchain, and a ‚Äúbatteries included‚Äù approach, it provides a low-cognitive-overhead solution for developers seeking simplicity and productivity. Its zero-config philosophy, coupled with built-in formatting (<code class="language-plaintext highlighter-rouge">go fmt</code>), linting<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> (<code class="language-plaintext highlighter-rouge">go vet</code>), and testing tools, promotes code uniformity and reduces team friction over stylistic choices. The sizeable Go community is centralised, using Slack in this case, which serves as a focal point for communication, support, networking, and staying informed about the latest developments.<br />
While Go may lack a REPL as sophisticated as IPython or the Julia interactive environment, this limitation encourages proper Test-Driven Development practices rather than the post-implementation testing often seen in REPL-heavy environments. Tools like <a href="https://github.com/fatih/vim-go">vim-go</a>‚Äôs <code class="language-plaintext highlighter-rouge">:GoRun</code> and Go Playground provide sufficient interactive development capabilities for most use cases.<br />
Below I‚Äôm collecting some thoughts on attractive aspects of Go I‚Äôve discerned so far and how they compare with other languages I‚Äôve considered. The list of Go‚Äôs features is far from complete, for example I‚Äôve not mentioned goroutines among others.</p>

<h2 id="python-vs-go-libraries-comparison">Python vs Go Libraries Comparison</h2>

<table>
  <thead>
    <tr>
      <th>Domain</th>
      <th>Python Library</th>
      <th>Go Equivalent</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Numerical Computing</td>
      <td><a href="https://github.com/numpy/numpy">NumPy</a></td>
      <td><a href="https://github.com/gonum/gonum">gonum</a></td>
    </tr>
    <tr>
      <td>Data Processing</td>
      <td><a href="https://github.com/pandas-dev/pandas">Pandas</a></td>
      <td><a href="https://github.com/go-gota/gota">gota</a></td>
    </tr>
    <tr>
      <td>Visualisation</td>
      <td><a href="https://github.com/plotly/plotly.py">Plotly</a></td>
      <td><a href="https://github.com/MetalBlueberry/go-plotly">go-plotly</a></td>
    </tr>
    <tr>
      <td>Gradient Boosting</td>
      <td><a href="https://github.com/dmlc/xgboost">XGBoost</a></td>
      <td><a href="https://github.com/Unity-Technologies/go-xgboost">go-xgboost</a></td>
    </tr>
    <tr>
      <td>Machine Learning</td>
      <td><a href="https://github.com/scikit-learn/scikit-learn">Scikit-Learn</a></td>
      <td><a href="https://github.com/sjwhitworth/golearn">golearn</a></td>
    </tr>
    <tr>
      <td>Deep Learning</td>
      <td><a href="https://github.com/tensorflow/tensorflow">TensorFlow</a><br /><a href="https://github.com/pytorch/pytorch">PyTorch</a></td>
      <td><a href="https://github.com/galeone/tfgo">tfgo</a><br /><a href="https://github.com/sugarme/gotch">gotch</a></td>
    </tr>
    <tr>
      <td>LLM Development</td>
      <td><a href="https://github.com/langchain-ai/langchain">LangChain</a></td>
      <td><a href="https://github.com/tmc/langchaingo">langchaingo</a></td>
    </tr>
    <tr>
      <td>Vector Search</td>
      <td><a href="https://github.com/weaviate/weaviate-python-client">Weaviate Client</a></td>
      <td><a href="https://github.com/weaviate/weaviate-python-client">Weaviate Go Client</a></td>
    </tr>
  </tbody>
</table>

<p><em>Update: <a href="https://github.com/Promacanthus/awesome-golang-ai">Awesome Golang.ai</a> is a very nice curated list of AI-related Go libraries worth checking.</em></p>

<h2 id="development-experience">Development Experience</h2>

<p>Go‚Äôs tooling is exceptional. With <a href="https://github.com/fatih/vim-go">vim-go</a> in <a href="https://neovim.io/">Neovim</a>, you get immediate access to formatting, linting, and code navigation. Unlike JVM languages or JavaScript frameworks that may require more complex build configurations, Go projects maintain a simple, predictable structure thanks to <code class="language-plaintext highlighter-rouge">go mod</code>. The <code class="language-plaintext highlighter-rouge">go fmt</code> command -triggered on save by default- enforces consistent code style eliminating debates over formatting and best practices, while <code class="language-plaintext highlighter-rouge">go vet</code> catches common mistakes early.</p>

<h2 id="error-handling-done-right">Error Handling Done Right</h2>

<p>Go‚Äôs approach to error handling initially feels verbose:</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">result</span><span class="p">,</span> <span class="n">err</span> <span class="o">:=</span> <span class="n">someFunction</span><span class="p">()</span>
<span class="k">if</span> <span class="n">err</span> <span class="o">!=</span> <span class="no">nil</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">err</span>
<span class="p">}</span>
</code></pre></div></div>

<p>But this explicitness pays dividends. By treating errors as values that must be handled, Go forces developers to think about failure cases upfront. The <code class="language-plaintext highlighter-rouge">defer</code> keyword complements this by ensuring clean-up code runs regardless of errors:</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">file</span><span class="p">,</span> <span class="n">err</span> <span class="o">:=</span> <span class="n">os</span><span class="o">.</span><span class="n">Open</span><span class="p">(</span><span class="s">"data.txt"</span><span class="p">)</span>
<span class="k">if</span> <span class="n">err</span> <span class="o">!=</span> <span class="no">nil</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">err</span>
<span class="p">}</span>
<span class="k">defer</span> <span class="n">file</span><span class="o">.</span><span class="n">Close</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="mlai-capabilities">ML/AI Capabilities</h2>

<p>While Go isn‚Äôt the primary choice for ML/AI experimentation, its simplicity and performance make it excellent for production deployments. Its standard library and growing ecosystem provide solid foundations for numerical computing (<a href="https://github.com/gonum/gonum">gonum</a>), data processing (<a href="https://github.com/go-gota/gota">gota</a>), and ML/AI applications (<a href="https://github.com/gorgonia/gorgonia">Gorgonia</a>, <a href="https://github.com/galeone/tfgo">tfgo</a>, <a href="https://github.com/sugarme/gotch">gotch</a>). The language‚Äôs focus on simplicity and performance makes it particularly suitable for model serving and inference workloads.</p>

<h2 id="language-design">Language Design</h2>

<p>Go‚Äôs refreshingly concise specification (under 50 pages) contrasts sharply with other languages. Even the highly promising Zig, a younger language half of Go‚Äôs age, has a 74-page specification despite being positioned as a simpler low-level language.</p>
<figure>
    <img src="https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/refs/heads/main/images/Zig%20language%20spec.png" width="80%" height="80%" />
    <figcaption>Zig's language spec</figcaption>
</figure>

<p>Go‚Äôs intentionally limited feature set and single way of solving problems promote maintainable, uniform code that‚Äôs easier to reason about and review, as reflected in its compact language spec.</p>
<figure>
    <img src="https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/refs/heads/main/images/Go%20language%20spec.png" width="80%" height="80%" />
    <figcaption>Go's language spec</figcaption>
</figure>

<p>For ML engineers and developers seeking a reliable, low-overhead language that excels at building robust, production-ready applications, Go offers a compelling choice. While it won‚Äôt replace Python for rapid prototyping and research, its simplicity, performance, and consolidated toolchain make it an very compelling addition to any developer‚Äôs toolkit.</p>

<h2 id="conclusion">Conclusion</h2>

<p>To my eyes, Go stands out as a pragmatic choice for modern development through its key strengths:</p>

<ul>
  <li>Minimal cognitive overhead with a 47-page specification</li>
  <li>Zero-config toolchain including formatting, testing, and package management</li>
  <li>Centralised community, providing a single-source of truth</li>
  <li>Enforced error handling and clean resource management via <code class="language-plaintext highlighter-rouge">defer</code></li>
  <li>Growing ML/AI ecosystem comparable to Python‚Äôs established libraries</li>
  <li>Cross-platform compilation and efficient garbage collection</li>
  <li>Single, clear way to solve problems, reducing team friction</li>
  <li>Lightweight development environment compared to JVM, .NET, BEAM or Node.js</li>
</ul>

<p>While Python remains dominant for ML/AI research, prototyping and -frequently- production, Go excels in production environments where code maintainability, performance, and team collaboration are crucial. Its intentionally limited feature set, combined with a comprehensive standard library and maturing ML ecosystem, makes it a very attractive choice for developers seeking simplicity without sacrificing capability.<br />
The language‚Äôs design philosophy strongly aligns with my needs as a Data professional looking to reduce tooling complexity and maintain consistent, reliable codebases. Go‚Äôs lightweight yet rich toolchain allows writing safe, efficient AI and data-oriented code based on simplicity and reliability. This refreshing alternative in today‚Äôs complex development landscape has strongly tempted me to start moving my practice to Go‚Äôs more principled approach.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Vet is -in essence- a linter, since it helps improve code quality. Quoting Go‚Äôs <a href="https://go.dev/src/cmd/vet/doc.go">vet doc</a> <em>‚ÄúVet examines Go source code and reports suspicious constructs, such as Printf calls whose arguments do not align with the format string. Vet uses heuristics that do not guarantee all reports are genuine problems, but it can find errors not caught by the compilers.‚Äù</em>¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="ai" /><category term="go" /><category term="llm" /><category term="minimal" /><category term="machine-learning" /><category term="toolchain" /><category term="zero-config" /><category term="code-quality" /><category term="cross-platform" /><category term="production" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üîß A 10-Minute Guide to Engineering Machine Learning Systems</title><link href="http://0.0.0.0:4000/ml-best-practices/" rel="alternate" type="text/html" title="üîß A 10-Minute Guide to Engineering Machine Learning Systems" /><published>2025-01-21T00:00:00+00:00</published><updated>2025-01-21T00:00:00+00:00</updated><id>http://0.0.0.0:4000/ml-best-practices</id><content type="html" xml:base="http://0.0.0.0:4000/ml-best-practices/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>This is a concise reference guide distilling Martin Zinkevich‚Äôs <a href="https://developers.google.com/machine-learning/guides/rules-of-ml">influential Google article on machine learning best practices</a>. While the original spans 43 detailed rules, this 10-minute summary captures the essential principles for building production ML systems. Whether you‚Äôre starting a new project or reviewing an existing one, this summary can be used as a practical checklist for engineering-focused machine learning.</p>

<h2 id="core-philosophy">Core Philosophy</h2>
<blockquote>
  <p>Do machine learning like the great engineer you are, not like the great machine learning expert you aren‚Äôt.</p>
</blockquote>

<p>Most ML gains come from great features, not algorithms. The basic approach should be:</p>
<ol>
  <li>Ensure solid end-to-end pipeline</li>
  <li>Start with reasonable objective</li>
  <li>Add common-sense features simply</li>
  <li>Maintain pipeline integrity</li>
</ol>

<h2 id="phase-i-before-machine-learning-rules-1-3">Phase I: Before Machine Learning (Rules #1-3)</h2>
<ol>
  <li><strong>Don‚Äôt be afraid to launch without ML</strong>
    <ul>
      <li>Simple heuristics get you 50% of the way</li>
      <li>Launch with heuristics when data is insufficient</li>
      <li>Example: Use install rate for app ranking</li>
    </ul>
  </li>
  <li><strong>First, design and implement metrics</strong>
    <ul>
      <li>Track everything possible in current system</li>
      <li>Get early permission from users</li>
      <li>Design systems with metric instrumentation</li>
      <li>Implement experiment framework</li>
    </ul>
  </li>
  <li><strong>Choose ML over complex heuristics</strong>
    <ul>
      <li>Simple heuristics for launching</li>
      <li>Complex heuristics become unmaintainable</li>
      <li>ML models are easier to maintain long-term</li>
    </ul>
  </li>
</ol>

<h2 id="phase-ii-first-pipeline-rules-4-11">Phase II: First Pipeline (Rules #4-11)</h2>
<ol>
  <li><strong>Keep first model simple, get infrastructure right</strong>
    <ul>
      <li>Focus on data pipeline integrity</li>
      <li>Define clear evaluation metrics</li>
      <li>Plan model integration carefully</li>
    </ul>
  </li>
  <li><strong>Pipeline Health is Critical</strong>
    <ul>
      <li>Test infrastructure independently</li>
      <li>Monitor freshness requirements</li>
      <li>Watch for silent failures</li>
      <li>Give feature columns owners</li>
      <li>Document feature expectations</li>
    </ul>
  </li>
  <li><strong>Starting Your ML System</strong>
    <ul>
      <li>Test getting data into algorithm</li>
      <li>Test getting models out correctly</li>
      <li>Monitor data statistics continuously</li>
      <li>Build alerting system</li>
    </ul>
  </li>
</ol>

<h2 id="your-first-objective-rules-12-15">Your First Objective (Rules #12-15)</h2>
<ol>
  <li><strong>Choose Objectives Wisely</strong>
    <ul>
      <li>Don‚Äôt overthink initial objective choice</li>
      <li>Start with simple, observable metrics</li>
      <li>Use directly observed user behaviours</li>
      <li>Example: clicks, downloads, shares</li>
    </ul>
  </li>
  <li><strong>Model Selection Guidelines</strong>
    <ul>
      <li>Start with interpretable models</li>
      <li>Separate spam filtering from quality ranking</li>
      <li>Use simple linear models initially</li>
      <li>Make debugging easier</li>
    </ul>
  </li>
</ol>

<h2 id="phase-iii-feature-engineering-rules-16-22">Phase III: Feature Engineering (Rules #16-22)</h2>
<ol>
  <li><strong>Plan to launch and iterate</strong>
    <ul>
      <li>Expect regular model updates</li>
      <li>Design for feature flexibility</li>
      <li>Keep infrastructure clean</li>
    </ul>
  </li>
  <li><strong>Feature Engineering Principles</strong>
    <ul>
      <li>Start with directly observed features</li>
      <li>Use cross-product features wisely</li>
      <li>Clean up unused features</li>
      <li>Scale feature complexity with data</li>
    </ul>
  </li>
  <li><strong>Feature Coverage and Quality</strong>
    <ul>
      <li>Features that generalise across contexts</li>
      <li>Monitor feature coverage</li>
      <li>Document feature ownership</li>
      <li>Regular feature clean-up</li>
    </ul>
  </li>
</ol>

<h2 id="human-analysis-rules-23-28">Human Analysis (Rules #23-28)</h2>
<ol>
  <li><strong>Testing and Validation</strong>
    <ul>
      <li>Use crowdsourcing or live experiments</li>
      <li>Measure model deltas explicitly</li>
      <li>Look for error patterns</li>
      <li>Consider long-term effects</li>
    </ul>
  </li>
  <li><strong>Common Pitfalls</strong>
    <ul>
      <li>Engineers aren‚Äôt typical users</li>
      <li>Beware of confirmation bias</li>
      <li>Quantify undesirable behaviours</li>
    </ul>
  </li>
</ol>

<h2 id="training-serving-skew-rules-29-37">Training-Serving Skew (Rules #29-37)</h2>
<ol>
  <li><strong>Prevent Skew</strong>
    <ul>
      <li>Save serving-time features</li>
      <li>Weight sampled data properly</li>
      <li>Reuse code between training/serving</li>
      <li>Test on future data</li>
    </ul>
  </li>
  <li><strong>Monitor Everything</strong>
    <ul>
      <li>Track performance metrics</li>
      <li>Watch data distributions</li>
      <li>Monitor feature coverage</li>
      <li>Check prediction bias</li>
    </ul>
  </li>
</ol>

<h2 id="phase-iv-optimisation-and-complex-models-rules-38-43">Phase IV: Optimisation and Complex Models (Rules #38-43)</h2>
<ol>
  <li><strong>When to Add Complexity</strong>
    <ul>
      <li>After simple approaches plateau</li>
      <li>When objectives are well-aligned</li>
      <li>If maintenance cost justifies gains</li>
    </ul>
  </li>
  <li><strong>Advanced Techniques</strong>
    <ul>
      <li>Keep ensembles simple</li>
      <li>Look for new information sources</li>
      <li>Balance complexity vs. benefits</li>
    </ul>
  </li>
</ol>

<h2 id="final-recommendations">Final Recommendations</h2>
<ol>
  <li><strong>Launch Decisions</strong>
    <ul>
      <li>Consider multiple metrics</li>
      <li>Use proxies for long-term goals</li>
      <li>Balance simple vs. complex</li>
    </ul>
  </li>
  <li><strong>System Evolution</strong>
    <ul>
      <li>Start simple, add complexity gradually</li>
      <li>Monitor consistently</li>
      <li>Keep infrastructure clean</li>
      <li>Document everything</li>
    </ul>
  </li>
</ol>]]></content><author><name></name></author><category term="machine-learning" /><category term="best-practices" /><category term="mlops" /><category term="monitoring" /><category term="production" /><category term="quality-assurance" /><category term="data-science" /><category term="decision-making" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">ü§ñ Understanding AI Agents: Tools, Planning, and Evaluation</title><link href="http://0.0.0.0:4000/agents-chip-huyen/" rel="alternate" type="text/html" title="ü§ñ Understanding AI Agents: Tools, Planning, and Evaluation" /><published>2025-01-14T00:00:00+00:00</published><updated>2025-01-14T00:00:00+00:00</updated><id>http://0.0.0.0:4000/agents-chip-huyen</id><content type="html" xml:base="http://0.0.0.0:4000/agents-chip-huyen/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>This article summarises Chip Huyen‚Äôs comprehensive blog post ‚Äú<a href="https://huyenchip.com//2025/01/07/agents.html">Agents</a>‚Äù adapted from her upcoming book AI Engineering (2025). The original piece provides an in-depth examination of intelligent agents, which represent a fundamental concept in AI, defined by Russell and Norvig in their seminal 1995 book <a href="https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach">Artificial Intelligence: A Modern Approach</a> as anything that can perceive its environment through sensors and act upon it through actuators. Huyen explores how the unprecedented capabilities of foundational models have transformed theoretical possibilities into practical applications, enabling agents to operate in diverse environments -from digital workspaces for coding to physical settings for robotics. These agents can now assist with tasks ranging from website creation to complex negotiations.</p>

<h2 id="understanding-agents-and-their-tools">Understanding Agents and Their Tools</h2>
<p>An agent‚Äôs effectiveness is determined by two key factors: its environment and its tool inventory. The environment defines the scope of possible actions, while tools enable the agent to perceive and act within this environment. Modern agents leverage three distinct categories of tools.<br />
Knowledge augmentation tools, including text retrievers and web browsing capabilities, prevent model staleness by enabling access to current information. However, web browsing tools require careful API selection to protect against unreliable or harmful content. Capability extension tools address inherent model limitations -for instance, providing calculators for precise arithmetic or code interpreters for programming tasks. These interpreters demand robust security measures to prevent code injection attacks.<br />
Write actions represent the most powerful and potentially risky category, enabling agents to modify databases or send emails. These tools are distinguished from read-only actions by their ability to affect the environment directly. The <a href="https://arxiv.org/abs/2304.09842">Chameleon</a> system demonstrated the power of tool augmentation, achieving an 11.37% improvement on ScienceQA (a science question answering task) and 17% on TabMWP (a tabular math problem-solving task) through strategic tool combination.</p>

<center>
    <figure>
           <a href="https://huyenchip.com//2025/01/07/agents.html"><img src="https://huyenchip.com/assets/pics/agents/8-tool-transition.png" width="80%" height="80%" /></a>
        <figcaption>A tool transition tree by Chameleon</figcaption>
    </figure>
</center>

<h2 id="planning-and-execution-strategies">Planning and Execution Strategies</h2>
<p>Effective planning requires balancing granularity and flexibility. While <a href="https://arxiv.org/abs/2302.04761">Toolformer</a> managed with 5 tools and <a href="https://arxiv.org/abs/2304.09842">Chameleon</a> with 13, <a href="https://arxiv.org/abs/2305.15334">Gorilla</a> attempted to handle 1,645 APIs, illustrating the complexity of tool selection. Plans can be expressed either in natural language or specific function calls, each approach offering different advantages in maintainability and precision.<br />
Foundational Model planners require minimal training but need careful prompting, while Reinforcement Learning planners demand extensive training for robustness. Modern planning systems support multiple control flows: sequential, parallel, conditional, and iterative patterns. The <a href="https://arxiv.org/abs/2210.03629">ReAct</a> framework successfully combines reasoning with action,</p>
<center>
    <figure>
        <a href="https://huyenchip.com//2025/01/07/agents.html"><img src="https://huyenchip.com/assets/pics/agents/5-ReAct.png" width="80%" height="80%" /></a>
        <figcaption>ReAct agent</figcaption>
    </figure>
</center>

<p>while <a href="https://arxiv.org/abs/2303.11366">Reflexion</a> separates evaluation and self-reflection for improved performance.</p>
<center>
    <figure>
        <a href="https://huyenchip.com//2025/01/07/agents.html"><img src="https://huyenchip.com/assets/pics/agents/6-reflexion.png" width="80%" height="80%" /></a>
        <figcaption>Reflexion agent</figcaption>
    </figure>
</center>

<h2 id="reflection-and-error-management">Reflection and Error Management</h2>
<p>Continuous reflection and error correction form the backbone of reliable agent systems. The process begins with query validation, continues through plan assessment, and extends to execution monitoring. Chameleon‚Äôs tool transition analysis shows how tools are commonly used together, while Voyager‚Äôs skill manager builds on this by tracking and reusing successful tool combinations.</p>

<h2 id="evaluation-framework">Evaluation Framework</h2>
<p>Agent evaluation requires a comprehensive approach to failure mode analysis. Planning failures might involve invalid tools or incorrect parameters, while tool-specific failures demand targeted analysis. Efficiency metrics must consider not just step count and costs, but also completion time constraints. When comparing AI and human agents, it‚Äôs essential to recognise their different operational patterns -what‚Äôs efficient for one may be inefficient for the other. Working with domain experts helps identify missing tools and validate performance metrics.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Huyen‚Äôs analysis demonstrates that successful AI agents emerge from the careful orchestration of three key elements: strategic tool selection, sophisticated planning mechanisms, and robust evaluation frameworks. While tools dramatically enhance agent capabilities -as evidenced by Chameleon‚Äôs significant performance improvements- their effectiveness depends on thoughtful curation, balancing between Toolformer‚Äôs minimal approach and Gorilla‚Äôs extensive API integration. The integration of planning frameworks like ReAct and Reflexion shows how combining reasoning with action and incorporating systematic reflection can enhance agent performance. However, as an emerging field without established theoretical frameworks, significant challenges remain in tool selection, planning efficiency, and error management. Future developments will focus on agent framework evaluation and memory systems for handling information beyond context limits, while maintaining the delicate balance between capability and control that Huyen emphasises throughout her analysis.</p>]]></content><author><name></name></author><category term="ai" /><category term="llm" /><category term="prompt-engineering" /><category term="system-prompts" /><category term="evaluation" /><category term="best-practices" /><category term="toolchain" /><category term="machine-learning" /><summary type="html"><![CDATA[]]></summary></entry></feed>