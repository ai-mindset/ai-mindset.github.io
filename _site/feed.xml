<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2025-02-09T13:14:37+00:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Just-in-Time learning</title><subtitle>Inquisitive. Learning. Sharing. Simplicity = Reliability</subtitle><entry><title type="html">üîÑ TIL: To Prepare for AI, Study History‚Äôs Tech Cycles</title><link href="http://0.0.0.0:4000/TIL-prepare-for-ai/" rel="alternate" type="text/html" title="üîÑ TIL: To Prepare for AI, Study History‚Äôs Tech Cycles" /><published>2025-02-09T00:00:00+00:00</published><updated>2025-02-09T00:00:00+00:00</updated><id>http://0.0.0.0:4000/TIL-prepare-for-ai</id><content type="html" xml:base="http://0.0.0.0:4000/TIL-prepare-for-ai/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p><a href="https://jeremy.fast.ai/">Jeremy Howard</a> isn‚Äôt just another voice in the AI conversation. As the creator of <a href="https://towardsdatascience.com/understanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664">ULMFiT</a> (the algorithm that modern LLMs like ChatGPT are based on), founding researcher at <a href="https://course.fast.ai/">fast.ai</a>, and <a href="https://www.answer.ai/">Answer.AI</a>, Howard brings a unique perspective shaped by decades at the forefront of AI development. Recently, when <a href="https://xcancel.com/chrisbarber/status/1888037803566747942">asked about preparing for AI</a>, his response wasn‚Äôt about futuristic predictions or doomsday scenarios. Instead, he offered something more valuable: practical wisdom drawn from historical patterns.</p>

<h2 id="why-this-matters-now">Why This Matters Now</h2>
<p>We‚Äôre at a critical juncture with AI, similar to where we were with the internet in 1990. Just as the internet transformed every aspect of our lives, AI is poised to do the same. The difference? We can learn from history this time. Howard‚Äôs insights are particularly valuable because they come from someone who has not only observed but shaped these technological transitions.</p>

<h2 id="key-insights-on-technology-evolution">Key Insights on Technology Evolution</h2>
<p>Howard emphasises a crucial pattern: technology doesn‚Äôt just grow linearly. Each innovation follows a ‚Äúhockey stick‚Äù growth curve before flattening into a sigmoid.</p>

<center>
    <figure>
        <img src="https://github.com/ai-mindset/ai-mindset.github.io/blob/0a6eced3bce4c70b7ba715fe7873d1659ce2e9a9/images/hockey-stick-growth.png" width="80%" height="80%" />
    <figcaption>Hockey stick growth</figcaption>
    </figure>
</center>

<p>More importantly, new ‚Äúhockey sticks‚Äù emerge unexpectedly in different areas. This pattern repeats ‚Äúlike clockwork‚Äù making historical understanding more valuable than future predictions.</p>

<h2 id="practical-preparation-strategy">Practical Preparation Strategy</h2>
<p>Rather than trying to predict AI‚Äôs future, Howard advocates for:</p>
<ul>
  <li>Embracing uncertainty while avoiding both dismissive fear and blind hype</li>
  <li>Taking a counter-cyclical approach: pursuing opportunities others overlook</li>
  <li>Investing months in mastering AI tools, accepting initial poor results as part of the learning process</li>
  <li>Combining AI capabilities with deep domain expertise</li>
  <li>Building practical knowledge through side projects and community engagement</li>
</ul>

<h2 id="the-education-perspective">The Education Perspective</h2>
<p>Howard challenges traditional educational paths, suggesting alternatives:</p>
<ul>
  <li>Self-directed learning through resources like fast.ai</li>
  <li>Multiple side hustles to build practical experience</li>
  <li>Community building with like-minded innovators</li>
  <li>Using AI itself to learn technical skills</li>
  <li>Developing both technical and human skills as a generalist</li>
</ul>

<h2 id="conclusion">Conclusion</h2>
<p>The key takeaway isn‚Äôt about predicting AI‚Äôs future -it‚Äôs about preparing for it intelligently. Howard‚Äôs message is: success in the AI era won‚Äôt come from perfect predictions or traditional career paths. Instead, it will come from practical engagement, continuous learning, and the ability to combine domain expertise with AI capabilities. As he puts it, those who master this combination will have ‚Äúsuperpowers‚Äù compared to those who don‚Äôt adapt.<br />
The most valuable insight? Even AI experts can‚Äôt predict AI‚Äôs future reliably. The best strategy is to engage deeply with the technology while maintaining a grounded, practical approach to learning and application. The future belongs to the tinkerers, the experimenters, and those willing to learn from both past and present.</p>]]></content><author><name></name></author><category term="til" /><category term="ai" /><category term="fast-ai" /><category term="llm" /><category term="machine-learning" /><category term="best-practices" /><category term="decision-making" /><category term="evolution" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üöÄ A Minimal, Pragmatic Approach to Production-Ready AI &amp;amp; ML with Go</title><link href="http://0.0.0.0:4000/go-pragmatic-modern-development/" rel="alternate" type="text/html" title="üöÄ A Minimal, Pragmatic Approach to Production-Ready AI &amp;amp; ML with Go" /><published>2025-01-26T00:00:00+00:00</published><updated>2025-01-26T00:00:00+00:00</updated><id>http://0.0.0.0:4000/go-pragmatic-modern-development</id><content type="html" xml:base="http://0.0.0.0:4000/go-pragmatic-modern-development/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>Modern software development often involves navigating complex toolchains, opinionated frameworks, and resource-heavy development environments. Many languages require extensive configuration, multiple runtime dependencies, and introduce significant cognitive overhead through their vast feature sets and multiple approaches to solving the same problem. Node.js, JVM languages, and even Python with its extensive ecosystem can lead to analysis paralysis, code inhomogeneity and team disagreements over tooling and style.<br />
Go offers a refreshing alternative. With a language specification under 50 pages, a consolidated toolchain, and a ‚Äúbatteries included‚Äù approach, it provides a low-cognitive-overhead solution for developers seeking simplicity and productivity. Its zero-config philosophy, coupled with built-in formatting (<code class="language-plaintext highlighter-rouge">go fmt</code>), linting<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> (<code class="language-plaintext highlighter-rouge">go vet</code>), and testing tools, promotes code uniformity and reduces team friction over stylistic choices. The sizeable Go community is centralised, using Slack in this case, which serves as a focal point for communication, support, networking, and staying informed about the latest developments.<br />
While Go may lack a REPL as sophisticated as IPython or the Julia interactive environment, this limitation encourages proper Test-Driven Development practices rather than the post-implementation testing often seen in REPL-heavy environments. Tools like <a href="https://github.com/fatih/vim-go">vim-go</a>‚Äôs <code class="language-plaintext highlighter-rouge">:GoRun</code> and Go Playground provide sufficient interactive development capabilities for most use cases.<br />
Below I‚Äôm collecting some thoughts on attractive aspects of Go I‚Äôve discerned so far and how they compare with other languages I‚Äôve considered. The list of Go‚Äôs features is far from complete, for example I‚Äôve not mentioned goroutines among others.</p>

<h2 id="python-vs-go-libraries-comparison">Python vs Go Libraries Comparison</h2>

<table>
  <thead>
    <tr>
      <th>Domain</th>
      <th>Python Library</th>
      <th>Go Equivalent</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Numerical Computing</td>
      <td><a href="https://github.com/numpy/numpy">NumPy</a></td>
      <td><a href="https://github.com/gonum/gonum">gonum</a></td>
    </tr>
    <tr>
      <td>Data Processing</td>
      <td><a href="https://github.com/pandas-dev/pandas">Pandas</a></td>
      <td><a href="https://github.com/go-gota/gota">gota</a></td>
    </tr>
    <tr>
      <td>Visualisation</td>
      <td><a href="https://github.com/plotly/plotly.py">Plotly</a></td>
      <td><a href="https://github.com/MetalBlueberry/go-plotly">go-plotly</a></td>
    </tr>
    <tr>
      <td>Gradient Boosting</td>
      <td><a href="https://github.com/dmlc/xgboost">XGBoost</a></td>
      <td><a href="https://github.com/Unity-Technologies/go-xgboost">go-xgboost</a></td>
    </tr>
    <tr>
      <td>Machine Learning</td>
      <td><a href="https://github.com/scikit-learn/scikit-learn">Scikit-Learn</a></td>
      <td><a href="https://github.com/sjwhitworth/golearn">golearn</a></td>
    </tr>
    <tr>
      <td>Deep Learning</td>
      <td><a href="https://github.com/tensorflow/tensorflow">TensorFlow</a><br /><a href="https://github.com/pytorch/pytorch">PyTorch</a></td>
      <td><a href="https://github.com/galeone/tfgo">tfgo</a><br /><a href="https://github.com/sugarme/gotch">gotch</a></td>
    </tr>
    <tr>
      <td>LLM Development</td>
      <td><a href="https://github.com/langchain-ai/langchain">LangChain</a></td>
      <td><a href="https://github.com/tmc/langchaingo">langchaingo</a></td>
    </tr>
    <tr>
      <td>Vector Search</td>
      <td><a href="https://github.com/weaviate/weaviate-python-client">Weaviate Client</a></td>
      <td><a href="https://github.com/weaviate/weaviate-python-client">Weaviate Go Client</a></td>
    </tr>
  </tbody>
</table>

<p><em>Update: <a href="https://github.com/Promacanthus/awesome-golang-ai">Awesome Golang.ai</a> is a very nice curated list of AI-related Go libraries worth checking.</em></p>

<h2 id="development-experience">Development Experience</h2>

<p>Go‚Äôs tooling is exceptional. With <a href="https://github.com/fatih/vim-go">vim-go</a> in <a href="https://neovim.io/">Neovim</a>, you get immediate access to formatting, linting, and code navigation. Unlike JVM languages or JavaScript frameworks that may require more complex build configurations, Go projects maintain a simple, predictable structure thanks to <code class="language-plaintext highlighter-rouge">go mod</code>. The <code class="language-plaintext highlighter-rouge">go fmt</code> command -triggered on save by default- enforces consistent code style eliminating debates over formatting and best practices, while <code class="language-plaintext highlighter-rouge">go vet</code> catches common mistakes early.</p>

<h2 id="error-handling-done-right">Error Handling Done Right</h2>

<p>Go‚Äôs approach to error handling initially feels verbose:</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">result</span><span class="p">,</span> <span class="n">err</span> <span class="o">:=</span> <span class="n">someFunction</span><span class="p">()</span>
<span class="k">if</span> <span class="n">err</span> <span class="o">!=</span> <span class="no">nil</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">err</span>
<span class="p">}</span>
</code></pre></div></div>

<p>But this explicitness pays dividends. By treating errors as values that must be handled, Go forces developers to think about failure cases upfront. The <code class="language-plaintext highlighter-rouge">defer</code> keyword complements this by ensuring clean-up code runs regardless of errors:</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">file</span><span class="p">,</span> <span class="n">err</span> <span class="o">:=</span> <span class="n">os</span><span class="o">.</span><span class="n">Open</span><span class="p">(</span><span class="s">"data.txt"</span><span class="p">)</span>
<span class="k">if</span> <span class="n">err</span> <span class="o">!=</span> <span class="no">nil</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">err</span>
<span class="p">}</span>
<span class="k">defer</span> <span class="n">file</span><span class="o">.</span><span class="n">Close</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="mlai-capabilities">ML/AI Capabilities</h2>

<p>While Go isn‚Äôt the primary choice for ML/AI experimentation, its simplicity and performance make it excellent for production deployments. Its standard library and growing ecosystem provide solid foundations for numerical computing (<a href="https://github.com/gonum/gonum">gonum</a>), data processing (<a href="https://github.com/go-gota/gota">gota</a>), and ML/AI applications (<a href="https://github.com/gorgonia/gorgonia">Gorgonia</a>, <a href="https://github.com/galeone/tfgo">tfgo</a>, <a href="https://github.com/sugarme/gotch">gotch</a>). The language‚Äôs focus on simplicity and performance makes it particularly suitable for model serving and inference workloads.</p>

<h2 id="language-design">Language Design</h2>

<p>Go‚Äôs refreshingly concise specification (under 50 pages) contrasts sharply with other languages. Even the highly promising Zig, a younger language half of Go‚Äôs age, has a 74-page specification despite being positioned as a simpler low-level language.</p>
<figure>
    <img src="https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/refs/heads/main/images/Zig%20language%20spec.png" width="80%" height="80%" />
    <figcaption>Zig's language spec</figcaption>
</figure>

<p>Go‚Äôs intentionally limited feature set and single way of solving problems promote maintainable, uniform code that‚Äôs easier to reason about and review, as reflected in its compact language spec.</p>
<figure>
    <img src="https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/refs/heads/main/images/Go%20language%20spec.png" width="80%" height="80%" />
    <figcaption>Go's language spec</figcaption>
</figure>

<p>For ML engineers and developers seeking a reliable, low-overhead language that excels at building robust, production-ready applications, Go offers a compelling choice. While it won‚Äôt replace Python for rapid prototyping and research, its simplicity, performance, and consolidated toolchain make it an very compelling addition to any developer‚Äôs toolkit.</p>

<h2 id="conclusion">Conclusion</h2>

<p>To my eyes, Go stands out as a pragmatic choice for modern development through its key strengths:</p>

<ul>
  <li>Minimal cognitive overhead with a 47-page specification</li>
  <li>Zero-config toolchain including formatting, testing, and package management</li>
  <li>Centralised community, providing a single-source of truth</li>
  <li>Enforced error handling and clean resource management via <code class="language-plaintext highlighter-rouge">defer</code></li>
  <li>Growing ML/AI ecosystem comparable to Python‚Äôs established libraries</li>
  <li>Cross-platform compilation and efficient garbage collection</li>
  <li>Single, clear way to solve problems, reducing team friction</li>
  <li>Lightweight development environment compared to JVM, .NET, BEAM or Node.js</li>
</ul>

<p>While Python remains dominant for ML/AI research, prototyping and -frequently- production, Go excels in production environments where code maintainability, performance, and team collaboration are crucial. Its intentionally limited feature set, combined with a comprehensive standard library and maturing ML ecosystem, makes it a very attractive choice for developers seeking simplicity without sacrificing capability.<br />
The language‚Äôs design philosophy strongly aligns with my needs as a Data professional looking to reduce tooling complexity and maintain consistent, reliable codebases. Go‚Äôs lightweight yet rich toolchain allows writing safe, efficient AI and data-oriented code based on simplicity and reliability. This refreshing alternative in today‚Äôs complex development landscape has strongly tempted me to start moving my practice to Go‚Äôs more principled approach.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Vet is -in essence- a linter, since it helps improve code quality. Quoting Go‚Äôs <a href="https://go.dev/src/cmd/vet/doc.go">vet doc</a> <em>‚ÄúVet examines Go source code and reports suspicious constructs, such as Printf calls whose arguments do not align with the format string. Vet uses heuristics that do not guarantee all reports are genuine problems, but it can find errors not caught by the compilers.‚Äù</em>¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="ai" /><category term="llm" /><category term="minimal" /><category term="machine-learning" /><category term="toolchain" /><category term="zero-config" /><category term="code-quality" /><category term="cross-platform" /><category term="production" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üîß A 10-Minute Guide to Engineering Machine Learning Systems</title><link href="http://0.0.0.0:4000/ml-best-practices/" rel="alternate" type="text/html" title="üîß A 10-Minute Guide to Engineering Machine Learning Systems" /><published>2025-01-21T00:00:00+00:00</published><updated>2025-01-21T00:00:00+00:00</updated><id>http://0.0.0.0:4000/ml-best-practices</id><content type="html" xml:base="http://0.0.0.0:4000/ml-best-practices/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>This is a concise reference guide distilling Martin Zinkevich‚Äôs <a href="https://developers.google.com/machine-learning/guides/rules-of-ml">influential Google article on machine learning best practices</a>. While the original spans 43 detailed rules, this 10-minute summary captures the essential principles for building production ML systems. Whether you‚Äôre starting a new project or reviewing an existing one, this summary can be used as a practical checklist for engineering-focused machine learning.</p>

<h2 id="core-philosophy">Core Philosophy</h2>
<blockquote>
  <p>Do machine learning like the great engineer you are, not like the great machine learning expert you aren‚Äôt.</p>
</blockquote>

<p>Most ML gains come from great features, not algorithms. The basic approach should be:</p>
<ol>
  <li>Ensure solid end-to-end pipeline</li>
  <li>Start with reasonable objective</li>
  <li>Add common-sense features simply</li>
  <li>Maintain pipeline integrity</li>
</ol>

<h2 id="phase-i-before-machine-learning-rules-1-3">Phase I: Before Machine Learning (Rules #1-3)</h2>
<ol>
  <li><strong>Don‚Äôt be afraid to launch without ML</strong>
    <ul>
      <li>Simple heuristics get you 50% of the way</li>
      <li>Launch with heuristics when data is insufficient</li>
      <li>Example: Use install rate for app ranking</li>
    </ul>
  </li>
  <li><strong>First, design and implement metrics</strong>
    <ul>
      <li>Track everything possible in current system</li>
      <li>Get early permission from users</li>
      <li>Design systems with metric instrumentation</li>
      <li>Implement experiment framework</li>
    </ul>
  </li>
  <li><strong>Choose ML over complex heuristics</strong>
    <ul>
      <li>Simple heuristics for launching</li>
      <li>Complex heuristics become unmaintainable</li>
      <li>ML models are easier to maintain long-term</li>
    </ul>
  </li>
</ol>

<h2 id="phase-ii-first-pipeline-rules-4-11">Phase II: First Pipeline (Rules #4-11)</h2>
<ol>
  <li><strong>Keep first model simple, get infrastructure right</strong>
    <ul>
      <li>Focus on data pipeline integrity</li>
      <li>Define clear evaluation metrics</li>
      <li>Plan model integration carefully</li>
    </ul>
  </li>
  <li><strong>Pipeline Health is Critical</strong>
    <ul>
      <li>Test infrastructure independently</li>
      <li>Monitor freshness requirements</li>
      <li>Watch for silent failures</li>
      <li>Give feature columns owners</li>
      <li>Document feature expectations</li>
    </ul>
  </li>
  <li><strong>Starting Your ML System</strong>
    <ul>
      <li>Test getting data into algorithm</li>
      <li>Test getting models out correctly</li>
      <li>Monitor data statistics continuously</li>
      <li>Build alerting system</li>
    </ul>
  </li>
</ol>

<h2 id="your-first-objective-rules-12-15">Your First Objective (Rules #12-15)</h2>
<ol>
  <li><strong>Choose Objectives Wisely</strong>
    <ul>
      <li>Don‚Äôt overthink initial objective choice</li>
      <li>Start with simple, observable metrics</li>
      <li>Use directly observed user behaviours</li>
      <li>Example: clicks, downloads, shares</li>
    </ul>
  </li>
  <li><strong>Model Selection Guidelines</strong>
    <ul>
      <li>Start with interpretable models</li>
      <li>Separate spam filtering from quality ranking</li>
      <li>Use simple linear models initially</li>
      <li>Make debugging easier</li>
    </ul>
  </li>
</ol>

<h2 id="phase-iii-feature-engineering-rules-16-22">Phase III: Feature Engineering (Rules #16-22)</h2>
<ol>
  <li><strong>Plan to launch and iterate</strong>
    <ul>
      <li>Expect regular model updates</li>
      <li>Design for feature flexibility</li>
      <li>Keep infrastructure clean</li>
    </ul>
  </li>
  <li><strong>Feature Engineering Principles</strong>
    <ul>
      <li>Start with directly observed features</li>
      <li>Use cross-product features wisely</li>
      <li>Clean up unused features</li>
      <li>Scale feature complexity with data</li>
    </ul>
  </li>
  <li><strong>Feature Coverage and Quality</strong>
    <ul>
      <li>Features that generalize across contexts</li>
      <li>Monitor feature coverage</li>
      <li>Document feature ownership</li>
      <li>Regular feature clean-up</li>
    </ul>
  </li>
</ol>

<h2 id="human-analysis-rules-23-28">Human Analysis (Rules #23-28)</h2>
<ol>
  <li><strong>Testing and Validation</strong>
    <ul>
      <li>Use crowdsourcing or live experiments</li>
      <li>Measure model deltas explicitly</li>
      <li>Look for error patterns</li>
      <li>Consider long-term effects</li>
    </ul>
  </li>
  <li><strong>Common Pitfalls</strong>
    <ul>
      <li>Engineers aren‚Äôt typical users</li>
      <li>Beware of confirmation bias</li>
      <li>Quantify undesirable behaviours</li>
    </ul>
  </li>
</ol>

<h2 id="training-serving-skew-rules-29-37">Training-Serving Skew (Rules #29-37)</h2>
<ol>
  <li><strong>Prevent Skew</strong>
    <ul>
      <li>Save serving-time features</li>
      <li>Weight sampled data properly</li>
      <li>Reuse code between training/serving</li>
      <li>Test on future data</li>
    </ul>
  </li>
  <li><strong>Monitor Everything</strong>
    <ul>
      <li>Track performance metrics</li>
      <li>Watch data distributions</li>
      <li>Monitor feature coverage</li>
      <li>Check prediction bias</li>
    </ul>
  </li>
</ol>

<h2 id="phase-iv-optimization-and-complex-models-rules-38-43">Phase IV: Optimization and Complex Models (Rules #38-43)</h2>
<ol>
  <li><strong>When to Add Complexity</strong>
    <ul>
      <li>After simple approaches plateau</li>
      <li>When objectives are well-aligned</li>
      <li>If maintenance cost justifies gains</li>
    </ul>
  </li>
  <li><strong>Advanced Techniques</strong>
    <ul>
      <li>Keep ensembles simple</li>
      <li>Look for new information sources</li>
      <li>Balance complexity vs. benefits</li>
    </ul>
  </li>
</ol>

<h2 id="final-recommendations">Final Recommendations</h2>
<ol>
  <li><strong>Launch Decisions</strong>
    <ul>
      <li>Consider multiple metrics</li>
      <li>Use proxies for long-term goals</li>
      <li>Balance simple vs. complex</li>
    </ul>
  </li>
  <li><strong>System Evolution</strong>
    <ul>
      <li>Start simple, add complexity gradually</li>
      <li>Monitor consistently</li>
      <li>Keep infrastructure clean</li>
      <li>Document everything</li>
    </ul>
  </li>
</ol>]]></content><author><name></name></author><category term="machine-learning" /><category term="best-practices" /><category term="mlops" /><category term="monitoring" /><category term="production" /><category term="quality-assurance" /><category term="data-science" /><category term="decision-making" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">ü§ñ Understanding AI Agents: Tools, Planning, and Evaluation</title><link href="http://0.0.0.0:4000/agents-chip-huyen/" rel="alternate" type="text/html" title="ü§ñ Understanding AI Agents: Tools, Planning, and Evaluation" /><published>2025-01-14T00:00:00+00:00</published><updated>2025-01-14T00:00:00+00:00</updated><id>http://0.0.0.0:4000/agents-chip-huyen</id><content type="html" xml:base="http://0.0.0.0:4000/agents-chip-huyen/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>This article summarises Chip Huyen‚Äôs comprehensive blog post ‚Äú<a href="https://huyenchip.com//2025/01/07/agents.html">Agents</a>‚Äù adapted from her upcoming book AI Engineering (2025). The original piece provides an in-depth examination of intelligent agents, which represent a fundamental concept in AI, defined by Russell and Norvig in their seminal 1995 book <a href="https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach">Artificial Intelligence: A Modern Approach</a> as anything that can perceive its environment through sensors and act upon it through actuators. Huyen explores how the unprecedented capabilities of foundational models have transformed theoretical possibilities into practical applications, enabling agents to operate in diverse environments -from digital workspaces for coding to physical settings for robotics. These agents can now assist with tasks ranging from website creation to complex negotiations.</p>

<h2 id="understanding-agents-and-their-tools">Understanding Agents and Their Tools</h2>
<p>An agent‚Äôs effectiveness is determined by two key factors: its environment and its tool inventory. The environment defines the scope of possible actions, while tools enable the agent to perceive and act within this environment. Modern agents leverage three distinct categories of tools.<br />
Knowledge augmentation tools, including text retrievers and web browsing capabilities, prevent model staleness by enabling access to current information. However, web browsing tools require careful API selection to protect against unreliable or harmful content. Capability extension tools address inherent model limitations -for instance, providing calculators for precise arithmetic or code interpreters for programming tasks. These interpreters demand robust security measures to prevent code injection attacks.<br />
Write actions represent the most powerful and potentially risky category, enabling agents to modify databases or send emails. These tools are distinguished from read-only actions by their ability to affect the environment directly. The <a href="https://arxiv.org/abs/2304.09842">Chameleon</a> system demonstrated the power of tool augmentation, achieving an 11.37% improvement on ScienceQA (a science question answering task) and 17% on TabMWP (a tabular math problem-solving task) through strategic tool combination.</p>

<center>
    <figure>
           <a href="https://huyenchip.com//2025/01/07/agents.html"><img src="https://huyenchip.com/assets/pics/agents/8-tool-transition.png" width="80%" height="80%" /></a>
        <figcaption>A tool transition tree by Chameleon</figcaption>
    </figure>
</center>

<h2 id="planning-and-execution-strategies">Planning and Execution Strategies</h2>
<p>Effective planning requires balancing granularity and flexibility. While <a href="https://arxiv.org/abs/2302.04761">Toolformer</a> managed with 5 tools and <a href="https://arxiv.org/abs/2304.09842">Chameleon</a> with 13, <a href="https://arxiv.org/abs/2305.15334">Gorilla</a> attempted to handle 1,645 APIs, illustrating the complexity of tool selection. Plans can be expressed either in natural language or specific function calls, each approach offering different advantages in maintainability and precision.<br />
Foundational Model planners require minimal training but need careful prompting, while Reinforcement Learning planners demand extensive training for robustness. Modern planning systems support multiple control flows: sequential, parallel, conditional, and iterative patterns. The <a href="https://arxiv.org/abs/2210.03629">ReAct</a> framework successfully combines reasoning with action,</p>
<center>
    <figure>
        <a href="https://huyenchip.com//2025/01/07/agents.html"><img src="https://huyenchip.com/assets/pics/agents/5-ReAct.png" width="80%" height="80%" /></a>
        <figcaption>ReAct agent</figcaption>
    </figure>
</center>

<p>while <a href="https://arxiv.org/abs/2303.11366">Reflexion</a> separates evaluation and self-reflection for improved performance.</p>
<center>
    <figure>
        <a href="https://huyenchip.com//2025/01/07/agents.html"><img src="https://huyenchip.com/assets/pics/agents/6-reflexion.png" width="80%" height="80%" /></a>
        <figcaption>Reflexion agent</figcaption>
    </figure>
</center>

<h2 id="reflection-and-error-management">Reflection and Error Management</h2>
<p>Continuous reflection and error correction form the backbone of reliable agent systems. The process begins with query validation, continues through plan assessment, and extends to execution monitoring. Chameleon‚Äôs tool transition analysis shows how tools are commonly used together, while Voyager‚Äôs skill manager builds on this by tracking and reusing successful tool combinations.</p>

<h2 id="evaluation-framework">Evaluation Framework</h2>
<p>Agent evaluation requires a comprehensive approach to failure mode analysis. Planning failures might involve invalid tools or incorrect parameters, while tool-specific failures demand targeted analysis. Efficiency metrics must consider not just step count and costs, but also completion time constraints. When comparing AI and human agents, it‚Äôs essential to recognise their different operational patterns -what‚Äôs efficient for one may be inefficient for the other. Working with domain experts helps identify missing tools and validate performance metrics.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Huyen‚Äôs analysis demonstrates that successful AI agents emerge from the careful orchestration of three key elements: strategic tool selection, sophisticated planning mechanisms, and robust evaluation frameworks. While tools dramatically enhance agent capabilities -as evidenced by Chameleon‚Äôs significant performance improvements- their effectiveness depends on thoughtful curation, balancing between Toolformer‚Äôs minimal approach and Gorilla‚Äôs extensive API integration. The integration of planning frameworks like ReAct and Reflexion shows how combining reasoning with action and incorporating systematic reflection can enhance agent performance. However, as an emerging field without established theoretical frameworks, significant challenges remain in tool selection, planning efficiency, and error management. Future developments will focus on agent framework evaluation and memory systems for handling information beyond context limits, while maintaining the delicate balance between capability and control that Huyen emphasizes throughout her analysis.</p>]]></content><author><name></name></author><category term="ai" /><category term="llm" /><category term="prompt-engineering" /><category term="system-prompts" /><category term="evaluation" /><category term="best-practices" /><category term="toolchain" /><category term="machine-learning" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üí° TIL: A Simple Yet Effective Ensemble Technique called Model Soup üç≤</title><link href="http://0.0.0.0:4000/TIL-model-soups/" rel="alternate" type="text/html" title="üí° TIL: A Simple Yet Effective Ensemble Technique called Model Soup üç≤" /><published>2025-01-10T00:00:00+00:00</published><updated>2025-01-10T00:00:00+00:00</updated><id>http://0.0.0.0:4000/TIL-model-soups</id><content type="html" xml:base="http://0.0.0.0:4000/TIL-model-soups/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>While most ensemble methods in machine learning combine model predictions, thanks to <a href="https://bsky.app/profile/chrisalbon.com/post/3lfbbixka7c25">Chris Albon</a> I recently learned about an alternative approach called ‚Äú<em>model soups</em>‚Äù that works directly with model parameters. Instead of aggregating outputs, model soups blend the actual weights and biases of neural networks, showing promising results in computer vision and language tasks.</p>

<center>
   <a href="https://bsky.app/profile/chrisalbon.com/post/3lfbbixka7c25"><img src="https://cdn.bsky.app/img/feed_fullsize/plain/did:plc:umpsiyampiq3bpgce7kigydz/bafkreihvr4b4gid7v6y7karhiusawtqfdbhoen2bt6q55pmugyioj3q3gq@jpeg" width="80%" height="80%" /></a>
</center>

<h2 id="main-concept">Main Concept</h2>
<p>Model soups are created by averaging the parameters (weights and biases) of multiple independently trained neural networks that share the same architecture and training setup. For example, if we have three models with weights 2.32, 4.21, and 1.23 for a particular parameter, the ‚Äúsouped‚Äù model would use (2.32 + 4.21 + 1.23) / 3 = 2.587 for that parameter. This process is repeated across all parameters in the network. However, not all parameter combinations lead to improvements -models typically need similar training datasets, optimisation methods, and hyperparameters (like learning rate and batch size) to blend effectively. When done right, parameter-averaged models can outperform both individual networks and traditional prediction-averaging ensembles, while maintaining the inference speed of a single model.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Model soups challenge our intuitions about neural networks by showing that directly averaging weights can produce better results than averaging predictions. While the technique requires careful consideration of training conditions, it provides a computationally efficient way to combine multiple models into a single network, making it particularly valuable for resource-constrained production environments where running multiple models in parallel isn‚Äôt feasible.</p>]]></content><author><name></name></author><category term="neural-network" /><category term="machine-learning" /><category term="performance" /><category term="mlops" /><category term="production" /><category term="evaluation" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üîç Understanding LLM Interpretability</title><link href="http://0.0.0.0:4000/interpreting-llms/" rel="alternate" type="text/html" title="üîç Understanding LLM Interpretability" /><published>2025-01-09T00:00:00+00:00</published><updated>2025-01-09T00:00:00+00:00</updated><id>http://0.0.0.0:4000/interpreting-llms</id><content type="html" xml:base="http://0.0.0.0:4000/interpreting-llms/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>Large Language Models (LLMs) have become increasingly sophisticated, yet understanding their inner workings remains a critical challenge for AI safety and development. This blog post summarises concepts and research presented in <a href="https://www.youtube.com/watch?v=UGO_Ehywuxc">Welch Labs‚Äô video on mechanistic interpretability</a>, examining how LLMs process information and recent advances in making their decision-making processes more transparent.</p>

<h2 id="how-llms-think">How LLMs Think</h2>
<p>LLMs process text through a sophisticated pipeline:</p>
<ol>
  <li>Text is converted into tokens and mapped to vectors</li>
  <li>These vectors flow through multiple layers via ‚Äú<em>residual streams</em>‚Äù</li>
  <li>Each layer transforms the information through attention mechanisms</li>
  <li>Final outputs emerge from probability distributions across possible tokens</li>
</ol>

<p>This process, while mathematically precise, creates a black box of neural connections that resist simple interpretation.</p>

<h2 id="the-challenge-of-model-transparency">The Challenge of Model Transparency</h2>
<p><a href="https://ai.google.dev/gemma">Google Gemma</a> models‚Äô analysis of the sentence ‚Äú<em>the reliability of Wikipedia is very</em>‚Äù demonstrates this complexity. The model assigns varying probabilities to different completions:</p>
<ul>
  <li>‚Äú<em>important</em>‚Äù (20.21%)</li>
  <li>‚Äú<em>high</em>‚Äù (11.16%)</li>
  <li>‚Äú<em>questionable</em>‚Äù (9.48%)</li>
</ul>

<p>These probabilities emerge from intricate interactions between neurons, leading to a phenomenon called <em>superposition</em><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<h2 id="superposition-and-its-solution">Superposition and Its Solution</h2>
<p>Unlike vision models where neurons correspond to specific concepts, LLMs exhibit <a href="https://arxiv.org/abs/2210.01892">polysemanticity</a> -individual neurons respond to multiple, unrelated concepts. This occurs because LLMs encode more concepts than available neurons by using specific neuron combinations.</p>

<p>This complexity necessitated the development of <a href="/sparse-autoencoders/">sparse autoencoders</a>, which:</p>
<ol>
  <li>Map complex neuron combinations to specific concepts</li>
  <li>Extract interpretable features from LLMs</li>
  <li>Enable direct manipulation of model behaviour</li>
</ol>

<h2 id="practical-implications">Practical Implications</h2>
<p>Understanding LLM internals has crucial implications:</p>
<ul>
  <li><strong>AI Safety</strong>: Better control over model behaviours and outputs</li>
  <li><strong>Development</strong>: More targeted improvements in model capabilities</li>
  <li><strong>Deployment</strong>: Enhanced ability to predict and prevent unwanted behaviours</li>
  <li><strong>Trust</strong>: Greater transparency in AI decision-making processes</li>
</ul>

<h2 id="conclusions">Conclusions</h2>
<p>While tools like sparse autoencoders have provided unprecedented insights into model behaviour, they‚Äôve also revealed the vast complexity of LLM internal mechanisms -the ‚Äúdark matter‚Äù of AI. As these models become more integral to society, advancing our ability to interpret and control them becomes increasingly critical for responsible AI development.<br />
This improved understanding represents not just academic progress, but a crucial step toward safer, more reliable AI systems.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>superposition in the context of neural networks is the ability of a single neuron to represent multiple features simultaneously.  <a href="https://hdl.handle.net/1721.1/157073">https://hdl.handle.net/1721.1/157073</a>¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="ai" /><category term="llm" /><category term="machine-learning" /><category term="neural-network" /><category term="model-governance" /><category term="ai-alignment" /><category term="interpretability" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üìê Sparse Autoencoders: A Technical Overview</title><link href="http://0.0.0.0:4000/sparse-autoencoders/" rel="alternate" type="text/html" title="üìê Sparse Autoencoders: A Technical Overview" /><published>2025-01-09T00:00:00+00:00</published><updated>2025-01-09T00:00:00+00:00</updated><id>http://0.0.0.0:4000/sparse-autoencoders</id><content type="html" xml:base="http://0.0.0.0:4000/sparse-autoencoders/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>Supervised learning has achieved remarkable successes in areas ranging from computer vision to genomics. However, as Andrew Ng points out in his <a href="https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf">CS294A lecture notes</a>, it faces a fundamental limitation: the need for manually engineered features. While researchers have spent years crafting specialised features for vision, audio, and text processing, this approach neither scales nor generalises well.
Sparse autoencoders offer an elegant solution to this challenge by automatically learning features from unlabelled data. These neural networks are distinguished by two key characteristics:</p>
<ol>
  <li>They attempt to reconstruct their input, forcing them to capture essential data patterns</li>
  <li>They employ a sparsity constraint that mimics biological neural systems, where neurons fire infrequently and selectively</li>
</ol>

<p>While simple implementations may not outperform hand-engineered features in specific domains like computer vision, their strength lies in their generality and biological plausibility. The sparse coding principle has proven effective across diverse domains including audio, text, and visual processing.<br />
The mathematical framework combines reconstruction error, regularisation, and sparsity penalties to learn efficient, interpretable representations. This approach not only advances machine learning capabilities but also provides insights into how biological neural networks might learn and process information.
This overview examines the mathematical foundations, practical implementation, and emergent properties of sparse autoencoders, following the framework presented in Stanford‚Äôs CS294A course notes.</p>

<h2 id="sparse-autoencoders">Sparse Autoencoders</h2>
<p>An autoencoder is a neural network that learns to reconstruct its input. In a sparse autoencoder, we add a critical biological constraint: neurons should be ‚Äúinactive‚Äù most of the time, mimicking how biological neurons exhibit low average firing rates.<br />
The basic architecture is:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input (x) -&gt; Hidden Layer (sparse activation) -&gt; Output (xÃÇ)
</code></pre></div></div>
<p>Where:</p>
<ul>
  <li>Input and output dimensions are equal $(x, \hat{x} \in \R^n)$</li>
  <li>Hidden layer learns a sparse representation</li>
  <li>Network uses sigmoid activation: $f(z) = \frac{1}{1+e^{-z}}$</li>
</ul>

<h2 id="mathematical-framework">Mathematical Framework</h2>

<ol>
  <li>
    <p><strong>Base Cost Function</strong> (single training example):</p>

\[J(W,b; x,y) = \frac{1}{2}||h_{W,b}(x) - y||^2\]

    <p>For a single training example:<br />
     - Measures reconstruction error between network output $h_{W,b}(x)$ and target $y$<br />
     - For autoencoders: $y = x$ (we reconstruct the input)<br />
     - $\frac{1}{2}$ factor simplifies gradient computations<br />
     - Squared L2 norm penalises larger reconstruction errors quadratically</p>
  </li>
  <li>
    <p><strong>Full Cost Function with Weight Decay</strong>:</p>

    <p>The cost function $J(W,b)$ combines the average reconstruction error<br />
 $\frac{1}{m}\sum_{i=1}^m \frac{1}{2}||h_{W,b}(x^{(i)}) - x^{(i)}||^2$</p>

    <p>with the weight decay regularisation, to prevent overfitting by penalising large weights:<br />
 $\frac{\lambda}{2}\sum_{l=1}^{n_l-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(W_{ji}^{(l)})^2$</p>

\[J(W,b) = \left[\frac{1}{m}\sum_{i=1}^m \frac{1}{2}||h_{W,b}(x^{(i)}) - y^{(i)}||^2\right] + \frac{\lambda}{2}\sum_{l=1}^{n_l-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(W_{ji}^{(l)})^2\]

    <p>Key points:</p>
    <ul>
      <li>For autoencoders, output $y^{(i)}$ equals input $x^{(i)}$</li>
      <li>Weight decay applies only to weights $W$, not biases $b$</li>
      <li>$\lambda$ balances reconstruction accuracy vs. weight magnitude</li>
      <li>The $\frac{1}{2}$ factor simplifies derivative calculations in backpropagation</li>
      <li>This regularisation is distinct from the sparsity constraint (KL divergence term)</li>
    </ul>
  </li>
  <li>
    <p><strong>Sparsity Measurement</strong>:</p>

    <p>The average activation $\hat{\rho}_j$ measures how frequently hidden unit $j$ fires across the training set:</p>

\[\hat{\rho}_j = \frac{1}{m}\sum_{i=1}^m[a_j^{(2)}(x^{(i)})]\]

    <p>Key points:</p>
    <ul>
      <li>$a_j^{(2)}(x^{(i)})$ is hidden unit $j$‚Äôs activation for input $x^{(i)}$</li>
      <li>With sigmoid activation:
        <ul>
          <li>Values near 1 mean ‚Äúactive‚Äù or ‚Äúfiring‚Äù</li>
          <li>Values near 0 mean ‚Äúinactive‚Äù</li>
        </ul>
      </li>
      <li>We constrain $\hat{\rho}_j \approx \rho$ where $\rho$ is small (typically 0.05)</li>
      <li>This enforces selective firing: each neuron responds strongly to specific input patterns</li>
    </ul>
  </li>
  <li>
    <p><strong>Sparsity Penalty</strong> (using <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a>):</p>

    <p>The sparsity penalty uses KL divergence to enforce \(\hat{\rho}_j \approx \rho\):</p>

\[\sum_{j=1}^{s_2}\rho\log\frac{\rho}{\hat{\rho}_j} + (1-\rho)\log\frac{1-\rho}{1-\hat{\rho}_j}\]

    <p>Properties of this penalty:</p>
    <ul>
      <li>Minimised (zero) when $\hat{\rho}_j = \rho$</li>
      <li>Monotonically increases as $\hat{\rho}_j$ deviates from $\rho$</li>
      <li>Becomes infinite as $\hat{\rho}_j$ approaches 0 or 1</li>
    </ul>
  </li>
  <li>
    <p><strong>Final Cost Function</strong>:</p>

\[J_{sparse}(W,b) = J(W,b) + \beta\sum_{j=1}^{s_2}KL(\rho||\hat{\rho}_j)\]

    <p>Components:</p>
    <ul>
      <li>$J(W,b)$: Standard autoencoder cost (reconstruction error + weight decay)</li>
      <li>Sparsity term: KL divergence penalty summed over $s_2$ hidden units</li>
    </ul>

    <p>$\beta$ controls:</p>
    <ul>
      <li>Balance between accurate reconstruction and sparse representation</li>
      <li>Strength of sparsity enforcement</li>
      <li>Higher $\beta$ ‚Üí stronger sparsity constraint</li>
    </ul>

    <p>This formulation naturally penalises both over- and under-activation of hidden units relative to target sparsity $\rho$.</p>
  </li>
</ol>

<h2 id="training-process">Training Process</h2>

<p>The key modification to standard backpropagation occurs in the hidden layer:</p>

\[\delta_i^{(2)} = \left(\sum_{j=1}^{s_3}W_{ji}^{(3)}\delta_j^{(3)}\right)f'(s_i^{(2)}) + \beta\left(-\frac{\rho}{\hat{\rho}_i} + \frac{1-\rho}{1-\hat{\rho}_i}\right)\]

<p>Where:</p>
<ul>
  <li>First term: Standard backpropagation gradient through the network</li>
  <li>Second term: Gradient of KL-divergence sparsity penalty</li>
  <li>$s_i^{(2)}$ is weighted input sum to hidden unit $i$</li>
  <li>$\hat{\rho}_i$ must be pre-computed using full training set</li>
</ul>

<p>This modification ensures gradient descent optimises both reconstruction accuracy and sparsity.</p>

<h2 id="practical-guidelines">Practical Guidelines</h2>

<ul>
  <li>$\rho$ ‚âà 0.05 (5% target activation rate)</li>
  <li>$\beta$ controls sparsity penalty strength</li>
  <li>Initialise weights randomly near zero</li>
  <li>Must compute forward pass on all examples first to calculate $\hat{\rho}$</li>
</ul>

<h2 id="results">Results</h2>
<p>When trained on images, the network naturally learns edge detectors at different orientations, similar to what is found in the visual cortex. This emergence of biologically plausible features validates the sparsity approach.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Sparse autoencoders represent a mathematically principled approach to unsupervised feature learning, combining biological inspiration with rigorous optimisation techniques. Their key innovation lies in the sparsity constraint, implemented through KL divergence, which forces hidden units to develop specialised, interpretable features.</p>

<p>The mathematical framework achieves this through three key components:</p>
<ol>
  <li>A reconstruction cost that ensures faithful data representation</li>
  <li>A weight decay term that prevents overfitting</li>
  <li>A sparsity penalty that enforces selective neural activation</li>
</ol>

<p>This formulation has proven successful in practice, typically leading to:</p>
<ul>
  <li>Edge and feature detectors emerging naturally from visual data</li>
  <li>Interpretable representations comparable to biological neural coding</li>
  <li>Robust feature learning even with <a href="https://en.wikipedia.org/wiki/Overcompleteness">overcomplete</a> hidden layers</li>
</ul>

<p>The practical value of sparse autoencoders extends beyond their theoretical elegance -they provide a foundation for understanding how neural networks can learn meaningful data representations without supervision. Their success in learning biologically plausible features validates both their design principles and their potential for advanced machine learning applications. Their main limitation lies in hyperparameter sensitivity, particularly to the sparsity target œÅ and weight Œ≤, requiring careful tuning for optimal performance.</p>]]></content><author><name></name></author><category term="ai" /><category term="llm" /><category term="neural-network" /><category term="machine-learning" /><category term="data-science" /><category term="linear-algebra" /><category term="statistics" /><category term="evaluation" /><category term="interpretability" /><category term="modelling-mindsets" /><category term="design-principles" /><category term="best-practices" /><category term="data-processing" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üí° TIL: How Different Societies View and Value Choice</title><link href="http://0.0.0.0:4000/TIL-the-art-of-choice/" rel="alternate" type="text/html" title="üí° TIL: How Different Societies View and Value Choice" /><published>2025-01-08T00:00:00+00:00</published><updated>2025-01-08T00:00:00+00:00</updated><id>http://0.0.0.0:4000/TIL-the-art-of-choice</id><content type="html" xml:base="http://0.0.0.0:4000/TIL-the-art-of-choice/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>Today I revisited a talk on <a href="https://www.youtube.com/watch?v=lDq9-QxvsNU">the art of choosing</a> by Sheena Iyengar. A humourous and informative presentation, it reminded me that our assumptions about choice ‚Äìas studied by Prof. Iyengar through research spanning American, European and Asian populations‚Äì reveals fascinating cultural differences in how we perceive and respond to choice. Her research reveals some eye-opening insights that I‚Äôll briefly summarise below.</p>

<h2 id="perceiving-choice">Perceiving Choice</h2>
<p>First, while Americans believe individual choice is sacred (think ‚Äúhave it your way‚Äù), research shows this isn‚Äôt universal. When studying children solving puzzles, Asian-American children actually performed better when their mothers chose for them, while Anglo-American children did better choosing for themselves. This reveals how deeply cultural context shapes not just our preferences, but the actual effectiveness of our choices.</p>

<p>Second, remember how overwhelming it feels staring at 50 different breakfast cereals? Turns out, people from post-communist countries often saw seven different sodas as just one choice: ‚Äúsoda or no soda.‚Äù This isn‚Äôt because they‚Äôre less sophisticated, it‚Äôs because the ability to spot tiny differences between products is a learned skill -not a natural one.</p>

<p>Most striking was the research on medical decisions. When comparing American and French parents making end-of-life decisions for infants, American parents had more negative emotions and guilt despite insisting on having the choice, while French parents, whose doctors made the decisions, coped better. This challenges the core American belief that having choice is always better.</p>

<p>Concluding with a personal story, Prof. Iyengar -who is blind- shared how she once brought two ‚Äúclearly different‚Äù shades of pink nail polish to her lab. When she removed the labels, half the participants couldn‚Äôt tell them apart. Those who could, chose differently when the labels were present versus absent, showing how marketing narratives shape what we think we‚Äôre choosing.</p>

<h2 id="conclusions">Conclusions</h2>
<p>The TL;DR is: Through cross-cultural research, Prof. Iyengar shows that how we understand and value choice varies dramatically across cultures. Sometimes, having fewer choices or letting others choose for us might actually lead to better outcomes. <br />
As a technologist, inundated with a very wide choice of tools that often offer similar results, I have made the conscious decision to reduce my tooling footprint to the minimum viable toolstack possible. I‚Äôm happy to let more knowledgeable professionals choose, with <em>adequate justification</em>, tools for my line of work but I do disagree with the zealotry that‚Äôs occasionally observed in tech and complemented by big egos.</p>]]></content><author><name></name></author><category term="til" /><category term="decision-making" /><category term="best-practices" /><category term="evaluation" /><category term="statistics" /><category term="design-principles" /><category term="modelling-mindsets" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üí° TIL: The Matrix Equation That Makes Linear Regression Work</title><link href="http://0.0.0.0:4000/TIL-lin-alg-applied-to-stats/" rel="alternate" type="text/html" title="üí° TIL: The Matrix Equation That Makes Linear Regression Work" /><published>2025-01-08T00:00:00+00:00</published><updated>2025-01-08T00:00:00+00:00</updated><id>http://0.0.0.0:4000/TIL-lin-alg-applied-to-stats</id><content type="html" xml:base="http://0.0.0.0:4000/TIL-lin-alg-applied-to-stats/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>This morning <a href="https://xcancel.com/andrew_n_carr/status/1876855682529480844">an interesting interview question</a> motivated me to remind myself how it‚Äôs possible to solve linear regression through matrix algebra. Below is what I learned:</p>

<h2 id="the-theory-an-elegant-mathematical-solution">The Theory: An Elegant Mathematical Solution</h2>
<p>Linear regression finds the best-fit line through data points by finding optimal coefficients ($\beta$) that minimise squared errors. The equation $\beta = (X^TX)^{-1}X^Ty$ elegantly solves this optimization problem using matrix algebra.</p>

<p>The solution involves these key components:</p>
<ol>
  <li>$X$ is our feature matrix (n samples √ó p features)</li>
  <li>$y$ is our target values (n √ó 1)</li>
  <li>$X^T$ is the transpose of X</li>
  <li>$\beta$ is our solution vector (p √ó 1) of coefficients</li>
</ol>

<p>Here‚Äôs how this elegant solution works:</p>
<ol>
  <li>$X^TX$ creates a $(p \times p)$ matrix of feature products:
    <ul>
      <li>Each element $(i,j)$ contains the dot product between features $i$ and $j$</li>
      <li>When features are centred, these products are proportional to covariances<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></li>
      <li>When features are also standardised, it yields correlations scaled by $n$</li>
    </ul>
  </li>
  <li>$(X^TX)^{-1}$ computes the inverse of this matrix:
    <ul>
      <li>Compensates for feature correlations in coefficient calculations<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></li>
      <li>Required for solving the normal equations $X^TX\beta = X^Ty$</li>
      <li>Exists only when no feature is a linear combination of others</li>
    </ul>
  </li>
  <li>$X^Ty$ creates a $(p \times 1)$ vector of feature-target products:
    <ul>
      <li>Each element $i$ contains the dot product of feature $i$ with target $y$</li>
      <li>Represents raw feature-target relationships before adjustment</li>
      <li>When centred, proportional to feature-target covariances<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></li>
    </ul>
  </li>
  <li>Final multiplication $(X^TX)^{-1}X^Ty$:
    <ul>
      <li>Solves the normal equations $X^TX\beta = X^Ty$</li>
      <li>Accounts for inter-feature correlations in determining coefficients</li>
      <li>Mathematically guarantees minimum squared error</li>
    </ul>
  </li>
</ol>

<p>For more information, check Hastie, Tibshirani &amp; Friedman‚Äôs ‚Äú<a href="https://archive.org/details/elementsofstatis0000hast">Elements of Statistical Learning</a>‚Äù seminal book.</p>

<h2 id="the-real-world-catch">The Real-World Catch</h2>
<p>While mathematically elegant, this direct solution has practical limitations in real-world applications:</p>
<ol>
  <li><em>Computational Complexity</em>: Computing $(X^TX)^{-1}$ requires $\Omicron(n^3)$ operations, becoming prohibitively expensive for large feature sets. This is why gradient descent, with its $\Omicron(n^2)$  per-iteration complexity, often proves more practical.</li>
  <li><em>Numerical Instability</em>: When features are highly correlated (like monthly and annual income), $X^TX$ becomes nearly singular<sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>. Even small rounding errors in the computation of its inverse can lead to large errors in $\beta$. In extreme cases, when features are perfectly correlated, the inverse doesn‚Äôt exist at all. Gradient descent avoids this matrix inversion entirely.</li>
  <li><em>Memory Constraints</em>: Large datasets require holding the entire $X^TX$ matrix in memory, while gradient descent can work with mini-batches, making it more memory-efficient.</li>
</ol>

<h2 id="conclusions">Conclusions</h2>
<p>While this equation brilliantly demonstrates the power of linear algebra in statistics, real-world machine learning often favours gradient descent‚Äôs iterative approach. Think of it as choosing between a perfect GPS route through heavy traffic (direct solution) versus taking smaller, adaptable steps through clear side streets (gradient descent). Both reach the same destination, but the practical path often wins in real-world conditions.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>When features are centred (mean = 0), each product becomes $n$ times the covariance. This means $X^TX$ captures how features vary together, which is crucial because correlated features can lead to unstable coefficients if not accounted for. The relationship between $X^TX$ and covariance comes from the definition of sample covariance: $cov(X_i, X_j) = \frac{1}{n-1}\sum_{k=1}^n (x_{ki} - \bar{x_i})(x_{kj} - \bar{x_j})$. When data is centred, this simplifies to $\frac{1}{n-1}(X^TX)_{ij}$.  $\frac{X^TX}{n-1}$ returns the sample covariance matrix. This matters because a) when features are uncentred, $(X^TX)$ gives the sum of products, b) when centred $\frac{X^TX}{n-1}$ gives covariances, c) when also standardised (std = 1), $\frac{X^TX}{n-1}$ gives correlations.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Adjusts coefficient estimates to account for shared information between features. For example, if height and weight are correlated, we need to determine each variable‚Äôs unique contribution to the prediction, not their overlapping effect.¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>A matrix is singular (or non-invertible) when its determinant is zero. In practical terms, this means one or more columns can be expressed as linear combinations of other columns.¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="data-science" /><category term="machine-learning" /><category term="statistics" /><category term="ai" /><category term="linear-algebra" /><category term="til" /><category term="modelling-mindsets" /><category term="data-modeling" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üíä Lessons for Modern Drug Development from the Golden Age of Antibiotics</title><link href="http://0.0.0.0:4000/golden-age-of-antibiotics/" rel="alternate" type="text/html" title="üíä Lessons for Modern Drug Development from the Golden Age of Antibiotics" /><published>2025-01-07T00:00:00+00:00</published><updated>2025-01-07T00:00:00+00:00</updated><id>http://0.0.0.0:4000/golden-age-of-antibiotics</id><content type="html" xml:base="http://0.0.0.0:4000/golden-age-of-antibiotics/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>In a thought-provoking analysis<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, Our World in Data reveals a striking paradox in medical progress: the most productive period in antibiotic development occurred in the two decades following World War II, with scientific capabilities far more limited than today. This ‚ÄúGolden Age of Antibiotics‚Äù (1940s-1960s) produced nearly two-thirds of the antibiotic drug classes we still rely on<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.<br />
Even more surprisingly, since 1970 -despite exponential advances in computing power and biotechnology- only eight new classes of antibiotics have been approved<sup id="fnref:2:1" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. This indicates a stark decline that threatens the foundation of modern medicine. Traditional screening methods now rediscover existing compounds most of the time rather than finding new ones<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.<br />
Modern tools like genome sequencing and systematic screening methods offer unprecedented capabilities. We‚Äôve only identified a small fraction of bacterial species, many of which could harbour new antibiotic compounds<sup id="fnref:2:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Yet despite these capabilities, development has stagnated due to fundamental market failures and fragmented research efforts.<br />
This article examines this paradoxical inverse relationship between technological capability and antibiotic development: How did the Golden Age achieve such remarkable success with limited tools? Why has progress slowed as our capabilities have grown? Most importantly, what combinations of economic incentives and modern technology could spark a new era of antibiotic discovery?</p>

<h2 id="when-urgency-met-innovation">When Urgency Met Innovation</h2>

<p>The Golden Age of Antibiotics stands as medicine‚Äôs most productive period in antimicrobial discovery, yielding over 20 new antibiotic classes -more than double what we‚Äôve developed in the 50 years since<sup id="fnref:2:3" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Three pivotal breakthroughs, coupled with unprecedented coordination, drove this remarkable success.<br />
The foundation was laid by Paul Ehrlich‚Äôs systematic approach to drug discovery. By methodically testing hundreds of compounds, he discovered <a href="https://en.wikipedia.org/wiki/Arsphenamine">salvarsan</a> in 1910 -the first synthetic antibiotic that effectively treated syphilis<sup id="fnref:2:4" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. A second milestone emerged when Alexander Fleming discovered penicillin in 1928. However, the real innovation came through coordinated wartime effort. With infections being the second-most common cause of hospital admissions in the US Army, the U.S. Office of Scientific Research and Development (OSRD) launched a global search for more productive penicillin strains, ultimately finding a high-yielding strain on a cantaloupe<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>.<br />
The third breakthrough came from Selman Waksman‚Äôs insight into soil bacteria. His discovery that soil-dwelling <a href="https://en.wikipedia.org/wiki/Actinomycetales">actinomycetes</a> bacteria naturally produce antibiotics led to <a href="https://en.wikipedia.org/wiki/Streptomycin">streptomycin</a>‚Äôs development and opened an entirely new avenue for antibiotic discovery<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>.<br />
What transformed these breakthroughs into a ‚Äúgolden age‚Äù was unprecedented coordination. The U.S. War Production Board orchestrated collaboration between government, academia, and industry -removing patent restrictions, sharing data, and streamlining clinical trials<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>. The results were remarkable: some antibiotics, like <a href="https://en.wikipedia.org/wiki/Tetracycline_antibiotics">tetracyclines</a> and <a href="https://en.wikipedia.org/wiki/Macrolide">macrolides</a>, went from discovery to clinical use within the same year.</p>

<h2 id="scientific-progress-and-market-failure">Scientific Progress and Market Failure</h2>

<p>The contrast between the Golden Age and our current era reflects a fundamental misalignment between public health needs and market incentives<sup id="fnref:2:5" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. The market structure fundamentally disfavours antibiotics in two ways:</p>
<ol>
  <li>Revenue Structure: While chronic disease medications can generate billions in annual revenue over decades, new antibiotics typically generate only tens of millions annually<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>, by comparison. This revenue gap has driven many large pharmaceutical companies away from antibiotic development<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup>.</li>
  <li>Conservation Requirements: New antibiotics must be reserved for severe drug-resistant infections, reaching less than 1% of hospitalised patients<sup id="fnref:7:1" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>. This necessary conservation practice severely limits market potential.</li>
</ol>

<p>Meanwhile, our technological capabilities offer three particularly promising approaches:</p>
<ol>
  <li>Genome mining: a breakthrough technique that identifies hidden antibiotic genes in microbes that remain dormant under standard laboratory conditions. This computational approach has already yielded promising candidates like humimycins<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>.</li>
  <li>Advanced bacterial exploration: research into extreme environments like deep oceans and deserts, where previously ‚Äúunculturable‚Äù bacteria might harbour entirely new antibiotic classes<sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.</li>
  <li>Smart combination strategies: exploiting the observation that bacterial resistance to one antibiotic can increase vulnerability to others, opening new therapeutic possibilities<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>.</li>
</ol>

<p>Yet these powerful tools remain underutilised due to insufficient investment and coordination. The challenge isn‚Äôt scientific capability -it‚Äôs the failure to create systems that effectively deploy these technologies within sustainable economic frameworks.</p>

<h2 id="integrating-economics-and-technology">Integrating Economics and Technology</h2>

<p>Drawing from evidence in antibiotic development research, several promising approaches could help overcome current market failures while leveraging modern technological capabilities<sup id="fnref:7:2" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>.</p>

<h3 id="economic-solutions-to-market-failures">Economic Solutions to Market Failures</h3>

<ol>
  <li>Subscription Models: The UK has pioneered a system where healthcare systems pay annual fees for antibiotic access rather than per-volume pricing. This addresses both the revenue challenge and conservation requirements by providing stable income while supporting appropriate antibiotic use<sup id="fnref:7:3" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>.</li>
  <li>Advance Market Commitments: These provide guaranteed payments to companies that successfully develop new antibiotics, similar to successful vaccine development programs. This directly addresses the revenue uncertainty that has driven companies away from antibiotic development<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup>.</li>
  <li>Collaborative Funding Initiatives: Organisations like CARB-X and GARDP help smaller companies navigate costly clinical trials, distributing development risks that large pharmaceutical companies are unwilling to bear<sup id="fnref:7:4" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>.</li>
</ol>

<h3 id="leveraging-modern-technology">Leveraging Modern Technology</h3>

<p>To maximise the impact of these economic incentives, three technological approaches show particular promise:</p>
<ol>
  <li>Systematic Genome Mining: Using computational power to identify promising antibiotic-producing genes in bacterial genomes, revealing compounds that traditional screening would miss<sup id="fnref:9:1" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>.</li>
  <li>Environmental Exploration: Research into extreme environments could unlock entirely new antibiotic classes, enabled by modern sequencing technologies<sup id="fnref:3:2" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.</li>
  <li>Smart Combination Strategies: Systematic exploration of how resistance to one antibiotic can increase vulnerability to others, offering new therapeutic possibilities<sup id="fnref:10:1" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>The story of antibiotic development demonstrates that scientific capability alone cannot drive progress. The Golden Age succeeded through a powerful combination of systematic approaches, unprecedented collaboration, and removal of institutional barriers -even with limited technological tools<sup id="fnref:2:6" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.<br />
Today‚Äôs challenge is fundamentally different. We possess sophisticated tools -from genome mining to advanced screening methods- yet development has stalled. This paradox reveals that progress requires three key elements working in concert: economic incentives, institutional coordination, and technological application<sup id="fnref:7:5" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>.<br />
The evidence-based solutions presented in the original Our World in Data article<sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> offer a path forward. Market reforms like subscription models and advance market commitments could help correct the fundamental economic misalignment in antibiotic development<sup id="fnref:8:1" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup>. Meanwhile, systematic application of computational tools, genomic analysis, and bacterial exploration could help unlock new classes of antibiotics that traditional methods miss<sup id="fnref:9:2" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>.<br />
The urgency is clear. Antimicrobial resistance threatens to undermine many advances in modern medicine<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">12</a></sup>. However, by combining proven coordination approaches from the Golden Age with modern capabilities and sustainable economic frameworks, we can revitalise antibiotic development for the challenges ahead.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Our World in Data (2024). ‚ÄúWhat was the Golden Age of Antibiotics, and how can we spark a new one?‚Äù <a href="https://ourworldindata.org/golden-age-antibiotics">https://ourworldindata.org/golden-age-antibiotics</a>¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Hutchings, M. I., Truman, A. W., &amp; Wilkinson, B. (2019). Antibiotics: Past, present and future. Current Opinion in Microbiology, 51, 72‚Äì80. <a href="https://doi.org/10.1016/j.mib.2019.10.008">https://doi.org/10.1016/j.mib.2019.10.008</a>¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a>¬†<a href="#fnref:2:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a>¬†<a href="#fnref:2:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a>¬†<a href="#fnref:2:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a>¬†<a href="#fnref:2:5" class="reversefootnote" role="doc-backlink">&#8617;<sup>6</sup></a>¬†<a href="#fnref:2:6" class="reversefootnote" role="doc-backlink">&#8617;<sup>7</sup></a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Kolter, R., &amp; Van Wezel, G. P. (2016). Goodbye to brute force in antibiotic discovery? Nature Microbiology, 1(2), 15020. <a href="https://doi.org/10.1038/nmicrobiol.2015.20">https://doi.org/10.1038/nmicrobiol.2015.20</a>¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a>¬†<a href="#fnref:3:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Gaynes, R. (2017). The Discovery of Penicillin -New Insights After More Than 75 Years of Clinical Use. Emerging Infectious Diseases, 23(5), 849‚Äì853. <a href="https://doi.org/10.3201/eid2305.161556">https://doi.org/10.3201/eid2305.161556</a>¬†<a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Waksman, S. A., &amp; Schatz, A. (1945). Streptomycin‚ÄìOrigin, Nature, and Properties. Journal of the American Pharmaceutical Association, 34(11), 273‚Äì291. <a href="https://doi.org/10.1002/jps.3030341102">https://doi.org/10.1002/jps.3030341102</a>¬†<a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Sampat, B. N. (2023). Second World War and the Direction of Medical Innovation. SSRN Electronic Journal. <a href="https://doi.org/10.2139/ssrn.4422261">https://doi.org/10.2139/ssrn.4422261</a>¬†<a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>√Ördal, C., et al. (2020). Antibiotic development -economic, regulatory and societal challenges. Nature Reviews Microbiology, 18(5), 267-274. <a href="https://doi.org/10.1038/s41579-019-0293-3">https://doi.org/10.1038/s41579-019-0293-3</a>¬†<a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:7:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a>¬†<a href="#fnref:7:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a>¬†<a href="#fnref:7:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a>¬†<a href="#fnref:7:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a>¬†<a href="#fnref:7:5" class="reversefootnote" role="doc-backlink">&#8617;<sup>6</sup></a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>Renwick, M. J., Brogan, D. M., &amp; Mossialos, E. (2016). A systematic review and critical assessment of incentive strategies for discovery and development of novel antibiotics. The Journal of Antibiotics, 69(2), 73-88. <a href="https://doi.org/10.1038/ja.2015.98">https://doi.org/10.1038/ja.2015.98</a>¬†<a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:8:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>Chu, J., et al. (2016). Discovery of MRSA active antibiotics using primary sequence from the human microbiome. Nature Chemical Biology, 12(12), 1004-1006. <a href="https://doi.org/10.1038/nchembio.2207">https://doi.org/10.1038/nchembio.2207</a>¬†<a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:9:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a>¬†<a href="#fnref:9:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>Baym, M., Stone, L. K., &amp; Kishony, R. (2016). Multidrug evolutionary strategies to reverse antibiotic resistance. Science, 351(6268), aad3292. <a href="https://doi.org/10.1126/science.aad3292">https://doi.org/10.1126/science.aad3292</a>¬†<a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:10:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>Kremer, M., Levin, J., &amp; Snyder, C. M. (2020). Advance Market Commitments: Insights from Theory and Experience. AEA Papers and Proceedings, 110, 269-273. <a href="https://www.aeaweb.org/articles?id=10.1257/pandp.20201017">https://www.aeaweb.org/articles?id=10.1257/pandp.20201017</a>¬†<a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12" role="doc-endnote">
      <p>World Health Organization (2024). Antimicrobial Resistance Fact Sheet. <a href="https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance">https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance</a>¬†<a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="iterative-refinement" /><category term="evolution" /><category term="data-science" /><category term="evaluation" /><category term="decision-making" /><category term="best-practices" /><category term="modelling-mindsets" /><category term="production" /><summary type="html"><![CDATA[]]></summary></entry></feed>