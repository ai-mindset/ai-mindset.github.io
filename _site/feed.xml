<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2025-01-08T18:10:30+00:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Just-in-Time learning</title><subtitle>Inquisitive. Learning. Sharing. Simplicity = Reliability</subtitle><entry><title type="html">üí° TIL: How Different Societies View and Value Choice</title><link href="http://0.0.0.0:4000/TIL-the-art-of-choice/" rel="alternate" type="text/html" title="üí° TIL: How Different Societies View and Value Choice" /><published>2025-01-08T00:00:00+00:00</published><updated>2025-01-08T00:00:00+00:00</updated><id>http://0.0.0.0:4000/TIL-the-art-of-choice</id><content type="html" xml:base="http://0.0.0.0:4000/TIL-the-art-of-choice/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>Today I revisited a talk on <a href="https://www.youtube.com/watch?v=lDq9-QxvsNU">the art of choosing</a> by Sheena Iyengar. A humourous and informative presentation, it reminded me that our assumptions about choice ‚Äìas studied by Prof. Iyengar through research spanning American, European and Asian populations‚Äì reveals fascinating cultural differences in how we perceive and respond to choice. Her research reveals some eye-opening insights that I‚Äôll briefly summarise below.</p>

<h2 id="perceiving-choice">Perceiving Choice</h2>
<p>First, while Americans believe individual choice is sacred (think ‚Äúhave it your way‚Äù), research shows this isn‚Äôt universal. When studying children solving puzzles, Asian-American children actually performed better when their mothers chose for them, while Anglo-American children did better choosing for themselves. This reveals how deeply cultural context shapes not just our preferences, but the actual effectiveness of our choices.</p>

<p>Second, remember how overwhelming it feels staring at 50 different breakfast cereals? Turns out, people from post-communist countries often saw seven different sodas as just one choice: ‚Äúsoda or no soda.‚Äù This isn‚Äôt because they‚Äôre less sophisticated, it‚Äôs because the ability to spot tiny differences between products is a learned skill -not a natural one.</p>

<p>Most striking was the research on medical decisions. When comparing American and French parents making end-of-life decisions for infants, American parents had more negative emotions and guilt despite insisting on having the choice, while French parents, whose doctors made the decisions, coped better. This challenges the core American belief that having choice is always better.</p>

<p>Concluding with a personal story, Prof. Iyengar -who is blind- shared how she once brought two ‚Äúclearly different‚Äù shades of pink nail polish to her lab. When she removed the labels, half the participants couldn‚Äôt tell them apart. Those who could, chose differently when the labels were present versus absent, showing how marketing narratives shape what we think we‚Äôre choosing.</p>

<h2 id="conclusions">Conclusions</h2>
<p>The TL;DR is: Through cross-cultural research, Prof. Iyengar shows that how we understand and value choice varies dramatically across cultures. Sometimes, having fewer choices or letting others choose for us might actually lead to better outcomes. <br />
As a technologist, inundated with a very wide choice of tools that often offer similar results, I have made the conscious decision to reduce my tooling footprint to the minimum viable toolstack possible. I‚Äôm happy to let more knowledgeable professionals choose, with <em>adequate justification</em>, tools for my line of work but I do disagree with the zealotry that‚Äôs occasionally observed in tech and complemented by big egos.</p>]]></content><author><name></name></author><category term="til" /><category term="decision-making" /><category term="best-practices" /><category term="evaluation" /><category term="statistics" /><category term="design-principles" /><category term="modelling-mindsets" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üí° TIL: The Matrix Equation That Makes Linear Regression Work</title><link href="http://0.0.0.0:4000/TIL-lin-alg-applied-to-stats/" rel="alternate" type="text/html" title="üí° TIL: The Matrix Equation That Makes Linear Regression Work" /><published>2025-01-08T00:00:00+00:00</published><updated>2025-01-08T00:00:00+00:00</updated><id>http://0.0.0.0:4000/TIL-lin-alg-applied-to-stats</id><content type="html" xml:base="http://0.0.0.0:4000/TIL-lin-alg-applied-to-stats/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>Today <a href="https://xcancel.com/andrew_n_carr/status/1876855682529480844">I remembered</a> how the equation $\beta = (X^TX)^{-1}X^Ty$ elegantly solves linear regression using matrix algebra. Here‚Äôs why it‚Äôs brilliant:</p>

<h2 id="what-it-solves">What it Solves</h2>
<p>Linear regression finds the best-fit line through data points by finding optimal coefficients ($\beta$) that minimise squared errors.</p>

<h2 id="the-components">The Components</h2>
<ol>
  <li>$X$ is our feature matrix (n samples √ó p features)</li>
  <li>$y$ is our target values (n √ó 1)</li>
  <li>$X^T$ is the transpose of X</li>
  <li>$\beta$ is our solution vector (p √ó 1) of coefficients</li>
</ol>

<h2 id="how-it-works">How it Works</h2>
<ol>
  <li>$X^TX$ creates a (p √ó p) correlation matrix:
    <ul>
      <li>Normalises input features</li>
      <li>Captures feature relationships</li>
      <li>Makes the problem solvable</li>
    </ul>
  </li>
  <li>$(X^TX)^{-1}$ inverts this matrix:
    <ul>
      <li>Acts as a normalising factor</li>
      <li>Corrects for feature correlations</li>
      <li>Only exists if features are linearly independent</li>
    </ul>
  </li>
  <li>$X^Ty$ projects targets into feature space:
    <ul>
      <li>Maps target values to feature relationships</li>
      <li>Dimensions: (p √ó 1)</li>
    </ul>
  </li>
  <li>Final multiplication gives $\beta$:
    <ul>
      <li>Each element is the optimal weight for a feature</li>
      <li>Minimises sum of squared residuals</li>
      <li>Solves $\min_\beta ||X\beta - y||^2$</li>
    </ul>
  </li>
</ol>

<h2 id="conclusions">Conclusions</h2>
<p>This single equation replaces iterative optimisation with a direct solution, returning the globally optimal coefficients in one step. It‚Äôs the effectiveness of linear algebra applied to statistics. You can think of it like having a GPS for finding the best-fit line. Instead of wandering around hoping to stumble upon it, we calculate the exact destination mathematically.</p>]]></content><author><name></name></author><category term="data-science" /><category term="machine-learning" /><category term="statistics" /><category term="ai" /><category term="linear-algebra" /><category term="til" /><category term="modelling-mindsets" /><category term="data-modeling" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üíä Lessons for Modern Drug Development from the Golden Age of Antibiotics</title><link href="http://0.0.0.0:4000/golden-age-of-antibiotics/" rel="alternate" type="text/html" title="üíä Lessons for Modern Drug Development from the Golden Age of Antibiotics" /><published>2025-01-07T00:00:00+00:00</published><updated>2025-01-07T00:00:00+00:00</updated><id>http://0.0.0.0:4000/golden-age-of-antibiotics</id><content type="html" xml:base="http://0.0.0.0:4000/golden-age-of-antibiotics/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>In a thought-provoking analysis<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, Our World in Data reveals a striking paradox in medical progress: the most productive period in antibiotic development occurred in the two decades following World War II, with scientific capabilities far more limited than today. This ‚ÄúGolden Age of Antibiotics‚Äù (1940s-1960s) produced nearly two-thirds of the antibiotic drug classes we still rely on<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.<br />
Even more surprisingly, since 1970 -despite exponential advances in computing power and biotechnology- only eight new classes of antibiotics have been approved<sup id="fnref:2:1" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. This indicates a stark decline that threatens the foundation of modern medicine. Traditional screening methods now rediscover existing compounds most of the time rather than finding new ones<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.<br />
Modern tools like genome sequencing and systematic screening methods offer unprecedented capabilities. We‚Äôve only identified a small fraction of bacterial species, many of which could harbour new antibiotic compounds<sup id="fnref:2:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Yet despite these capabilities, development has stagnated due to fundamental market failures and fragmented research efforts.<br />
This article examines this paradoxical inverse relationship between technological capability and antibiotic development: How did the Golden Age achieve such remarkable success with limited tools? Why has progress slowed as our capabilities have grown? Most importantly, what combinations of economic incentives and modern technology could spark a new era of antibiotic discovery?</p>

<h2 id="when-urgency-met-innovation">When Urgency Met Innovation</h2>

<p>The Golden Age of Antibiotics stands as medicine‚Äôs most productive period in antimicrobial discovery, yielding over 20 new antibiotic classes -more than double what we‚Äôve developed in the 50 years since<sup id="fnref:2:3" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Three pivotal breakthroughs, coupled with unprecedented coordination, drove this remarkable success.<br />
The foundation was laid by Paul Ehrlich‚Äôs systematic approach to drug discovery. By methodically testing hundreds of compounds, he discovered <a href="https://en.wikipedia.org/wiki/Arsphenamine">salvarsan</a> in 1910 -the first synthetic antibiotic that effectively treated syphilis<sup id="fnref:2:4" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. A second milestone emerged when Alexander Fleming discovered penicillin in 1928. However, the real innovation came through coordinated wartime effort. With infections being the second-most common cause of hospital admissions in the US Army, the U.S. Office of Scientific Research and Development (OSRD) launched a global search for more productive penicillin strains, ultimately finding a high-yielding strain on a cantaloupe<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>.<br />
The third breakthrough came from Selman Waksman‚Äôs insight into soil bacteria. His discovery that soil-dwelling <a href="https://en.wikipedia.org/wiki/Actinomycetales">actinomycetes</a> bacteria naturally produce antibiotics led to <a href="https://en.wikipedia.org/wiki/Streptomycin">streptomycin</a>‚Äôs development and opened an entirely new avenue for antibiotic discovery<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>.<br />
What transformed these breakthroughs into a ‚Äúgolden age‚Äù was unprecedented coordination. The U.S. War Production Board orchestrated collaboration between government, academia, and industry -removing patent restrictions, sharing data, and streamlining clinical trials<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>. The results were remarkable: some antibiotics, like <a href="https://en.wikipedia.org/wiki/Tetracycline_antibiotics">tetracyclines</a> and <a href="https://en.wikipedia.org/wiki/Macrolide">macrolides</a>, went from discovery to clinical use within the same year.</p>

<h2 id="scientific-progress-and-market-failure">Scientific Progress and Market Failure</h2>

<p>The contrast between the Golden Age and our current era reflects a fundamental misalignment between public health needs and market incentives<sup id="fnref:2:5" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. The market structure fundamentally disfavours antibiotics in two ways:</p>
<ol>
  <li>Revenue Structure: While chronic disease medications can generate billions in annual revenue over decades, new antibiotics typically generate only tens of millions annually<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>, by comparison. This revenue gap has driven many large pharmaceutical companies away from antibiotic development<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup>.</li>
  <li>Conservation Requirements: New antibiotics must be reserved for severe drug-resistant infections, reaching less than 1% of hospitalised patients<sup id="fnref:7:1" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>. This necessary conservation practice severely limits market potential.</li>
</ol>

<p>Meanwhile, our technological capabilities offer three particularly promising approaches:</p>
<ol>
  <li>Genome mining: a breakthrough technique that identifies hidden antibiotic genes in microbes that remain dormant under standard laboratory conditions. This computational approach has already yielded promising candidates like humimycins<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>.</li>
  <li>Advanced bacterial exploration: research into extreme environments like deep oceans and deserts, where previously ‚Äúunculturable‚Äù bacteria might harbour entirely new antibiotic classes<sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.</li>
  <li>Smart combination strategies: exploiting the observation that bacterial resistance to one antibiotic can increase vulnerability to others, opening new therapeutic possibilities<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>.</li>
</ol>

<p>Yet these powerful tools remain underutilised due to insufficient investment and coordination. The challenge isn‚Äôt scientific capability -it‚Äôs the failure to create systems that effectively deploy these technologies within sustainable economic frameworks.</p>

<h2 id="integrating-economics-and-technology">Integrating Economics and Technology</h2>

<p>Drawing from evidence in antibiotic development research, several promising approaches could help overcome current market failures while leveraging modern technological capabilities<sup id="fnref:7:2" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>.</p>

<h3 id="economic-solutions-to-market-failures">Economic Solutions to Market Failures</h3>

<ol>
  <li>Subscription Models: The UK has pioneered a system where healthcare systems pay annual fees for antibiotic access rather than per-volume pricing. This addresses both the revenue challenge and conservation requirements by providing stable income while supporting appropriate antibiotic use<sup id="fnref:7:3" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>.</li>
  <li>Advance Market Commitments: These provide guaranteed payments to companies that successfully develop new antibiotics, similar to successful vaccine development programs. This directly addresses the revenue uncertainty that has driven companies away from antibiotic development<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup>.</li>
  <li>Collaborative Funding Initiatives: Organisations like CARB-X and GARDP help smaller companies navigate costly clinical trials, distributing development risks that large pharmaceutical companies are unwilling to bear<sup id="fnref:7:4" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>.</li>
</ol>

<h3 id="leveraging-modern-technology">Leveraging Modern Technology</h3>

<p>To maximise the impact of these economic incentives, three technological approaches show particular promise:</p>
<ol>
  <li>Systematic Genome Mining: Using computational power to identify promising antibiotic-producing genes in bacterial genomes, revealing compounds that traditional screening would miss<sup id="fnref:9:1" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>.</li>
  <li>Environmental Exploration: Research into extreme environments could unlock entirely new antibiotic classes, enabled by modern sequencing technologies<sup id="fnref:3:2" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.</li>
  <li>Smart Combination Strategies: Systematic exploration of how resistance to one antibiotic can increase vulnerability to others, offering new therapeutic possibilities<sup id="fnref:10:1" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>The story of antibiotic development demonstrates that scientific capability alone cannot drive progress. The Golden Age succeeded through a powerful combination of systematic approaches, unprecedented collaboration, and removal of institutional barriers -even with limited technological tools<sup id="fnref:2:6" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.<br />
Today‚Äôs challenge is fundamentally different. We possess sophisticated tools -from genome mining to advanced screening methods- yet development has stalled. This paradox reveals that progress requires three key elements working in concert: economic incentives, institutional coordination, and technological application<sup id="fnref:7:5" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>.<br />
The evidence-based solutions presented in the original Our World in Data article<sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> offer a path forward. Market reforms like subscription models and advance market commitments could help correct the fundamental economic misalignment in antibiotic development<sup id="fnref:8:1" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup>. Meanwhile, systematic application of computational tools, genomic analysis, and bacterial exploration could help unlock new classes of antibiotics that traditional methods miss<sup id="fnref:9:2" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>.<br />
The urgency is clear. Antimicrobial resistance threatens to undermine many advances in modern medicine<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">12</a></sup>. However, by combining proven coordination approaches from the Golden Age with modern capabilities and sustainable economic frameworks, we can revitalise antibiotic development for the challenges ahead.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Our World in Data (2024). ‚ÄúWhat was the Golden Age of Antibiotics, and how can we spark a new one?‚Äù <a href="https://ourworldindata.org/golden-age-antibiotics">https://ourworldindata.org/golden-age-antibiotics</a>¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Hutchings, M. I., Truman, A. W., &amp; Wilkinson, B. (2019). Antibiotics: Past, present and future. Current Opinion in Microbiology, 51, 72‚Äì80. <a href="https://doi.org/10.1016/j.mib.2019.10.008">https://doi.org/10.1016/j.mib.2019.10.008</a>¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a>¬†<a href="#fnref:2:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a>¬†<a href="#fnref:2:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a>¬†<a href="#fnref:2:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a>¬†<a href="#fnref:2:5" class="reversefootnote" role="doc-backlink">&#8617;<sup>6</sup></a>¬†<a href="#fnref:2:6" class="reversefootnote" role="doc-backlink">&#8617;<sup>7</sup></a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Kolter, R., &amp; Van Wezel, G. P. (2016). Goodbye to brute force in antibiotic discovery? Nature Microbiology, 1(2), 15020. <a href="https://doi.org/10.1038/nmicrobiol.2015.20">https://doi.org/10.1038/nmicrobiol.2015.20</a>¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a>¬†<a href="#fnref:3:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Gaynes, R. (2017). The Discovery of Penicillin -New Insights After More Than 75 Years of Clinical Use. Emerging Infectious Diseases, 23(5), 849‚Äì853. <a href="https://doi.org/10.3201/eid2305.161556">https://doi.org/10.3201/eid2305.161556</a>¬†<a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Waksman, S. A., &amp; Schatz, A. (1945). Streptomycin‚ÄìOrigin, Nature, and Properties. Journal of the American Pharmaceutical Association, 34(11), 273‚Äì291. <a href="https://doi.org/10.1002/jps.3030341102">https://doi.org/10.1002/jps.3030341102</a>¬†<a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Sampat, B. N. (2023). Second World War and the Direction of Medical Innovation. SSRN Electronic Journal. <a href="https://doi.org/10.2139/ssrn.4422261">https://doi.org/10.2139/ssrn.4422261</a>¬†<a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>√Ördal, C., et al. (2020). Antibiotic development -economic, regulatory and societal challenges. Nature Reviews Microbiology, 18(5), 267-274. <a href="https://doi.org/10.1038/s41579-019-0293-3">https://doi.org/10.1038/s41579-019-0293-3</a>¬†<a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:7:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a>¬†<a href="#fnref:7:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a>¬†<a href="#fnref:7:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a>¬†<a href="#fnref:7:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a>¬†<a href="#fnref:7:5" class="reversefootnote" role="doc-backlink">&#8617;<sup>6</sup></a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>Renwick, M. J., Brogan, D. M., &amp; Mossialos, E. (2016). A systematic review and critical assessment of incentive strategies for discovery and development of novel antibiotics. The Journal of Antibiotics, 69(2), 73-88. <a href="https://doi.org/10.1038/ja.2015.98">https://doi.org/10.1038/ja.2015.98</a>¬†<a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:8:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>Chu, J., et al. (2016). Discovery of MRSA active antibiotics using primary sequence from the human microbiome. Nature Chemical Biology, 12(12), 1004-1006. <a href="https://doi.org/10.1038/nchembio.2207">https://doi.org/10.1038/nchembio.2207</a>¬†<a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:9:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a>¬†<a href="#fnref:9:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>Baym, M., Stone, L. K., &amp; Kishony, R. (2016). Multidrug evolutionary strategies to reverse antibiotic resistance. Science, 351(6268), aad3292. <a href="https://doi.org/10.1126/science.aad3292">https://doi.org/10.1126/science.aad3292</a>¬†<a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:10:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>Kremer, M., Levin, J., &amp; Snyder, C. M. (2020). Advance Market Commitments: Insights from Theory and Experience. AEA Papers and Proceedings, 110, 269-273. <a href="https://www.aeaweb.org/articles?id=10.1257/pandp.20201017">https://www.aeaweb.org/articles?id=10.1257/pandp.20201017</a>¬†<a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12" role="doc-endnote">
      <p>World Health Organization (2024). Antimicrobial Resistance Fact Sheet. <a href="https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance">https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance</a>¬†<a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="iterative-refinement" /><category term="evolution" /><category term="data-science" /><category term="evaluation" /><category term="decision-making" /><category term="best-practices" /><category term="modelling-mindsets" /><category term="production" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üí° TIL: Test-Driven Development Is Key to Better LLM System Prompts</title><link href="http://0.0.0.0:4000/TIL-tdd-good-system-prompts/" rel="alternate" type="text/html" title="üí° TIL: Test-Driven Development Is Key to Better LLM System Prompts" /><published>2025-01-02T00:00:00+00:00</published><updated>2025-01-02T00:00:00+00:00</updated><id>http://0.0.0.0:4000/TIL-tdd-good-system-prompts</id><content type="html" xml:base="http://0.0.0.0:4000/TIL-tdd-good-system-prompts/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>2024 has made clear that writing good automated evaluations for LLM-powered systems is the most critical skill for building useful applications. This insight parallels Anthropic‚Äôs internal approach to system prompt development. As usual, Simon Willison‚Äôs <a href="https://simonwillison.net/2024/Dec/31/llms-in-2024/#evals-really-matter">recent insightful 2024 LLM overview</a> was a treasure trove. One item I picked up on was evaluating system prompts using a test-driven approach.</p>

<h2 id="the-evaluation-first-approach">The Evaluation-First Approach</h2>

<p><a href="https://askell.io/">Amanda Askell</a>, leading fine-tuning at Anthropic, <a href="https://xcancel.com/amandaaskell/status/1866207266761760812">outlines a test-driven process</a> for system prompts:</p>
<ol>
  <li>Create a test set of messages where the model‚Äôs default behaviour fails to meet requirements</li>
  <li>Develop a system prompt that passes these tests</li>
  <li>Identify cases where the system prompt is misapplied and refine it</li>
  <li>Expand the test set and repeat</li>
</ol>

<p>This methodology‚Äôs importance extends beyond prompt engineering. Companies with strong evaluation suites can adopt new models faster and build more reliable features than competitors. As <a href="https://xcancel.com/cramforce/status/1860436022347075667">Vercel‚Äôs experience demonstrates</a>, moving from complex prompt protection to robust testing enables rapid iteration and development.</p>

<h2 id="conclusion">Conclusion</h2>

<p>While everyone acknowledges evals‚Äô importance, implementing them effectively remains challenging. The key insight is clear: robust automated evaluation isn‚Äôt just a quality check, it‚Äôs the foundation for building reliable LLM-powered systems.</p>]]></content><author><name></name></author><category term="ai" /><category term="llm" /><category term="til" /><category term="prompt-engineering" /><category term="testing" /><category term="best-practices" /><category term="evaluation" /><category term="machine-learning" /><category term="ai-alignment" /><category term="system-prompts" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üìù From Vim to VSCode to Neovim</title><link href="http://0.0.0.0:4000/vscode-to-neovim/" rel="alternate" type="text/html" title="üìù From Vim to VSCode to Neovim" /><published>2024-12-24T00:00:00+00:00</published><updated>2024-12-24T00:00:00+00:00</updated><id>http://0.0.0.0:4000/vscode-to-neovim</id><content type="html" xml:base="http://0.0.0.0:4000/vscode-to-neovim/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>Vim‚Äôs portable <code class="language-plaintext highlighter-rouge">.vimrc</code> embodies software minimalism at its best. One file, one minute to setup, resulting in a complete development environment. This simplicity served me well until Azure development motivated the use of VSCode.<br />
While VSCode worked reasonably well on macOS, Fedora revealed its constraints: keyboard input failures, heavy resource usage, and <a href="https://stackoverflow.com/questions/35368889/how-can-i-export-settings">complex environment portability</a> compared to Vim‚Äôs <code class="language-plaintext highlighter-rouge">vim +PlugInstall</code>. These limitations drove my search for tools that could maintain simplicity while meeting my development requirements with simplicity and portability in mind.</p>

<h2 id="vim---vscode---neovim">Vim -&gt; VSCode -&gt; Neovim</h2>
<p>Azure development initially pulled me into VSCode‚Äôs ecosystem. While stable on macOS, Fedora revealed deal-breakers: random keyboard input failures that only responded to command palette (Ctrl+Shift+P). No amount of configuration resets or reinstalls resolved these issues.</p>

<p>This instability, coupled with VSCode‚Äôs resource footprint, led me to Neovim. The timing aligned with my exploration of Clojure, where Neovim‚Äôs Conjure plugin offered a compelling Lisp development experience that rivaled Emacs.</p>

<p>My requirements were specific:</p>
<ul>
  <li>A lightweight Python IDE</li>
  <li>A lightweight Deno IDE</li>
  <li>A lightweight Clojure IDE</li>
</ul>

<p>Through <a href="/dialogue-engineering/">Dialogue Engineering</a>, I crafted a complete IDE using a <a href="https://github.com/ai-mindset/init.vim">single configuration file</a>. Neovim‚Äôs mixed ecosystem of package managers and dual Vimscript/Lua support presents a learning curve, but the resulting environment is fast, stable, and precisely tailored to my needs. One minor drawback is the complexity of adding colour to Conjure‚Äôs output, especially when compared to the rich REPL experiences offered by <a href="https://ipython.org/">IPython</a>, <a href="https://deno.com/">Deno</a>, and Clojure with <a href="https://github.com/bhauman/rebel-readline">rebel-readline</a>.</p>

<h2 id="conclusions">Conclusions</h2>
<p>The journey from Vim to VSCode and finally to Neovim reflects a common pattern in software development: sometimes we need to step backward to move forward. While VSCode offered modern IDE features, its stability and resource issues on Linux highlighted the enduring value of minimal, portable tools.<br />
Neovim strikes an elegant balance: it preserves Vim‚Äôs philosophy of simplicity and portability while providing modern IDE capabilities. Despite minor challenges with REPL colourisation, its single configuration file approach and robust plugin ecosystem make it a powerful choice for polyglot development. For developers who value both minimal tooling and modern features, Neovim proves that we don‚Äôt always have to choose between the two.</p>]]></content><author><name></name></author><category term="minimal" /><category term="cross-platform" /><category term="toolchain" /><category term="best-practices" /><category term="design-principles" /><category term="python" /><category term="deno" /><category term="zero-config" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üí° TIL: Exploring OpenAI‚Äôs API with Swagger</title><link href="http://0.0.0.0:4000/TIL-openai-openapi/" rel="alternate" type="text/html" title="üí° TIL: Exploring OpenAI‚Äôs API with Swagger" /><published>2024-12-23T00:00:00+00:00</published><updated>2024-12-23T00:00:00+00:00</updated><id>http://0.0.0.0:4000/TIL-openai-openapi</id><content type="html" xml:base="http://0.0.0.0:4000/TIL-openai-openapi/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>OpenAI maintains a comprehensive <a href="https://github.com/openai/openai-openapi/">OpenAPI specification</a> that documents their entire API surface. While browsing through their GitHub repository, <a href="https://simonwillison.net/">Simon Willison</a><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> discovered you can easily explore this spec using Swagger‚Äôs web interface.</p>

<h2 id="the-discovery">The Discovery</h2>
<p>Willison recently highlighted a neat trick: you can browse OpenAI‚Äôs full API documentation by loading their <a href="https://github.com/openai/openai-openapi/blob/master/openapi.yaml">OpenAPI YAML file</a> directly into <a href="https://petstore.swagger.io/?url=https://raw.githubusercontent.com/openai/openai-openapi/refs/heads/master/openapi.yaml#/">Swagger‚Äôs web UI</a>.</p>

<h2 id="why-this-matters">Why This Matters</h2>
<p>This approach offers several advantages:</p>
<ul>
  <li>Interactive exploration of all API endpoints</li>
  <li>Complete request/response schemas</li>
  <li>Built-in testing capability</li>
  <li>Detailed parameter documentation</li>
</ul>

<p>For developers working with AI APIs, this provides a valuable reference point - especially when building services that need to maintain compatibility with OpenAI‚Äôs API structure.</p>

<h2 id="try-it-yourself">Try It Yourself</h2>
<p>Visit the <a href="https://petstore.swagger.io/">Swagger UI</a> and paste this URL: <br />
<code class="language-plaintext highlighter-rouge">https://raw.githubusercontent.com/openai/openai-openapi/refs/heads/master/openapi.yaml</code></p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Co-founder of <a href="https://blog.natbat.net/post/61658401806/lanyrd-from-idea-to-exit">Lanyrd</a>, co-creator of <a href="https://simonwillison.net/2005/Jul/17/django/">Django</a> and <a href="https://datasette.io/">Datasette</a> and a prolific independent AI researcher¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="ai" /><category term="llm" /><category term="openai" /><category term="openapi" /><category term="spec" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üéõÔ∏è A Practical Guide to Fine-tuning LLMs with InstructLab</title><link href="http://0.0.0.0:4000/instructlab-and-rag/" rel="alternate" type="text/html" title="üéõÔ∏è A Practical Guide to Fine-tuning LLMs with InstructLab" /><published>2024-12-19T00:00:00+00:00</published><updated>2024-12-19T00:00:00+00:00</updated><id>http://0.0.0.0:4000/instructlab-and-rag</id><content type="html" xml:base="http://0.0.0.0:4000/instructlab-and-rag/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>The explosion of Large Language Models (LLMs) has created a pressing need for domain-specific adaptations. While base models like GPT-4, Claude, and Llama demonstrate impressive general capabilities, organisations often need models that excel in specific domains or exhibit particular behavioural traits. This customisation typically requires fine-tuning, a process that has historically demanded significant expertise, computational resources, and sophisticated infrastructure.</p>

<h3 id="the-fine-tuning-challenge">The Fine-tuning Challenge</h3>

<p>Traditional LLM fine-tuning presents a complex web of interconnected challenges that organisations must navigate. At its core lies the need for sophisticated infrastructure, often requiring specialised hardware and carefully orchestrated software stacks. This infrastructure challenge is compounded by substantial computational costs, making experimentation and iteration expensive.<br />
The data challenge is equally significant. Fine-tuning demands large, high-quality datasets that are both rare and expensive to create. Even when such datasets exist, organisations face the risk of catastrophic forgetting, where models lose their general capabilities while acquiring new ones. Moreover, validating improvements remains a complex task, requiring careful benchmarking and evaluation frameworks.<br />
These challenges have historically restricted fine-tuning to well-resourced organisations, creating a significant barrier to entry for smaller teams and organisations seeking to adapt LLMs to their specific needs.</p>

<h3 id="real-world-challenges">Real-world Challenges</h3>

<p>The adaptation of LLMs to specific domains presents organisations with a multifaceted set of practical challenges. In healthcare, medical institutions grapple with the need for models that can accurately process and generate content using complex medical terminology while maintaining strict clinical protocols. This domain expertise challenge extends beyond mere vocabulary; it encompasses understanding of medical procedures, drug interactions, and diagnostic reasoning.<br />
The financial sector faces equally demanding requirements, particularly around compliance and regulation. Banks and financial institutions must ensure their models operate within specific regulatory frameworks, making decisions that are not only accurate but also auditable and explainable to regulatory bodies.<br />
Data quality emerges as a persistent challenge across sectors. Organisations typically struggle with historical datasets that exhibit inconsistent formatting, missing values, and inherent biases. The challenge extends to maintaining proper version control and data lineage tracking, crucial for both compliance and model improvement cycles.<br />
Regulatory constraints add another layer of complexity. Healthcare organisations must ensure strict HIPAA compliance in their model development and deployment processes. Similarly, any organisation handling European data must adhere to GDPR requirements, while specific industries often face additional certification needs. These regulatory requirements must be considered not just in the final deployment but throughout the entire fine-tuning process.</p>

<h3 id="the-role-of-instructlab">The Role of InstructLab</h3>

<p><a href="https://instructlab.ai/">InstructLab</a> emerges as a systematic solution to these challenges, offering a novel approach to LLM fine-tuning that combines:</p>
<ul>
  <li>Synthetic data generation for high-quality training examples</li>
  <li>Efficient <a href="https://arxiv.org/abs/2305.14314">QLoRA</a>-based training pipelines</li>
  <li>Comprehensive evaluation frameworks</li>
  <li>Hardware-adaptive processing</li>
</ul>

<p>The rest of this article will elaborate on <a href="https://instructlab.ai/">InstructLab</a>‚Äôs architecture, workflow, and practical considerations, demonstrating how it makes LLM fine-tuning accessible while maintaining rigorous quality standards. It will explore how organisations can leverage this tool to enhance their AI capabilities efficiently and systematically.</p>

<h2 id="from-principles-to-practice">From Principles to Practice</h2>

<p><a href="https://instructlab.ai/">InstructLab</a> is built around the LAB (Large-Scale Alignment for ChatBots) methodology, leveraging [QLoRA(https://arxiv.org/abs/2305.14314) (Quantized Low-Rank Adaptation) for efficient fine-tuning. The system requires Python 3.10/3.11 and approximately 500GB of disc space for full operation.</p>

<h3 id="architectural-components">Architectural Components</h3>

<p>The system operates through three primary components:</p>
<ul>
  <li><strong>Taxonomy Repository</strong>: A structured collection of knowledge and skills, organised in YAML files (max 2300 words per Q&amp;A pair)</li>
  <li><strong>Synthetic Data Generator</strong>: Uses a teacher model (default: Mixtral/Mistral instruct for full pipeline, Merlinite 7b for simple) to transform taxonomy entries into diverse training examples</li>
  <li><strong>Training Pipeline System</strong>: <a href="https://arxiv.org/abs/2305.14314">QLoRA</a>-based training options optimised for different hardware configurations</li>
</ul>

<h3 id="training-pipelines">Training Pipelines</h3>

<p><a href="https://instructlab.ai/">InstructLab</a> offers three specialised training pipelines:</p>

<ol>
  <li><strong>Simple Pipeline</strong>
    <ul>
      <li>Fast training (~1 hour)</li>
      <li>Uses SFT Trainer (Linux) or MLX (MacOS)</li>
      <li>Great for initial experiments and validation</li>
    </ul>
  </li>
  <li><strong>Full Pipeline</strong>
    <ul>
      <li>Custom <a href="https://arxiv.org/abs/2305.14314">QLoRA</a> training loop optimised for CPU/MPS</li>
      <li>Enhanced data processing functions</li>
      <li>Memory requirement: 32GB RAM</li>
      <li>Balanced performance and accessibility</li>
    </ul>
  </li>
  <li><strong>Accelerated Pipeline</strong>
    <ul>
      <li>GPU-accelerated distributed <a href="https://arxiv.org/abs/2305.14314">QLoRA</a> training</li>
      <li>Supports NVIDIA CUDA and AMD ROCm</li>
      <li>Requires 18GB+ GPU memory</li>
      <li>Ideal for production-grade fine-tuning</li>
    </ul>
  </li>
</ol>

<h3 id="hardware-support-and-quantisation">Hardware Support and Quantisation</h3>

<p><a href="https://instructlab.ai/">InstructLab</a> supports various hardware configurations with automatic quantisation:</p>
<ul>
  <li>Apple M-series chips: MLX optimisation, MPS acceleration</li>
  <li>NVIDIA GPUs: CUDA support, 4-bit quantisation available</li>
  <li>AMD GPUs: ROCm support, similar quantisation options</li>
  <li>Standard CPUs: Optimised quantisation for memory efficiency</li>
</ul>

<h2 id="practical-workflow">Practical Workflow</h2>

<p>With the architectural foundation established, <a href="https://instructlab.ai/">InstructLab</a> provides a systematic approach to implementing these components through a straightforward command-line interface. The following sections detail the practical steps to leverage this architecture effectively.</p>

<h3 id="setup-and-installation">Setup and Installation</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>instructlab
ilab config init
</code></pre></div></div>

<p>Key requirements:</p>
<ul>
  <li>Python 3.10 or 3.11 (&gt;=3.12 not supported<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>)</li>
  <li>500GB recommended disc space</li>
  <li>16GB RAM minimum, 32GB recommended</li>
</ul>

<h3 id="core-workflow-steps">Core Workflow Steps</h3>

<ol>
  <li><strong>Model Acquisition</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ilab model download
</code></pre></div>    </div>
    <ul>
      <li>Downloads pre-trained base models</li>
      <li>Supports GGUF (4-bit to 16-bit) and Safetensors formats</li>
      <li>Automatic quantisation with configurable parameters</li>
    </ul>
  </li>
  <li><strong>Synthetic Data Generation</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ilab model serve
ilab data generate <span class="nt">--pipeline</span> <span class="o">[</span>simple|full]
</code></pre></div>    </div>
    <p>Common issues and solutions:</p>
    <ul>
      <li>Server conflicts: Use different ports with <code class="language-plaintext highlighter-rouge">--port</code></li>
      <li>Memory errors: Reduce batch size or use <code class="language-plaintext highlighter-rouge">--pipeline simple</code></li>
      <li>Teacher model issues: Verify model checksum and try re-downloading</li>
    </ul>
  </li>
  <li><strong>Training</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ilab model train
</code></pre></div>    </div>
    <p>Hyperparameters (configurable in config.yaml):</p>
    <ul>
      <li>Max epochs: 10</li>
    </ul>
  </li>
  <li><strong>Evaluation</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ilab model evaluate
</code></pre></div>    </div>
    <p>Benchmarks and typical scores:</p>
    <ul>
      <li><a href="http://en.wikipedia.org/wiki/MMLU">MMLU</a>: Knowledge (0.0-1.0 scale)</li>
      <li>MMLUBranch: Delta improvements</li>
      <li>MTBench: Skills (0.0-10.0 scale)</li>
      <li>MTBenchBranch: Skill improvements</li>
    </ul>
  </li>
</ol>

<h3 id="model-deployment">Model Deployment</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ilab model serve <span class="nt">--model-path</span> &lt;new-model-path&gt;
ilab model chat <span class="nt">-m</span> &lt;new-model-path&gt; <span class="c"># Optionally, chat with the model</span>
</code></pre></div></div>
<p>Deployment considerations:</p>
<ul>
  <li>Verify quantisation level matches hardware capabilities</li>
  <li>Monitor memory usage during serving</li>
  <li>Consider temperature settings for inference (default: 1.0)</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p><a href="https://instructlab.ai/">InstructLab</a> represents a significant advancement in democratising LLM fine-tuning, bridging the gap between research capabilities and practical deployment. Through its innovative LAB methodology and <a href="https://arxiv.org/abs/2305.14314">QLoRA</a>-based implementation, it makes sophisticated model adaptation accessible to practitioners across different hardware configurations.</p>

<h3 id="key-advantages">Key Advantages</h3>

<ul>
  <li><strong>Accessibility</strong>: From laptops to data centres, <a href="https://instructlab.ai/">InstructLab</a> scales with available resources</li>
  <li><strong>Flexibility</strong>: Multiple training pipelines accommodate different needs and constraints</li>
  <li><strong>Systematic</strong>: Structured approach to knowledge and skill injection through taxonomy</li>
  <li><strong>Verifiable</strong>: Comprehensive evaluation suite ensures quality of fine-tuned models</li>
</ul>

<h3 id="practical-impact">Practical Impact</h3>

<p><a href="https://instructlab.ai/">InstructLab</a> enables organisations to:</p>
<ul>
  <li>Create domain-specialised models without massive compute resources</li>
  <li>Systematically inject new capabilities through structured knowledge representation</li>
  <li>Validate improvements through quantitative benchmarks</li>
  <li>Deploy fine-tuned models with minimal operational overhead</li>
</ul>

<h3 id="limitations-and-considerations">Limitations and Considerations</h3>

<ul>
  <li><strong>Model Constraints</strong>: Currently supports models up to 7B parameters effectively</li>
  <li><strong>Resource Timeline</strong>: Typical deployment cycle from setup to production:
    <ul>
      <li>Initial setup: a few hours</li>
      <li>Synthetic Data generation: 15 minutes to 1+ hours depending on computing resources</li>
      <li>Training: several hours on consumer hardware</li>
      <li>Evaluation and deployment: a few hours</li>
    </ul>
  </li>
  <li><strong>Maintenance Requirements</strong>:
    <ul>
      <li>Regular model evaluations against new benchmarks</li>
      <li>Periodic retraining with updated taxonomy</li>
      <li>System updates and dependency management</li>
      <li>Storage management for checkpoints and datasets</li>
    </ul>
  </li>
</ul>

<h3 id="rag-vs-fine-tuning">RAG vs Fine-tuning</h3>

<p>It‚Äôs important to recognise that fine-tuning isn‚Äôt always the optimal solution. For dynamic, frequently changing knowledge bases, Retrieval-Augmented Generation (RAG) often provides a more practical and maintainable solution. Fine-tuning through <a href="https://instructlab.ai/">InstructLab</a> is most valuable for:</p>
<ul>
  <li>Stable knowledge domains (e.g., natural sciences, engineering)</li>
  <li>Consistent skill enhancement needs</li>
  <li>Cases where inference latency is critical</li>
</ul>

<p>The system‚Äôs architecture strikes a careful balance between computational efficiency and training effectiveness, making it a practical tool for both experimentation and production use. While not eliminating the complexity of LLM fine-tuning entirely, <a href="https://instructlab.ai/">InstructLab</a> significantly reduces the technical barriers to entry in this crucial domain.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Python version compatibility remains a significant consideration in the ML ecosystem. While newer versions (‚â•3.12) offer improved performance, they often lack compatibility with essential ML frameworks. This constraint informs <a href="https://instructlab.ai/">InstructLab</a>‚Äôs current version requirements.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="ai" /><category term="llm" /><category term="model-governance" /><category term="production" /><category term="quantisation" /><category term="python" /><category term="mlops" /><category term="best-practices" /><category term="data-science" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üí° TIL: Understanding GGUF Model Quantisation</title><link href="http://0.0.0.0:4000/TIL-llm-quantisation/" rel="alternate" type="text/html" title="üí° TIL: Understanding GGUF Model Quantisation" /><published>2024-12-07T00:00:00+00:00</published><updated>2024-12-07T00:00:00+00:00</updated><id>http://0.0.0.0:4000/TIL-llm-quantisation</id><content type="html" xml:base="http://0.0.0.0:4000/TIL-llm-quantisation/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>When experimenting with larger language models (12B, 30B, 70B etc.), choosing the right quantisation format becomes crucial for striking a good balance i.e. running them on consumer hardware while maintaining reasonably good performance. I wrote this guide after spending time looking up different GGUF quantisation types to optimise model selection for my machine‚Äôs constraints. This guide explains quantisation methods and their practical tradeoffs to help the reader select the optimal format for their setup.<br />
The quantisation formats discussed here are implemented in popular frameworks like <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>. Q4_K_S is typically the default format due to its good balance of size, speed, and quality, while Q2_K and Q3_K variants are offered for more constrained systems.</p>

<h2 id="what-is-quantisation">What is Quantisation?</h2>
<p>Quantisation converts model weights from 16-bit floating point (F16) to lower precision formats using fixed-size blocks. Each block contains multiple weights that share scaling parameters. <br />
Perplexity is the key metric used to measure model quality after quantisation. It indicates how well the model predicts text, the lower the perplexity the better the predictions. For example, a change from 5.91 to 6.78 perplexity represents a noticeable but often acceptable drop in prediction quality. A model with perplexity 6.78 is slightly less certain about its predictions than one with perplexity 5.91.</p>

<h2 id="basic-quantisation-types-and-k-quantisation">Basic Quantisation Types and K-Quantisation</h2>
<p>K-quantisation is a way to make AI models smaller using two methods to store weights (the model‚Äôs numbers):</p>

<ol>
  <li>Type-0 (simpler): reconstructs weight as <code class="language-plaintext highlighter-rouge">weight = scale √ó quant</code></li>
  <li>Type-1 (more precise): reconstructs weight as <code class="language-plaintext highlighter-rouge">weight = scale √ó quant + minimum</code></li>
</ol>

<p>The ‚Äúblock minimum‚Äù <code class="language-plaintext highlighter-rouge">minimum</code> is the smallest value found in a group of weights. By tracking this minimum, we can represent the other values more precisely relative to it, rather than having to represent their full absolute values.</p>

<p>Each format groups weights into ‚Äúsuper-blocks‚Äù to save space. Specifically:</p>

<p>Q2_K (2-bit):</p>
<ul>
  <li>Uses Type-1 formula</li>
  <li>Organises weights in groups of 256 (16 blocks √ó 16 weights)</li>
  <li>Uses 4 bits to store both scales and minimums</li>
  <li>Takes exactly 2.5625 bits per weight</li>
  <li>Result: Shrinks a 13GB model to 2.67GB, but quality drops (perplexity increases from 5.91 to 6.78)</li>
</ul>

<p>Q3_K (3-bit):</p>
<ul>
  <li>Uses Type-0 formula (simpler one)</li>
  <li>Same organization: 16 blocks √ó 16 weights</li>
  <li>Uses 6 bits to store scales</li>
  <li>Takes exactly 3.4375 bits per weight</li>
  <li>Better quality than Q2_K but bigger file size</li>
</ul>

<p>Q4_K (4-bit):</p>
<ul>
  <li>Uses Type-1 formula</li>
  <li>Different organisation: 8 blocks √ó 32 weights = 256 total</li>
  <li>Uses 6 bits for both scales and minimums</li>
  <li>Takes exactly 4.5 bits per weight</li>
  <li>Much better quality, file size around 3.56GB</li>
</ul>

<p>Q5_K (5-bit):</p>
<ul>
  <li>Uses Type-1 formula</li>
  <li>Same organisation as Q4_K</li>
  <li>Also uses 6 bits for scales and minimums</li>
  <li>Takes exactly 5.5 bits per weight</li>
  <li>Quality getting very close to original</li>
</ul>

<p>Q6_K (6-bit):</p>
<ul>
  <li>Uses Type-0 formula</li>
  <li>Back to 16 blocks √ó 16 weights</li>
  <li>Uses 8 bits for scales</li>
  <li>Takes exactly 6.5625 bits per weight</li>
  <li>Almost perfect quality, file size 5.15GB</li>
</ul>

<p>The main tradeoff: Fewer bits means smaller files but lower quality. More bits means better quality but larger files. This lets users choose what works best for their needs.<br />
When compressing numbers in Type-1 quantisation, each block keeps track of its smallest value (the minimum). When reconstructing the weights, this minimum is added back after multiplication. This helps preserve the range of values more accurately than just using scaling alone.</p>

<p>A simple way to think of this concept is:</p>
<ul>
  <li>Type-0 just stretches/shrinks values using a scale</li>
  <li>Type-1 first shifts all numbers by subtracting the minimum (making them smaller), then scales them for storage, and when reconstructing adds the minimum back</li>
</ul>

<p>This is why Type-1 generally gives better quality results but needs more storage space. It has to keep track of both the scale and minimum for each block.</p>

<h2 id="mixed-precision-strategies">Mixed Precision Strategies</h2>
<p>K-quantisations use different precision levels for different model components. From <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> documentation, there are three variants:</p>

<ul>
  <li>S (Small): Uses single quantisation throughout
Example using Q3_K_S:
    <blockquote>
      <p>All model tensors ‚Üí Q3_K (3-bit)<br />
Result: 2.75GB size, 6.46 perplexity (7B model)</p>
    </blockquote>
  </li>
  <li>M (Medium): Strategic mixed precision
Example using Q3_K_M:
    <blockquote>
      <p>attention.wv<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, attention.wo<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, feed_forward.w2<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> ‚Üí Q4_K (4-bit)<br />
All other tensors ‚Üí Q3_K (3-bit)<br />
Result: 3.06GB size, 6.15 perplexity (7B model)</p>
    </blockquote>
  </li>
  <li>L (Large): Higher precision mix
Example using Q3_K_L:
    <blockquote>
      <p>attention.wv<sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, attention.wo<sup id="fnref:2:1" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, feed_forward.w2<sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> ‚Üí Q5_K (5-bit)<br />
All other tensors ‚Üí Q3_K (3-bit)<br />
Result: 3.35GB size, 6.09 perplexity (7B model)</p>
    </blockquote>
  </li>
</ul>

<p>These strategies target attention and feed-forward layers with higher precision because they directly impact text processing quality, as demonstrated by the perplexity improvements in benchmarks: Q3_K_S (6.46) ‚Üí Q3_K_M (6.15) ‚Üí Q3_K_L (6.09).<br />
The improvement in perplexity scores demonstrates why mixed precision strategies are effective, though they require more storage space.</p>

<h2 id="performance-comparison-7b-model">Performance Comparison (7B model)</h2>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Format | Size(GB) | Reduction | BPW  | Perplexity | RTX4080  | M2Max   
F16    | 13.0     | 1.0x      | 16.0 | 5.91       | 60.0ms   | 116ms
Q2_K   | 2.67     | 4.9x      | 2.56 | 6.78       | 15.5ms   | 56ms
Q3_K_S | 2.75     | 4.7x      | 3.44 | 6.46       | 18.6ms   | 81ms
Q4_K_S | 3.56     | 3.7x      | 4.50 | 6.02       | 15.5ms   | 50ms
Q6_K   | 5.15     | 2.5x      | 6.56 | 5.91       | 18.3ms   | 75ms
</code></pre></div></div>
<p>*BPW = Bits Per Weight, Speed in milliseconds per token</p>

<p>Practical Recommendations:</p>
<ul>
  <li>Balanced Performance: Q4_K_S</li>
  <li>Maximum Compression: Q2_K</li>
  <li>Best Quality: Q6_K (matches F16)</li>
  <li>Limited RAM: Q2_K or Q3_K</li>
  <li>GPU Inference: Q4_K (optimal speed/quality)</li>
</ul>

<p>All data are from recent <a href="https://github.com/ggerganov/llama.cpp/pull/1684">llama.cpp</a> performance benchmarks and <a href="https://github.com/ggerganov/ggml">GGML</a> implementation details.</p>

<h2 id="memory-requirements-for-inference">Memory Requirements for Inference</h2>
<p>When running quantised models, more RAM is required than the model size alone for inference overhead. Memory requirements depend on several factors:</p>
<ul>
  <li>Model architecture and size</li>
  <li>Batch size for inference</li>
  <li>Number of layers loaded at once</li>
  <li>Operating system and framework overhead</li>
</ul>

<p>For 7B models (verified from benchmarks):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Format | Model Size | Note
F16    | 13.0GB    | Base format
Q4_K_S | 3.56GB    | Common choice
Q3_K_S | 2.75GB    | Minimum size
Q6_K   | 5.15GB    | Highest quality
</code></pre></div></div>

<p>For larger models scale the memory requirements proportionally and ensure additional overhead memory is available for inference. Test with smaller models first to gauge the system‚Äôs capabilities.<br />
Actual RAM/VRAM requirements will be higher than the model size. Consider monitoring memory usage during inference to determine exact requirements for a specific setup.<br />
Here is an example memory usage scenario for a Q4_K_S 7B model:</p>
<ul>
  <li>Model size: 3.56GB</li>
  <li>Inference overhead: ~2GB for standard settings</li>
  <li>Operating system buffer: ~1GB recommended</li>
  <li>Total recommended free memory: ~7GB</li>
</ul>

<p>This explains why a model that‚Äôs ‚Äú3.56GB‚Äù might need 6-7GB of free RAM/VRAM to run smoothly. The exact overhead varies based on your settings and system.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Modern quantisation techniques offer multiple ways to run large language models on consumer hardware. Here‚Äôs what we need to remember:</p>

<ul>
  <li>K-quantisation provides the best balance through super-blocks and mixed precision strategies</li>
  <li>Q4_K_S (4-bit) represents the current sweet spot for most users, offering:
    <ul>
      <li>3.7x size reduction</li>
      <li>Good perplexity (6.02)</li>
      <li>Excellent inference speed on both GPU and CPU</li>
    </ul>
  </li>
  <li>For more constrained setups, Q2_K/Q3_K variants can run larger models with acceptable quality loss</li>
  <li>Higher bits (Q5_K, Q6_K) approach F16 quality but require more memory</li>
  <li>The _S/_M/_L variants let the user fine-tune the quality-size tradeoff by adjusting precision where it matters most</li>
</ul>

<p>Before downloading a quantised model, check the system‚Äôs available RAM and choose a format that leaves enough memory for comfortable operation. For most users with modern GPUs, Q4_K variants will provide the best experience.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In <a href="https://github.com/ggerganov/llama.cpp/tree/master/examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp">llama.cpp</a>, <code class="language-plaintext highlighter-rouge">attention.wv</code> refers to a tensor that holds the weights for the value vectors in the self-attention mechanism of the model. This tensor is crucial for determining how much focus the model places on different parts of the input when generating responses.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><code class="language-plaintext highlighter-rouge">attention.wo</code> refers to the weight matrix used in the output layer of the attention mechanism within a transformer model. It plays a crucial role in transforming the attention output into the final representation that is used for generating predictions.¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p><code class="language-plaintext highlighter-rouge">feed_forward.w1</code> projects input to a higher-dimensional space, enabling the capture of complex features. <code class="language-plaintext highlighter-rouge">feed_forward.w2</code> projects transformed input back to the original dimension with a non-linear activation function, whereas <code class="language-plaintext highlighter-rouge">feed_forward.w3</code> applies an additional transformation to enhance the learning of complex patterns. These matrices collectively enable the feed-forward network to transform and learn from the input effectively, contributing to the overall performance of the transformer model.¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="ai" /><category term="llm" /><category term="energy-reduction" /><category term="performance" /><category term="quantisation" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">üí° TIL: LLM Evaluation using Critique Shadowing</title><link href="http://0.0.0.0:4000/TIL-llm-eval-critique-shadowing/" rel="alternate" type="text/html" title="üí° TIL: LLM Evaluation using Critique Shadowing" /><published>2024-12-05T00:00:00+00:00</published><updated>2024-12-05T00:00:00+00:00</updated><id>http://0.0.0.0:4000/TIL-llm-eval-critique-shadowing</id><content type="html" xml:base="http://0.0.0.0:4000/TIL-llm-eval-critique-shadowing/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>As LLMs increasingly drive critical business decisions, ensuring their reliability becomes paramount. Many teams struggle with complex metrics and scoring systems that lead to confusion rather than clarity. <a href="https://hamel.dev/">Hamel Husain</a>‚Äôs Critique Shadowing methodology<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> offers a systematic path from drowning in metrics to developing reliable evaluation systems.</p>

<h2 id="the-critique-shadowing-method">The Critique Shadowing Method</h2>
<p>The key insight behind Critique Shadowing is deceptively simple: start with binary (pass/fail) expert judgements and detailed critiques before building automated evaluation systems. This approach solves two critical challenges: capturing domain expertise and scaling evaluation processes.</p>

<p>This expert-centric approach echoes <a href="https://en.wikipedia.org/wiki/Knowledge_engineering">knowledge engineering</a> practices from the 1970-80s, when AI researchers first recognized the necessity of systematically capturing domain expertise. Just as <a href="https://en.wikipedia.org/wiki/Mycin">MYCIN</a>‚Äôs creators worked closely with medical doctors to encode diagnostic knowledge, Critique Shadowing similarly structures the process of extracting expert judgement for LLM evaluation. While the technology has evolved from rule-based systems to large language models, the fundamental challenge of effectively capturing and operationalising expert knowledge remains central.</p>

<h3 id="implementation-process">Implementation Process</h3>
<p>The methodology follows a structured, iterative process:</p>

<center>
    <img src="https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/refs/heads/master/images/Critique%20Framework%20Hamel%20Husain.png" width="80%" height="80%" />
</center>

<ol>
  <li>Identify a principal domain expert as the arbiter of quality</li>
  <li>Create a diverse dataset covering different scenarios and user types</li>
  <li>Expert conducts binary pass/fail judgements with detailed critiques</li>
  <li>Address discovered issues and verify fixes</li>
  <li>Develop LLM-based judges using expert critiques as few-shot examples</li>
  <li>Analyze error patterns and root causes</li>
  <li>Create specialized judges for persistent issues</li>
</ol>

<p>The process is continuous, repeating periodically or when material changes occur. For simpler applications or when manual review is feasible, teams can adapt or streamline these steps while maintaining the core principle of systematic data examination.</p>

<h2 id="beyond-automation">Beyond Automation</h2>
<p>Husain‚Äôs most striking observation is that the process of developing evaluation systems often provides more value than the resulting automated judges. The systematic collection of expert feedback reveals product insights, user needs, and failure modes that might otherwise remain hidden. This understanding drives improvements in the core system, not just its evaluation.</p>

<h2 id="conclusion">Conclusion</h2>
<p>The Critique Shadowing methodology succeeds by prioritizing expert knowledge and systematic data collection over premature automation. For teams building LLM applications, this approach offers a clear path to reliable evaluation systems while simultaneously deepening their understanding of their product and users.<br />
LLM evaluation is an active area of interest and research both in academia and industry. Here is a short list of resources to look into:</p>
<ul>
  <li><a href="https://www.ibm.com/think/topics/llm-evaluation">IBM LLM Evaluation</a></li>
  <li><a href="https://docs.mistral.ai/guides/evaluation/">Mistral AI - Evaluation</a></li>
  <li><a href="https://github.com/mistralai/mistral-evals">Mistral Evals</a></li>
  <li><a href="https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool">Anthropic - Using the Evaluation Tool</a></li>
  <li><a href="https://dev.to/guybuildingai/-top-5-open-source-llm-evaluation-frameworks-in-2024-98m">Top 5 Open-Source LLM Evaluation Frameworks in 2024</a></li>
</ul>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Husain, H. (2024). ‚ÄúCreating a LLM-as-a-Judge That Drives Business Results‚Äù https://hamel.dev/blog/posts/llm-judge/¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="til" /><category term="llm" /><category term="ai" /><category term="machine-learning" /><category term="mlops" /><category term="best-practices" /><category term="production" /><category term="model-governance" /><category term="evaluation" /><category term="observability" /><category term="monitoring" /><category term="quality-assurance" /><category term="iterative-refinement" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">‚úç A Path to Maintainable AI Systems using Norman‚Äôs Design Principles</title><link href="http://0.0.0.0:4000/design-principles-ds-ai/" rel="alternate" type="text/html" title="‚úç A Path to Maintainable AI Systems using Norman‚Äôs Design Principles" /><published>2024-12-03T00:00:00+00:00</published><updated>2024-12-03T00:00:00+00:00</updated><id>http://0.0.0.0:4000/design-principles-ds-ai</id><content type="html" xml:base="http://0.0.0.0:4000/design-principles-ds-ai/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>
<p>Don Norman‚Äôs principles of good design, outlined in <a href="https://archive.org/details/thedesignofeverydaythingsbydonnorman">The Design of Everyday Things</a>, are particularly relevant to Data Science and AI Engineering, where systems often suffer from unnecessary complexity. This article presents a minimalist approach to implementing these principles using a carefully selected set of tools that maximise impact while reducing operational overhead. Norman‚Äôs insights about visibility, feedback, constraints, and mappings translate powerfully to AI system design, where abstract interfaces and complex workflows can easily become overwhelming. Just as Norman observed that poorly designed physical objects lead to user frustration and errors, poorly architected AI systems can result in maintenance nightmares, hidden failure modes, and costly debugging cycles. By applying his principles - making system states visible, providing clear feedback, implementing appropriate constraints, and creating natural mappings between components, we can build AI systems that are not only more intuitive to use but also easier to maintain, debug, and evolve over time.</p>

<h2 id="design-principles-implementation">Design Principles Implementation</h2>
<h3 id="1-visibility">1. Visibility</h3>
<p>Implement comprehensive system observability using <a href="https://mlflow.org/">MLflow</a> as your central platform:</p>

<ul>
  <li>Track experiments, parameters, and metrics</li>
  <li>Version models and artefacts</li>
  <li>Log production predictions and outcomes</li>
  <li>Monitor model performance metrics</li>
</ul>

<p>For system-level metrics, use <a href="https://prometheus.io/docs/visualization/grafana/">Prometheus/Grafana</a> to:</p>
<ul>
  <li>Track resource utilisation (CPU, memory, latency)</li>
  <li>Monitor prediction throughput</li>
  <li>Create dashboards for key performance indicators</li>
</ul>

<p>Implement adaptive sampling for high-volume systems:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">should_log</span><span class="p">(</span><span class="n">request_id</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">hash</span><span class="p">(</span><span class="n">request_id</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">sampling_rate</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="2-feedback">2. Feedback</h3>
<p>Use <a href="https://prometheus.io/docs/visualization/grafana/">Prometheus/Grafana</a> for real-time monitoring and alerting:</p>

<ul>
  <li>Set up alerts for model performance degradation</li>
  <li>Monitor data distribution shifts</li>
  <li>Track system health metrics</li>
  <li>Configure tiered alerting based on severity</li>
</ul>

<p>Example metric collection:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">prometheus_client</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">Histogram</span>

<span class="n">PREDICTIONS</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="s">'model_predictions_total'</span><span class="p">,</span> <span class="s">'Total predictions made'</span><span class="p">)</span>
<span class="n">LATENCY</span> <span class="o">=</span> <span class="n">Histogram</span><span class="p">(</span><span class="s">'prediction_latency_seconds'</span><span class="p">,</span> <span class="s">'Time spent processing prediction'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">LATENCY</span><span class="p">.</span><span class="n">time</span><span class="p">():</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="n">PREDICTIONS</span><span class="p">.</span><span class="n">inc</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">prediction</span>
</code></pre></div></div>

<h3 id="3-constraints">3. Constraints</h3>
<p>Implement data and model guardrails using <a href="https://greatexpectations.io/">Great Expectations</a>:</p>

<ul>
  <li>Define data quality expectations</li>
  <li>Set distribution bounds for features</li>
  <li>Monitor for data drift</li>
  <li>Generate validation reports</li>
</ul>

<p>Example constraint implementation:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">great_expectations.dataset</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">def</span> <span class="nf">validate_features</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="n">dataset</span><span class="p">.</span><span class="n">expect_column_values_to_be_between</span><span class="p">(</span><span class="s">"age"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
    <span class="n">dataset</span><span class="p">.</span><span class="n">expect_column_values_to_not_be_null</span><span class="p">(</span><span class="s">"critical_feature"</span><span class="p">)</span>
    <span class="n">validation_result</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">validate</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">validation_result</span><span class="p">.</span><span class="n">success</span>
</code></pre></div></div>

<h3 id="4-mappings">4. Mappings</h3>
<p>Use <a href="https://mlflow.org/">MLflow</a> to maintain clear relationships between:</p>

<ul>
  <li>Experiments and business objectives</li>
  <li>Models and their training data</li>
  <li>Predictions and outcomes</li>
  <li>Performance metrics and business KPIs</li>
</ul>

<p>Example mapping structure:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">mlflow</span><span class="p">.</span><span class="n">start_run</span><span class="p">(</span><span class="n">run_name</span><span class="o">=</span><span class="s">"production_model_v1"</span><span class="p">):</span>
    <span class="n">mlflow</span><span class="p">.</span><span class="n">log_param</span><span class="p">(</span><span class="s">"business_objective"</span><span class="p">,</span> <span class="s">"customer_churn"</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="p">.</span><span class="n">log_param</span><span class="p">(</span><span class="s">"data_version"</span><span class="p">,</span> <span class="n">data_hash</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="p">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s">"business_impact"</span><span class="p">,</span> <span class="n">revenue_improvement</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="p">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="s">"feature_importance.json"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="5-error-prevention-and-recovery">5. Error Prevention and Recovery</h3>
<p>Integrate safeguards using your core toolset:</p>

<p><a href="https://mlflow.org/">MLflow</a>:</p>
<ul>
  <li>Version control for models and artefacts</li>
  <li>Rollback capabilities</li>
  <li>Experiment tracking for reproducibility</li>
</ul>

<p><a href="https://prometheus.io/docs/visualization/grafana/">Prometheus/Grafana</a>:</p>
<ul>
  <li>Early warning system for issues</li>
  <li>Performance degradation detection</li>
  <li>Resource exhaustion prevention</li>
</ul>

<p><a href="https://greatexpectations.io/">Great Expectations</a>:</p>
<ul>
  <li>Data quality validation</li>
  <li>Schema enforcement</li>
  <li>Distribution monitoring</li>
</ul>

<p>Example error prevention:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">safe_predict</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">validate_features</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">fallback_prediction</span><span class="p">()</span>
    
    <span class="k">try</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">LATENCY</span><span class="p">.</span><span class="n">time</span><span class="p">():</span>
            <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
            <span class="n">PREDICTIONS</span><span class="p">.</span><span class="n">inc</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">prediction</span>
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">ERROR_COUNTER</span><span class="p">.</span><span class="n">inc</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">fallback_prediction</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="implementation-strategy">Implementation Strategy</h2>
<ol>
  <li>Start with <a href="https://mlflow.org/">MLflow</a>
    <ul>
      <li>Set up experiment tracking</li>
      <li>Implement model versioning</li>
      <li>Configure basic logging</li>
    </ul>
  </li>
  <li>Add <a href="https://prometheus.io/docs/visualization/grafana/">Prometheus/Grafana</a>
    <ul>
      <li>Deploy basic monitoring</li>
      <li>Set up key alerts</li>
      <li>Create essential dashboards</li>
    </ul>
  </li>
  <li>Integrate <a href="https://greatexpectations.io/">Great Expectations</a>
    <ul>
      <li>Define core data quality rules</li>
      <li>Implement validation pipelines</li>
      <li>Monitor data distributions</li>
    </ul>
  </li>
</ol>

<h2 id="conclusions">Conclusions</h2>
<p>By focusing on a minimal set of powerful tools (<a href="https://mlflow.org/">MLflow</a>, <a href="https://prometheus.io/docs/visualization/grafana/">Prometheus/Grafana</a>, and <a href="https://greatexpectations.io/">Great Expectations</a>), you can implement Norman‚Äôs design principles effectively while maintaining system simplicity. This approach provides:</p>

<ul>
  <li>Comprehensive visibility through unified logging and monitoring</li>
  <li>Immediate feedback via real-time alerts</li>
  <li>Strong constraints through data validation</li>
  <li>Clear mappings between components</li>
  <li>Robust error prevention and recovery</li>
</ul>

<p>The key is to fully utilise these core tools rather than adding complexity with additional solutions. This creates maintainable, observable, and reliable AI systems that can scale with your needs.</p>]]></content><author><name></name></author><category term="ai" /><category term="data-science" /><category term="design-principles" /><category term="code-quality" /><category term="mlops" /><category term="monitoring" /><category term="observability" /><category term="production" /><category term="model-governance" /><category term="minimal" /><summary type="html"><![CDATA[]]></summary></entry></feed>