<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2025-03-28T15:23:15+00:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Just-in-Time learning</title><subtitle>Inquisitive. Learning. Sharing. Simplicity = Reliability</subtitle><entry><title type="html">🔨 REWORK</title><link href="http://0.0.0.0:4000/rework-the-art-of-working-smarter/" rel="alternate" type="text/html" title="🔨 REWORK" /><published>2025-03-28T00:00:00+00:00</published><updated>2025-03-28T00:00:00+00:00</updated><id>http://0.0.0.0:4000/rework-the-art-of-working-smarter</id><content type="html" xml:base="http://0.0.0.0:4000/rework-the-art-of-working-smarter/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>The traditional approach to business often involves comprehensive planning, rapid growth, long hours, and complex processes. But is this truly the most effective way to succeed? <a href="https://world.hey.com/jason">Jason Fried</a> and <a href="https://world.hey.com/david">David Heinemeier Hansson</a>, founders of <a href="https://en.wikipedia.org/w/index.php?title=Basecamp_(company)&amp;redirect=no">Basecamp</a> (now <a href="https://en.wikipedia.org/wiki/37signals">37signals</a>), challenge these conventional notions in their influential book “<a href="https://basecamp.com/books/rework">Rework</a>”. Published in 2010, this manifesto presents an alternative philosophy for building successful businesses in the digital age -one that emphasises simplicity, efficiency, and balance. Drawing from their experience creating profitable web applications with a small team, Fried and Hansson offer practical insights for entrepreneurs and companies of all sizes. Their approach advocates working smarter rather than harder, focusing on what truly matters, and challenging business orthodoxy at every turn.</p>

<h2 id="foundational-principles-of-rework">Foundational Principles of “Rework”</h2>

<h3 id="embrace-simplicity-and-constraints">Embrace Simplicity and Constraints</h3>

<p>Fried and Hansson consistently emphasise that constraints aren’t limitations but advantages. With limited resources, you’re forced to focus on what’s essential:</p>
<ul>
  <li><strong>Build half a product, not a half-ar🤬ed product</strong>: It’s better to do fewer things exceptionally well than to attempt everything poorly. Quality trumps quantity.</li>
  <li><strong>Embrace constraints</strong>: Limited time, budget, or people can spark creativity and force efficiency. They make you focus on doing more with less.</li>
  <li><strong>Underdo your competition</strong>: Instead of adding more features than competitors, focus on solving core problems elegantly. Simplicity is a competitive advantage.</li>
</ul>

<p>The authors point to specific examples, such as how Basecamp launched without billing functionality (adding it 30 days later) and how the Flip video camera succeeded by deliberately omitting features that competitors deemed essential.</p>

<h3 id="challenge-traditional-business-thinking">Challenge Traditional Business Thinking</h3>

<p>“Rework” consistently questions business conventions that many take for granted:</p>
<ul>
  <li><strong>Planning is guessing</strong>: Detailed long-term business plans are often exercises in fiction. Instead, make decisions just in time with the most current information available.</li>
  <li><strong>Working long hours is counterproductive</strong>: “Workaholism” leads to burnout and mediocre output. Productivity isn’t about hours worked but about focused, quality work.</li>
  <li><strong>Growth isn’t always good</strong>: The authors argue against the obsession with expansion, suggesting companies find their “right size” and focus on sustainability rather than constant growth.</li>
  <li><strong>Skip the “rock stars”</strong>: Instead of obsessing over hiring “ninjas” or “rock stars,” create an environment where ordinary people can do extraordinary work.</li>
</ul>

<h3 id="focus-on-action-over-discussion">Focus on Action Over Discussion</h3>

<p>A central theme in “Rework” is the importance of creating rather than just talking about creating:</p>
<ul>
  <li><strong>Start making something</strong>: Ideas are worthless without execution. The world is filled with people who “had that idea first” but never acted on it.</li>
  <li><strong>Launch now</strong>: Perfection is unattainable; get your product out quickly and iterate based on real feedback rather than assumptions.</li>
  <li><strong>Meetings are toxic</strong>: They interrupt productivity, waste collective time, and often accomplish little. Minimise them ruthlessly.</li>
</ul>

<p>The authors illustrate this with their own experience building Basecamp, launching quickly with core functionality and improving based on actual customer feedback rather than theoretical market research.</p>

<h3 id="build-an-audience-focused-business">Build an Audience-Focused Business</h3>

<p>Fried and Hansson outline a customer-centric approach to business development:</p>
<ul>
  <li><strong>Out-teach your competition</strong>: Share knowledge generously through blogs, articles, and tutorials. Teaching establishes authority and builds trust with potential customers.</li>
  <li><strong>Build an audience</strong>: Develop a following of people interested in what you have to say. When you launch products, you’ll already have an engaged audience.</li>
  <li><strong>Emulate chefs</strong>: Like celebrity chefs who share their recipes freely, sharing your expertise doesn’t diminish your business -it enhances it.</li>
</ul>

<p>Their company blog, Signal vs. Noise, exemplifies this approach, having built an audience of over 100,000 daily readers who became a natural customer base.</p>

<h3 id="create-a-sustainable-work-culture">Create a Sustainable Work Culture</h3>

<p>The authors advocate for work environments that prioritise sustainability over burnout:</p>
<ul>
  <li><strong>Send people home at 5</strong>: Reasonable working hours increase per-hour productivity and lead to more creative solutions.</li>
  <li><strong>Avoid policies that treat people like children</strong>: Trust adults to manage their time and make good decisions.</li>
  <li><strong>Avoid unnecessary formality</strong>: Communicate in a human voice rather than corporate-speak. Sound like yourself, not like a faceless entity.</li>
</ul>

<h2 id="potential-limitations">Potential Limitations</h2>

<p>While “Rework” offers valuable counter-conventional wisdom, some of its approaches may not suit all business contexts. The authors’ philosophy works particularly well for software and service businesses with low overhead, but manufacturing or capital-intensive industries may require more traditional planning. Additionally, their “do less” approach might not always scale for businesses with complex regulatory requirements or those serving enterprise clients with extensive needs.</p>

<h2 id="conclusion">Conclusion</h2>

<p>“Rework” offers a refreshing alternative to conventional business wisdom, advocating for a more thoughtful, balanced, and human approach to work. The book’s central message is that success doesn’t require sixty-hour workweeks, venture capital, or extensive planning -it requires focus on what matters, elimination of what doesn’t, and dedication to quality execution.<br />
By challenging assumptions about growth, working hours, planning, and hiring, Fried and Hansson present a blueprint for building businesses that are not only profitable but also sustainable and enjoyable to run. Their philosophy can be distilled to a few key principles: embrace constraints, focus on quality over quantity, prioritise action over planning, and build businesses that respect both customers and employees.<br />
Whether you’re running a startup, managing a team, or simply looking to work more effectively, “Rework” provides valuable insights for doing more with less and building something that lasts. It’s not about working more -it’s about working smarter.</p>]]></content><author><name></name></author><category term="37signals" /><category term="best-practices" /><category term="productivity" /><category term="efficiency" /><category term="company-culture" /><category term="remote-work" /><category term="minimal" /><category term="business-value" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">🌐 Remote: Office Not Required</title><link href="http://0.0.0.0:4000/remote-office-not-required/" rel="alternate" type="text/html" title="🌐 Remote: Office Not Required" /><published>2025-03-26T00:00:00+00:00</published><updated>2025-03-26T00:00:00+00:00</updated><id>http://0.0.0.0:4000/remote-office-not-required</id><content type="html" xml:base="http://0.0.0.0:4000/remote-office-not-required/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>In the rapidly evolving landscape of modern business, few changes have been as transformative as the shift toward remote work. In their insightful book “<a href="https://basecamp.com/books#remote">Remote: Office Not Required</a>”, <a href="https://world.hey.com/jason">Jason Fried</a> and <a href="https://world.hey.com/david">David Heinemeier Hansson</a> of <a href="https://en.wikipedia.org/w/index.php?title=Basecamp_(company)&amp;redirect=no">Basecamp</a> (now <a href="https://en.wikipedia.org/wiki/37signals">37signals</a>) present a compelling case for why working remotely isn’t just a viable option—it’s often superior to the traditional office-based model.<br />
Written in 2013, well before the pandemic forced companies to adopt remote work en masse, the book now seems prophetic. Fried and Hansson argue that remote work is not a fleeting trend but an inevitable evolution in how businesses operate. The authors, who built their successful software company with team members scattered across the globe, offer practical advice for implementing and thriving in a remote work environment.<br />
Let’s explore the core principles of “Remote” that can help businesses and workers navigate this new reality.</p>

<h2 id="why-remote-work-makes-sense">Why Remote Work Makes Sense</h2>

<h3 id="the-office-paradox">The Office Paradox</h3>

<p>One of the book’s most compelling arguments is that traditional offices often hinder productivity rather than enhance it. Fried and Hansson point out that when people need to get serious work done, they rarely cite the office as their preferred location. Instead, they choose early mornings, late evenings, or weekends—times when interruptions are minimal.<br />
Offices have become “interruption factories” where meaningful work is chopped into small, ineffective chunks. Meetings, impromptu desk visits, and constant noise create an environment where deep, focused work becomes nearly impossible. Remote work, by contrast, allows people to create their own distraction-free environments.</p>

<h3 id="the-end-of-commuting">The End of Commuting</h3>

<p>Another significant advantage of remote work is eliminating the commute. Beyond the obvious time savings, research shows that long commutes correlate with increased obesity, stress, neck and back pain, and even higher divorce rates. The authors calculate that an average commute consumes 300-400 hours per year—time that could be redirected toward productive work or personal well-being.</p>

<h3 id="access-to-global-talent">Access to Global Talent</h3>

<p>Perhaps most importantly, remote work dramatically expands the talent pool. Instead of limiting hiring to a specific geographical area, companies can recruit from anywhere in the world. This not only increases the chances of finding exceptional talent but also naturally leads to a more diverse workforce with varied perspectives.</p>

<h2 id="making-remote-work-work">Making Remote Work Work</h2>

<h3 id="communication-the-key-to-success">Communication: The Key to Success</h3>

<p>Effective remote work hinges on communication. The book advocates for a blend of synchronous and asynchronous communication methods:</p>

<ol>
  <li><strong>Overlap Time</strong>: Ensure team members have at least 4 hours of overlap in their workdays to allow for real-time collaboration when needed.</li>
  <li><strong>Screen Sharing</strong>: Use tools like WebEx, GoToMeeting, or Join.me to collaborate visually, making it feel more like sitting side-by-side.</li>
  <li><strong>Transparent Documentation</strong>: Make information accessible to everyone regardless of time zone, eliminating bottlenecks. Basecamp, their project management tool, was designed specifically with this in mind.</li>
  <li><strong>Virtual Water Cooler</strong>: Create spaces for casual conversation to maintain company culture and social connections. The authors used Campfire, their web-based chat service, for this purpose. While Campfire was discontinued as a standalone product, it was recently relaunched in 2024 as part of their ONCE line—allowing users to purchase and self-host the software on their own servers rather than subscribing to a SaaS model.</li>
</ol>

<h3 id="navigating-legal-and-financial-considerations">Navigating Legal and Financial Considerations</h3>

<p>The book doesn’t shy away from the practical challenges of remote work. In the chapter “Taxes, accounting, laws, oh my!” the authors tackle the nuts and bolts of remote employment:</p>
<ul>
  <li><strong>Domestic remote work</strong> is relatively straightforward from a legal standpoint, with few complications beyond potential state tax implications if employees work across state lines.</li>
  <li><strong>International remote work</strong> presents more challenges. The authors outline two main approaches: establishing a local office (expensive but comprehensive) or hiring people as contractors (simpler but with limitations on benefits and employment protections).</li>
  <li><strong>For remote workers</strong>, they recommend setting up a personal company and billing as a contractor if working for an international company, though they acknowledge this isn’t a perfect solution.</li>
</ul>

<p>The authors are refreshingly honest here, acknowledging that running with a less-than-perfect legal setup is common practice—though they recommend consulting professionals for complex situations.</p>

<h3 id="overcoming-common-objections">Overcoming Common Objections</h3>

<p>Fried and Hansson systematically address the objections typically raised against remote work:</p>

<ul>
  <li><strong>“How do I know people are working?”</strong> If you can’t trust employees to work remotely, the issue is hiring, not location.</li>
  <li><strong>“What about security?”</strong> With proper protocols, remote work can be just as secure as office work.</li>
  <li><strong>“We need face-to-face meetings.”</strong> Most meetings can be conducted effectively online, and occasional in-person gatherings can satisfy the need for face time.</li>
  <li><strong>“We need to maintain our culture.”</strong> Culture stems from values and actions, not physical proximity.</li>
</ul>

<h3 id="avoiding-remote-work-pitfalls">Avoiding Remote Work Pitfalls</h3>

<p>The book doesn’t gloss over remote work’s challenges:</p>

<ol>
  <li><strong>Isolation</strong>: Combat loneliness by encouraging employees to work from co-working spaces or cafés occasionally.</li>
  <li><strong>Overwork</strong>: Without clear boundaries, remote workers may struggle to disconnect. Managers should focus on results rather than hours worked and look out for signs of burnout.</li>
  <li><strong>Communication Barriers</strong>: When face-to-face interaction is limited, misunderstandings can occur. Clear, thoughtful communication becomes even more crucial.</li>
</ol>

<h2 id="building-and-managing-a-remote-team">Building and Managing a Remote Team</h2>

<h3 id="hiring-for-remote-work">Hiring for Remote Work</h3>

<p>The authors emphasise that great remote workers possess certain qualities:</p>
<ul>
  <li><strong>Self-motivation</strong>: They can stay productive without direct supervision.</li>
  <li><strong>Strong writing skills</strong>: Since much of remote communication is written, clear writing is essential.</li>
  <li><strong>Results-oriented mindset</strong>: They focus on output rather than hours at a desk.</li>
</ul>

<h3 id="creating-trust-and-accountability">Creating Trust and Accountability</h3>

<p>Rather than micromanaging, successful remote teams are built on trust. The book recommends:</p>
<ul>
  <li><strong>Focus on outputs</strong>: Judge work by what’s accomplished, not when or how it’s done.</li>
  <li><strong>Regular check-ins</strong>: Brief one-on-ones help maintain connection without becoming burdensome.</li>
  <li><strong>Eliminate roadblocks</strong>: Ensure remote workers have the authority and access they need to be effective.</li>
</ul>

<h3 id="the-remote-toolbox">The Remote Toolbox</h3>

<p>The authors provide a practical “Remote Toolbox” with specific recommendations:</p>
<ul>
  <li><strong>Basecamp</strong>: Their own project management tool for organising tasks, discussions, and files in one central location.</li>
  <li><strong>Video conferencing tools</strong>: Google Hangouts (now Google Meet) for group video calls with up to 10 people.</li>
  <li><strong>Screen sharing</strong>: WebEx, GoToMeeting, and Join.me for collaboration and demonstrations.</li>
  <li><strong>File sharing</strong>: Dropbox for keeping files synchronised across multiple devices and locations.</li>
  <li><strong>Collaborative documents</strong>: Google Docs for real-time collaboration on text documents and spreadsheets.</li>
  <li><strong>Co-working directories</strong>: Resources like Regus, LiquidSpace, Desktime, and the Coworking Wiki to find workspaces while travelling or to escape home office isolation.</li>
</ul>

<p>Many of these tools have evolved since the book’s publication, but the core functions they serve remain essential to remote work.</p>

<h3 id="the-importance-of-meetups">The Importance of Meetups</h3>

<p>Despite advocating for remote work, the authors stress the value of occasional in-person gatherings. At their company, they met at least twice yearly for 4-5 days<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. These meetups strengthen personal bonds, allow for intensive collaboration, and reinforce company culture.</p>

<h2 id="conclusion">Conclusion</h2>

<p>“Remote: Office Not Required” provides a comprehensive blueprint for implementing successful remote work practices. The authors convincingly argue that remote work offers numerous advantages: increased productivity, access to global talent, better work-life balance, and reduced overhead costs.<br />
What makes this book particularly valuable is its grounding in real-world experience. Fried and Hansson have built their business on these principles and have navigated the challenges they describe.<br />
As we continue to redefine what work means in the 21st century, the insights from “Remote” remain highly relevant. The authors envision a future where work is judged by results rather than location, where talent knows no geographical boundaries, and where both companies and employees enjoy greater freedom and flexibility.<br />
For businesses looking to thrive in this new landscape, “Remote” offers not just philosophy but practical strategies for turning the challenges of distributed work into competitive advantages. The future of work is indeed remote—and this book provides an excellent road map for that journey.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Reasonable adjustments must be considered for those who are not able to travel far due to health, family or other reasons beyond their control. It is possible to build and maintain a strong culture that does not necessitate travelling, or at least not travelling often or far, if circumstances don’t allow. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="37signals" /><category term="remote-work" /><category term="advantage" /><category term="company-culture" /><category term="productivity" /><category term="best-practices" /><category term="decision-making" /><category term="onboarding" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">😌 It Doesn’t Have to Be Crazy at Work</title><link href="http://0.0.0.0:4000/it-doesnt-have-to-be-crazy-at-work-37-signals/" rel="alternate" type="text/html" title="😌 It Doesn’t Have to Be Crazy at Work" /><published>2025-03-23T00:00:00+00:00</published><updated>2025-03-23T00:00:00+00:00</updated><id>http://0.0.0.0:4000/it-doesnt-have-to-be-crazy-at-work-37-signals</id><content type="html" xml:base="http://0.0.0.0:4000/it-doesnt-have-to-be-crazy-at-work-37-signals/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>In today’s hyperactive business environment, “crazy busy” has become a badge of honour. Endless workweeks, constant interruptions, and the expectation of instant responses have created workplaces where stress is the norm and burnout is inevitable. But does it have to be this way? <a href="https://world.hey.com/jason">Jason Fried</a> and <a href="https://world.hey.com/david">David Heinemeier Hansson</a>, the founders of <a href="https://en.wikipedia.org/w/index.php?title=Basecamp_(company)&amp;redirect=no">Basecamp</a> (renamed to <a href="https://en.wikipedia.org/wiki/37signals">37signals</a> since 2014), argue emphatically that it doesn’t. In their book “<a href="https://basecamp.com/books/calm">It Doesn’t Have to Be Crazy at Work</a>” they present a compelling case for a calmer, more sustainable approach to work -one where companies can still be successful without sacrificing the well-being of their employees.<br />
The authors, who have built a profitable business with minimal stress and reasonable working hours, dismiss the idea that growth-at-all-costs and around-the-clock work schedules are necessary for success. Instead, they advocate for what they call a “calm company” -an organisation that values sustainable workloads, reasonable expectations, sufficient rest, and focused productivity. Let’s dive into the key principles that can help transform a frantic workplace into a calm and productive environment.</p>

<h2 id="the-calm-company-philosophy">The Calm Company Philosophy</h2>

<h3 id="rethinking-time-and-attention">Rethinking Time and Attention</h3>

<p>One of the core insights from the book is that modern workplaces have become “interruption factories” where meaningful work is nearly impossible. Offices chop the workday into tiny fragments -fifteen minutes here, ten minutes there- with meetings, calls, and constant distractions preventing sustained focus.<br />
The authors argue that 8-hour workdays and 40-hour workweeks are plenty of time to accomplish great work, provided that time is actually protected. Instead of measuring commitment by hours spent at a desk, a calm company measures results and respects boundaries. Basecamp’s philosophy is straightforward: “Work 40 hours a week, then stop. No all-nighters, no weekends”.<br />
To protect time, the book advocates for asynchronous communication whenever possible. Not everything requires an immediate response. By promoting a culture of eventual response rather than instant reaction, companies give employees the space for deep, focused work. This might mean designating “library rules” in the office -quiet, focused concentration- and setting clear boundaries for when real-time collaboration is truly necessary.</p>

<h3 id="eliminate-excessive-mms-meetings-and-managers">Eliminate Excessive M&amp;Ms: Meetings and Managers</h3>

<p>Meetings and micromanagement are two primary culprits behind workplace chaos. The authors are particularly critical of the modern meeting culture, noting that “a one-hour meeting with ten people isn’t a one-hour meeting -it’s a ten-hour meeting”. Before calling a meeting, they suggest asking whether it’s truly worth pulling multiple people away from their focused work.<br />
Similarly, the book challenges managers to stop “managing the chairs” (monitoring when people arrive and leave) and instead focus on managing the work itself. This means setting clear expectations, providing necessary resources, removing obstacles, and then trusting people to execute without constant supervision.<br />
At Basecamp, they’ve institutionalised practices like “office hours” for experts, where rather than being constantly available for interruption, they designate specific times when they’re available for questions. They’ve also moved away from real-time chat for important discussions, recognising that this medium often creates an unhealthy expectation of immediate response.</p>

<h3 id="reasonable-expectations-and-focused-scope">Reasonable Expectations and Focused Scope</h3>

<p>Perhaps the most radical departure from conventional business thinking is the authors’ approach to goals and expectations. They proudly declare: “We don’t do goals at Basecamp”. Instead of chasing arbitrary targets, they focus on doing excellent work consistently and sustainably.<br />
The book introduces the concept of “dreadlines” versus deadlines. A dreadline appears when a deadline is paired with an ever-expanding scope. To combat this, Basecamp keeps deadlines fixed but makes scope flexible. Projects can only get smaller over time, not larger, ensuring teams can deliver quality work without burning out.<br />
This means being deliberate about what not to do. As the authors put it: “Having less to do isn’t a problem, it’s an advantage”. They suggest developing the skill of “narrowing as you go” -starting projects with exploration, then gradually focusing in on what’s truly important as you approach the deadline.<br />
Basecamp also embraces the “disagree and commit” approach to decision-making. Rather than requiring consensus, which can lead to endless debate, someone makes the final call after everyone has been heard -and then the whole team commits to making it work, even if some initially disagreed.</p>

<h3 id="building-a-healthy-remote-work-culture">Building a Healthy Remote Work Culture</h3>

<p>Remote work features prominently in Basecamp’s approach to building a calm company. By removing the expectation that everyone must be in the same physical space, they’ve created more flexibility while maintaining productivity.<br />
However, they emphasise that remote work requires intentionality. Teams need sufficient overlap in working hours, clear communication practices, and strong writing skills. In fact, the authors consider good writing essential for remote teams, as it eliminates ambiguity and creates a clear record of decisions and rationales.<br />
The authors also address the concern that remote work might lead to isolation or disconnection. They recommend regular in-person meetups<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> and maintaining a strong company culture based on shared values and respect, not forced socialisation or perks designed to keep people at the office longer.</p>

<h3 id="hiring-and-benefits-that-support-life-outside-work">Hiring and Benefits That Support Life Outside Work</h3>

<p>Basecamp’s approach to hiring focuses on finding talented people who value calm productivity over chaotic hustle. Their compensation philosophy is refreshingly straightforward: equal pay for equal work, regardless of location, with no complex negotiation processes.<br />
Their benefits are specifically designed to encourage life beyond work. Rather than offering free meals to keep employees in the office longer, they provide benefits that help people disconnect -like paid sabbaticals, summer hours (32-hour workweeks from May through August), and even covering the cost of employees’ vacations. This reinforces their belief that the best workers are well-rested ones with rich lives outside the office.</p>

<h2 id="conclusion">Conclusion</h2>

<p>“It Doesn’t Have to Be Crazy at Work” presents a refreshing alternative to the burnout culture that pervades much of today’s business world. The calm company model isn’t about doing less or lowering standards -it’s about working smarter, focusing on what truly matters, and creating sustainable conditions where people can do their best work without sacrificing their health and happiness.</p>

<p>The key takeaways from the book include:</p>

<ol>
  <li>Protect people’s time and attention by eliminating unnecessary interruptions</li>
  <li>Stick to reasonable work hours (40 hours per week is plenty)</li>
  <li>Replace constant meetings with more thoughtful, asynchronous communication</li>
  <li>Focus on the quality of work rather than hours logged</li>
  <li>Keep deadlines fixed but be flexible about scope</li>
  <li>Build a culture of trust where remote work can thrive</li>
  <li>Be intentional about what you say no to</li>
</ol>

<p>As the authors suggest, “calm is contagious” -and so is crazy. By choosing calm, companies can create environments where employees thrive, creativity flourishes, and sustainable success becomes possible. The choice, as they say, is yours.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Reasonable adjustments must be considered for those who are not able to travel far due to health, family or other reasons beyond their control. It is possible to build and maintain a strong culture that does not necessitate travelling, or at least not travelling often or far, if circumstances don’t allow. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="37signals" /><category term="advantage" /><category term="best-practices" /><category term="decision-making" /><category term="business-value" /><category term="slow-down" /><category term="onboarding" /><category term="remote-work" /><category term="productivity" /><category term="company-culture" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">⏪ Making Data Transformations Reversible with fasttransform</title><link href="http://0.0.0.0:4000/fasttransform-for-reversible-data-transformations/" rel="alternate" type="text/html" title="⏪ Making Data Transformations Reversible with fasttransform" /><published>2025-03-22T00:00:00+00:00</published><updated>2025-03-22T00:00:00+00:00</updated><id>http://0.0.0.0:4000/fasttransform-for-reversible-data-transformations</id><content type="html" xml:base="http://0.0.0.0:4000/fasttransform-for-reversible-data-transformations/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>Machine learning practitioners face a common problem: after applying multiple transformations to prepare data for training, it becomes difficult to visualise what the model actually sees. This visualisation gap makes debugging challenging and often leads to missing critical insights about model behaviour.<br />
For example, consider a model built to distinguish wolves from huskies that performs poorly on certain images. Without the ability to easily inspect how transformations affect the input data, one might miss that the model is actually detecting snow (common in wolf photos) rather than the animals themselves.<br />
Fast.ai’s solution to this problem is <a href="https://github.com/AnswerDotAI/fasttransform">fasttransform</a><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, a library that ensures any transformation applied to data can be easily reversed. Let’s explore how it works and why it matters.</p>

<h2 id="reversible-pipelines-made-simple">Reversible Pipelines Made Simple</h2>

<h3 id="the-problem-with-one-way-transforms">The Problem with One-Way Transforms</h3>

<p>Traditional data transformation pipelines in libraries like PyTorch are one-way streets. Consider this simple example of normalising an image:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span> <span class="k">as</span> <span class="n">T</span>
<span class="n">transforms_pt</span> <span class="o">=</span> <span class="n">T</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">T</span><span class="p">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
    <span class="n">T</span><span class="p">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
    <span class="n">T</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">T</span><span class="p">.</span><span class="n">Normalize</span><span class="p">(</span><span class="o">*</span><span class="n">imagenet_stats</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">"husky.jpeg"</span><span class="p">)</span>
<span class="n">img_transformed</span> <span class="o">=</span> <span class="n">transforms_pt</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</code></pre></div></div>

<p>Attempting to visualise <code class="language-plaintext highlighter-rouge">img_transformed</code> results in a mess of pixel values outside the displayable range. To see what the model sees, one needs to manually write an inverse transform function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">decode_pt</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">):</span> <span class="n">t</span><span class="p">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">s</span><span class="p">).</span><span class="n">add_</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">255</span><span class="p">).</span><span class="n">clamp</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">).</span><span class="n">byte</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div>

<p>This is tedious and error-prone, especially as your transformation pipeline grows more complex.</p>

<h3 id="an-elegant-solution">An Elegant Solution</h3>

<p>fasttransform takes a fundamentally different approach by pairing each transformation with its inverse. Here’s the same pipeline using fasttransform:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">transforms_ft</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
   <span class="n">PILImage</span><span class="p">.</span><span class="n">create</span><span class="p">,</span>
   <span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">"squish"</span><span class="p">),</span>
   <span class="n">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">"crop"</span><span class="p">),</span>
   <span class="n">ToTensor</span><span class="p">(),</span>
   <span class="n">IntToFloatTensor</span><span class="p">(),</span>
   <span class="n">Normalize</span><span class="p">.</span><span class="n">from_stats</span><span class="p">(</span><span class="o">*</span><span class="n">imagenet_stats</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Transform our image
</span><span class="n">fpath</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s">"./huskies_vs_wolves/train/husky/husky_0.jpeg"</span><span class="p">)</span>
<span class="n">img_transformed</span> <span class="o">=</span> <span class="n">transforms_ft</span><span class="p">(</span><span class="n">fpath</span><span class="p">)</span>
<span class="c1"># To reverse the transformations:
</span><span class="n">img_decoded</span> <span class="o">=</span> <span class="n">transforms_ft</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">img_transformed</span><span class="p">)</span>
</code></pre></div></div>

<p>The magic lies in how each transform defines both forward and reverse operations:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Normalize</span><span class="p">(</span><span class="n">Transform</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">mean</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">std</span> <span class="o">=</span> <span class="n">std</span>
        
    <span class="k">def</span> <span class="nf">encodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">std</span>  <span class="c1"># forward transform
</span>    <span class="k">def</span> <span class="nf">decodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">std</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">mean</span>    <span class="c1"># inverse transform
</span></code></pre></div></div>

<p>By defining both <code class="language-plaintext highlighter-rouge">encodes</code> and <code class="language-plaintext highlighter-rouge">decodes</code> methods, fasttransform automatically knows how to reverse your transformations. This is particularly valuable when working with fast.ai v2, where this kind of visualisation capability is built directly into core functions like <code class="language-plaintext highlighter-rouge">show_batch</code> and <code class="language-plaintext highlighter-rouge">show_results</code>.</p>

<h3 id="multiple-dispatch-the-secret-sauce">Multiple Dispatch: The Secret Sauce</h3>

<p>Another powerful feature of fasttransform is how it handles different types of data. Using a concept called <a href="https://www.youtube.com/watch?v=kc9HwsxE1OY">multiple dispatch</a><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, transformations can apply differently based on the type of data they receive.</p>

<p>This becomes particularly valuable when dealing with images and their labels, allowing a single pipeline to handle both:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Function that loads both image and its label
</span><span class="k">def</span> <span class="nf">load_img_and_label</span><span class="p">(</span><span class="n">fp</span><span class="p">):</span> <span class="k">return</span> <span class="n">PILImage</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="n">fp</span><span class="p">),</span> <span class="n">parent_label</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span>

<span class="n">transforms_ft</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
   <span class="n">load_img_and_label</span><span class="p">,</span>  <span class="c1"># Loads both image and label as a tuple
</span>   <span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">"squish"</span><span class="p">),</span>
   <span class="n">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">"crop"</span><span class="p">),</span>
   <span class="n">ToTensor</span><span class="p">(),</span>
   <span class="n">IntToFloatTensor</span><span class="p">(),</span>
   <span class="n">Normalize</span><span class="p">.</span><span class="n">from_stats</span><span class="p">(</span><span class="o">*</span><span class="n">imagenet_stats</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<p>The pipeline intelligently applies each transform only to the appropriate data types, eliminating the need for separate transformation pipelines.</p>

<h3 id="connections-to-julias-multiple-dispatch">Connections to Julia’s Multiple Dispatch</h3>

<p>Interestingly, the concept of multiple dispatch that fasttransform leverages is a core feature of the Julia programming language. In Julia, which method of a function gets called depends on the types of all arguments, not just the first one (as in traditional object-oriented programming).<br />
As explained in Julia’s documentation: “<em>Using all of a function’s arguments to choose which method should be invoked, rather than just the first, is known as multiple dispatch. Multiple dispatch is particularly useful for mathematical code, where it makes little sense to artificially deem the operations to ‘belong’ to one argument more than any of the others</em>”.<br />
The connection to Julia is particularly illuminating, as it demonstrates how concepts from one language can inspire powerful design patterns in another. Just as Julia’s multiple dispatch enables elegant mathematical code, fasttransform’s implementation of this concept allows for cleaner, more intuitive data pipelines in Python.</p>

<h2 id="conclusion">Conclusion</h2>

<p>fasttransform represents a significant step forward in making machine learning workflows more intuitive and debugging more accessible. By making transformations reversible through paired encode/decode methods and leveraging multiple dispatch to handle different data types intelligently, it solves two fundamental problems in data processing pipelines: the inability to easily reverse transformations to inspect data, and the need for separate transformation pipelines for different types of data.<br />
The ability to easily visualise transformed data isn’t just convenient -it’s essential for understanding model behaviour and catching issues like the wolf/husky example, where models learn spurious correlations rather than intended features.<br />
As machine learning systems grow more complex, tools like fasttransform that improve transparency and the ability to debug become increasingly valuable. Whether working with images, text, time series, or other data types, being able to see what a model sees provides critical insights that might otherwise be missed.<br />
Returning to our wolf/husky example, the ability to easily visualise transformed data allows researchers to immediately identify that their model is learning to detect snow backgrounds rather than animal features -a crucial insight for building more robust models.<br />
Those interested in trying fasttransform can install it with <code class="language-plaintext highlighter-rouge">pip install fasttransform</code> and check out the <a href="https://github.com/AnswerDotAI/fasttransform">official fasttransform documentation</a> for more examples and detailed API references. The library offers these capabilities with minimal performance overhead, as the paired transformation approach adds negligible computational cost while providing significant benefits for debugging and understanding model behaviour.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Rens Dimmendaal, Hamel Husain, &amp; Jeremy Howard. “<a href="https://www.fast.ai/posts/2025-02-20-fasttransform.html">fasttransform: Reversible Pipelines Made Simple</a>” fast.ai blog, February 20, 2025. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>“<a href="https://docs.julialang.org/en/v1/manual/methods/">Methods · The Julia Language</a>” Julia Documentation, docs.julialang.org. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="machine-learning" /><category term="data-processing" /><category term="fast-ai" /><category term="python" /><category term="data-science" /><category term="optimisation" /><category term="best-practices" /><category term="interpretability" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">💡 TIL: A Reactive Python Notebook That Might Replace Jupyter</title><link href="http://0.0.0.0:4000/git-friendly-literate-programming-with-marimo/" rel="alternate" type="text/html" title="💡 TIL: A Reactive Python Notebook That Might Replace Jupyter" /><published>2025-03-22T00:00:00+00:00</published><updated>2025-03-22T00:00:00+00:00</updated><id>http://0.0.0.0:4000/git-friendly-literate-programming-with-marimo</id><content type="html" xml:base="http://0.0.0.0:4000/git-friendly-literate-programming-with-marimo/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>As a long-time Vim/Neovim and IPython user, I’m quite particular about my development environment. So when I say a notebook platform caught my attention enough to consider switching, that’s significant. Recently, I stumbled upon <a href="https://marimo.io/">Marimo</a>, and it might just be the Jupyter alternative I’ve been searching for.</p>

<h2 id="what-is-marimo">What is Marimo?</h2>

<p>Marimo is a reactive Python notebook environment that solves many long-standing issues with traditional notebooks. Unlike Jupyter, which stores notebooks as JSON with embedded code and outputs, Marimo notebooks are pure Python files that are:</p>

<ul>
  <li><strong>Reactive</strong>: Run a cell, and Marimo automatically runs dependent cells or marks them as stale</li>
  <li><strong>Consistent</strong>: No hidden state problems that plague traditional notebooks</li>
  <li><strong>Executable</strong>: Can run as standard Python scripts from the command line</li>
  <li><strong>Git-friendly</strong>: Since they’re just <code class="language-plaintext highlighter-rouge">.py</code> files, they work seamlessly with version control</li>
  <li><strong>Deployable</strong>: Easily share as interactive web apps or slides</li>
</ul>

<h2 id="why-this-matters-for-literate-programming">Why This Matters for Literate Programming</h2>

<p>Literate programming -the approach of writing code as a narrative explanation interleaved with executable components- is incredibly powerful for data science, ML, and AI work. It helps create self-documenting, reproducible research and applications.<br />
The problem with Jupyter has always been that while it looks like literate programming, its execution model (arbitrary cell execution order) and hidden state make it fundamentally unreliable. Marimo solves this by ensuring deterministic execution based on variable dependencies rather than cell position.</p>

<h2 id="key-features-that-won-me-over">Key Features That Won Me Over</h2>

<ol>
  <li><strong>Vim keybindings</strong>: As a Neovim user, this is non-negotiable</li>
  <li><strong>Modern editor features</strong>: GitHub Copilot integration, AI completion, and variable explorer</li>
  <li><strong>Reactive runtime</strong>: No more “did I run all the cells in the right order?” problems</li>
  <li><strong>Interactive elements</strong>: Sliders, tables, and plots that automatically update dependent cells</li>
  <li><strong>SQL integration</strong>: Write SQL against dataframes, databases, or other sources right in your notebook</li>
  <li><strong>Package management</strong>: Built-in support for dependency tracking and isolated environments</li>
  <li><strong>Pure Python storage</strong>: No more JSON files with embedded outputs making git diffs unreadable</li>
</ol>

<h2 id="comparisons-with-other-literate-programming-tools">Comparisons with Other Literate Programming Tools</h2>

<h3 id="plutojl-julia">Pluto.jl (Julia)</h3>

<p>Pluto.jl pioneered the reactive notebook concept that Marimo implements. Both share:</p>
<ul>
  <li>Automatic reactivity based on variable dependencies</li>
  <li>Deterministic execution order</li>
  <li>Interactive UI elements</li>
</ul>

<p><strong>Differences</strong>:</p>
<ul>
  <li>Pluto is Julia-specific; Marimo is Python-specific</li>
  <li>Marimo stores notebooks as standard <code class="language-plaintext highlighter-rouge">.py</code> files; Pluto uses a custom format</li>
  <li>Marimo has more built-in integrations with Python data science libraries</li>
  <li>Pluto has tighter integration with Julia’s capabilities</li>
</ul>

<h3 id="livebook-elixir">Livebook (Elixir)</h3>

<p>Livebook brings reactive notebooks to the Elixir ecosystem, with:</p>
<ul>
  <li>Smart cells for common tasks</li>
  <li>Built-in deployment capabilities</li>
  <li>Collaborative editing</li>
</ul>

<p><strong>Differences</strong>:</p>
<ul>
  <li>Livebook embraces Elixir’s concurrency model; Marimo follows Python’s execution model</li>
  <li>Marimo’s Python foundation makes it more accessible for data science work</li>
  <li>Livebook has more built-in tools for building distributed systems</li>
</ul>

<h2 id="pros-and-cons">Pros and Cons</h2>

<h3 id="pros">Pros</h3>
<ul>
  <li><strong>Reproducibility</strong>: Deterministic execution eliminates the “run cells in wrong order” problem</li>
  <li><strong>Git-friendly</strong>: Pure Python files make version control and collaboration much easier</li>
  <li><strong>No hidden state</strong>: Deleted cell variables are removed from memory</li>
  <li><strong>Deployability</strong>: From notebook to web app with minimal effort</li>
  <li><strong>Testability</strong>: Run standard test suites against your notebooks</li>
  <li><strong>Modern IDE features</strong>: Seems like they’ve thought of everything</li>
</ul>

<h3 id="cons">Cons</h3>
<ul>
  <li><strong>Learning curve</strong>: The reactive model requires a shift in thinking if you’re used to Jupyter</li>
  <li><strong>Ecosystem maturity</strong>: Newer than Jupyter, so fewer third-party extensions</li>
  <li><strong>Performance considerations</strong>: Automatic reactivity could cause issues with expensive computations (though there are options to mitigate this<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>)</li>
  <li><strong>Language limitation</strong>: Python-only, unlike Jupyter which supports multiple kernels</li>
</ul>

<h2 id="getting-started">Getting Started</h2>

<p>Installation is straightforward:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv pip <span class="nb">install </span>marimo
<span class="c"># or with recommended extras</span>
uv pip <span class="nb">install </span>marimo[recommended]

<span class="c"># Try the tutorial</span>
marimo tutorial intro
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>As someone deeply invested in both Vim/Neovim and the Python data ecosystem, Marimo strikes an impressive balance. It brings the benefits of reactive programming to Python notebooks while maintaining the flexibility and familiarity that Python users expect.<br />
What truly sets Marimo apart is how it addresses the fundamental issues of reproducibility and hidden state that have plagued notebooks for years. By treating notebooks as actual programs with deterministic execution, it enables literate programming in a way that Jupyter always promised but never fully delivered.<br />
Is it perfect? No. But it’s the most compelling alternative I’ve seen so far, and I’m seriously considering making the switch for my daily work.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Marimo provides a “lazy” configuration option where cells that would be automatically re-executed are instead marked as “stale”, allowing users to manually control when expensive computations run. Users can also implement caching strategies using Marimo’s built-in caching functionality, compartmentalise heavy computations into separate cells to control their execution flow, or use the @mo.cell decorator with runtime configurations to customise how specific cells behave when dependencies change. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="til" /><category term="data-science" /><category term="python" /><category term="best-practices" /><category term="reproducibility" /><category term="literate-programming" /><category term="jupyter-alternative" /><category term="reactivity" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">🏠 Why Companies and Individuals Are Moving Back from the Cloud</title><link href="http://0.0.0.0:4000/cloud-repatriation-trends-implications/" rel="alternate" type="text/html" title="🏠 Why Companies and Individuals Are Moving Back from the Cloud" /><published>2025-03-20T00:00:00+00:00</published><updated>2025-03-20T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cloud-repatriation-trends-implications</id><content type="html" xml:base="http://0.0.0.0:4000/cloud-repatriation-trends-implications/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>The last decade has witnessed the meteoric rise of cloud computing, with organisations large and small migrating their data, applications, and infrastructure to public cloud environments. The promises were compelling: reduced capital expenditure, unlimited scalability, enhanced flexibility, and access to cutting-edge technologies without the overhead of managing physical infrastructure. However, a notable countertrend has emerged in recent years -cloud repatriation. This phenomenon, sometimes referred to as “reverse cloud migration,” involves moving workloads, applications, and data back from public cloud environments to on-premises data centres, private clouds, or hybrid setups (<a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International, 2023</a>). I’ve previously explored this topic in my article <a href="/cloud-repatriation/">The On-Prem Comeback (aka Cloud Repatriation)</a>, where I introduced the basic concepts and early examples of this trend.<br />
This article explores the growing cloud repatriation movement, examining why organisations and individuals are reconsidering their cloud-first strategies, the key drivers behind these decisions, and how they’re implementing these transitions to achieve more balanced and optimised IT infrastructures.</p>

<h2 id="the-scale-of-the-cloud-repatriation-movement">The Scale of the Cloud Repatriation Movement</h2>

<p>The repatriation trend is not isolated but represents a significant shift in how organisations approach their IT infrastructure strategy. According to a 2021 survey by IDC cited by <a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International</a>, 80% of organisations reported repatriating workloads or data from public cloud environments. More recent data from the end of 2024 showed that 86% of CIOs planned to move some public cloud workloads back to private cloud or on-premises -the highest on record for the Barclays CIO Survey (<a href="https://www.puppet.com/blog/cloud-repatriation">Puppet, 2025</a>).<br />
A recent survey by Rackspace found that nearly seven in 10 companies (69%) have moved at least some applications off the cloud and back to on-premise systems or private clouds (<a href="https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/">ZDNet, 2025</a>).<br />
It’s important to note that this doesn’t represent a wholesale abandonment of cloud computing. Only about 8% of organisations are moving their entire workloads off the cloud, according to an October 2024 IDC survey (<a href="https://www.puppet.com/blog/cloud-repatriation">Puppet, 2025</a>). Most are selectively repatriating specific workloads while maintaining others in the cloud, resulting in more nuanced, hybrid approaches to IT infrastructure.</p>

<h2 id="key-drivers-of-cloud-repatriation">Key Drivers of Cloud Repatriation</h2>

<h3 id="cost-optimisation">Cost Optimisation</h3>

<p>While the cloud initially promised cost savings through reduced capital expenditure and operational flexibility, many organisations have experienced what industry experts call “bill shock” as their cloud usage scales. According to <a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International</a>, “a Gartner study predicts that through 2024, 60% of infrastructure and operations leaders will encounter public cloud cost overruns that negatively impact their on-premises budgets”.<br />
This cost concern is particularly relevant for organisations with predictable, high-volume workloads. According to <a href="https://www.rsa.com/resources/blog/identity-governance-and-administration/cloud-repatriation-why-enterprise-it-is-returning-from-the-cloud/">RSA</a>, the company 37Signals announced that its “cloud exit” would save more than $10 million over five years. Similarly, a 2022 report by Andreessen Horowitz found that repatriation of cloud workloads could reduce cloud bills by 50% or more for some companies (<a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International, 2023</a>).<br />
David Linthicum, a leading consultant and former CTO with Deloitte, attributes much of this cost issue to technical debt: “<em>They didn’t refactor the applications to make them more efficient in running on the public cloud providers. So the public cloud providers, much like if we’re pulling too much electricity off the grid, just hit them with huge bills to support the computational and storage needs of those under-optimized applications</em>” (<a href="https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/">ZDNet, 2025</a>).</p>

<h3 id="performance-and-latency">Performance and Latency</h3>

<p>Performance requirements are driving many repatriation decisions, particularly for applications requiring ultra-low latency. According to <a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International</a>, “a study by the IEEE found that for certain AI workloads, on-premises GPU clusters outperformed cloud-based solutions by up to 30% in terms of performance per dollar”.<br />
This performance concern is especially critical in fields like financial trading, scientific research, and manufacturing where latency can significantly impact outcomes. As <a href="https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully">ComputerWeekly</a> notes, “time-sensitive data includes information that users need to access as rapidly as possible -think financial trading feeds -or where the application is sensitive to latency”.</p>

<h3 id="security-and-compliance">Security and Compliance</h3>

<p>Security concerns and regulatory compliance requirements are powerful motivators for cloud repatriation. According to the Rackspace survey cited by <a href="https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/">ZDNet</a>, data security and compliance concerns were the most common reason for repatriation, cited by 50% of respondents. <br />
The implementation of stringent regulations like GDPR has compelled many organisations to keep certain data within specific geographic boundaries. As <a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International</a> highlights, “<em>The Data Protection Commission reported a 59% increase in GDPR complaints in 2022, underscoring the importance of data sovereignty</em>”.<br />
Despite cloud providers’ significant security investments, many organisations prefer to maintain direct control over their most sensitive data. According to <a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International</a>, “<em>a 2023 Thales Cloud Security Study found that 45% of businesses have experienced a cloud-based data breach or failed audit in the past 12 months, highlighting ongoing security concerns</em>”.</p>

<h3 id="control-and-vendor-lock-in">Control and Vendor Lock-in</h3>

<p>The desire for greater control over hardware and software configurations, along with concerns about vendor lock-in, are also driving repatriation efforts. On-premises infrastructure offers more customisation possibilities that may not be available in public cloud environments.<br />
Richard Robbins, founder of TheTechnologyVault.com, observes that “<em>enterprises don’t like being dependent upon someone else’s cloud infrastructure</em>” (<a href="https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/">ZDNet, 2025</a>). This concern is particularly acute among regulated industries such as financial institutions, which are “<em>moving some or all of their web apps from the cloud back to on-prem or to hybrid setups</em>” due to “vulnerability and downsides to cloud hosting” that make “executives feel nervous about not having more control”.</p>

<h2 id="the-emergence-of-balanced-approaches">The Emergence of Balanced Approaches</h2>

<p>Rather than a binary choice between cloud and on-premises, organisations are increasingly adopting hybrid and multi-cloud approaches that offer the best of both worlds. This trend allows organisations to:</p>

<ul>
  <li>Keep sensitive or high-performance workloads on-premises</li>
  <li>Leverage cloud services for scalability and innovation</li>
  <li>Maintain flexibility to adapt to changing business needs</li>
</ul>

<p>According to <a href="https://blog.trginternational.com/cloud-repatriation-business-return-on-premises">TRG International</a>, “<em>The hybrid cloud market is expected to grow from $85.3 billion in 2022 to $262.4 billion by 2027, according to MarketsandMarkets research</em>”. Similarly, “<em>Flexera’s 2023 State of the Cloud Report revealed that 71% of enterprises are pursuing a hybrid cloud strategy, combining public cloud, private cloud, and on-premises infrastructure</em>”.</p>

<h2 id="personal-cloud-repatriation">Personal Cloud Repatriation</h2>

<p>The repatriation trend isn’t limited to enterprises. Individuals are also exploring self-hosting options for personal data.<br />
For example, <a href="https://hachyderm.io/@Jeffrey04/114175854454606516">a fediverse user</a> recently posted about developing a self-hosted photo album application when faced with cloud storage limitations: “<em>Being an enthusiastic photographer, my partner captured moments of us together. However, the increasing stack of photos is accelerating the imminent explosion of my cloud storage</em>” (<a href="https://kitfucoda.medium.com/a-love-story-in-code-building-my-self-hosted-photo-album-b56a4e89ebdd">KitFu Coda, 2023</a>). This personal project highlights how individuals with technical skills can leverage idle hardware to create cost-effective alternatives to cloud storage services.<br />
As <a href="https://kitfucoda.medium.com/a-love-story-in-code-building-my-self-hosted-photo-album-b56a4e89ebdd">they note</a>, “<em>Self-hosting your own data is becoming a trend these days, and it is really not hard to get started</em>”. This trend parallels the enterprise movement, with individuals seeking greater control, cost savings, and privacy for their personal data.</p>

<h2 id="planning-for-successful-repatriation">Planning for Successful Repatriation</h2>

<p>For organisations considering cloud repatriation, careful planning is essential. Key considerations include:</p>

<ol>
  <li><strong>Workload Assessment</strong>: Not all workloads benefit equally from repatriation. <a href="https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully">ComputerWeekly</a> advises that “<em>broadly, repatriation might be the best option where data is sensitive, time sensitive or expensive to store in the cloud</em>”.</li>
  <li><strong>Infrastructure Preparation</strong>: Organisations must ensure they have the physical capacity, networking, power, and cooling capabilities to support repatriated workloads. According to <a href="https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully">ComputerWeekly</a>, “<em>a large repatriation project might be a prompt to reorganise the datacentre, perhaps by moving to newer equipment that can pack more storage into a single rack or that consumes less power</em>”.</li>
  <li><strong>Skills Assessment</strong>: <a href="https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully">ComputerWeekly</a> notes the importance of having “<em>enough staff to provision and manage a larger system</em>” with the necessary “<em>security and privacy skills needed to handle sensitive data</em>” and “<em>technical know-how to handle mission-critical, latency sensitive applications</em>”.</li>
  <li><strong>Future-Proofing</strong>: <a href="https://www.rsa.com/resources/blog/identity-governance-and-administration/cloud-repatriation-why-enterprise-it-is-returning-from-the-cloud/">RSA</a> emphasises the importance of maintaining flexibility: “<em>Organizations should consider the long-term implications of repatriation for their overall IT strategy. This includes planning for future scalability, considering how repatriation fits into the broader digital transformation initiatives, and ensuring that the new infrastructure aligns with long-term business goals</em>”.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>Cloud repatriation represents a maturing perspective on IT infrastructure strategy rather than a rejection of cloud computing. As organisations gain experience with cloud environments, they’re becoming more strategic about which workloads belong where, based on factors like cost, performance, security, and control.<br />
The future likely belongs to balanced, hybrid approaches that leverage the strengths of both cloud and on-premises infrastructure. As <a href="https://www.puppet.com/blog/cloud-repatriation">Puppet</a> notes, “<em>Cloud repatriation is not an endpoint, but rather a strategic tool in the ongoing evolution of enterprise IT. It empowers organizations to take control of their digital assets, enhance their security posture, and align their technology infrastructure with their business objectives</em>”.<br />
For both organisations and individuals, the key is making informed decisions about where and how to deploy IT resources based on specific needs rather than following blanket “cloud-first” or “on-premises-first” policies. This nuanced approach to infrastructure strategy will likely characterise the next phase of digital transformation as the industry moves beyond the initial hype cycles of cloud adoption.</p>]]></content><author><name></name></author><category term="cloud" /><category term="on-prem" /><category term="performance" /><category term="security" /><category term="mlops" /><category term="deployment" /><category term="best-practices" /><category term="data-science" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">⚙️ Turning Data Science into Real-World Value with The Drivetrain Framework</title><link href="http://0.0.0.0:4000/the-drivetrain-method/" rel="alternate" type="text/html" title="⚙️ Turning Data Science into Real-World Value with The Drivetrain Framework" /><published>2025-03-20T00:00:00+00:00</published><updated>2025-03-20T00:00:00+00:00</updated><id>http://0.0.0.0:4000/the-drivetrain-method</id><content type="html" xml:base="http://0.0.0.0:4000/the-drivetrain-method/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>Most data science initiatives fail to deliver meaningful impact. Why? Because they focus on prediction rather than action. Organisations spend millions building sophisticated prediction models that tell them what <em>might</em> happen, but provide no clear path to influencing outcomes.<br />
This gap between prediction and value creation is what Jeremy Howard, data scientist and entrepreneur, addressed in his transformative “<a href="https://www.youtube.com/watch?v=vYrWTDxoeGg">Drivetrain Framework</a>” back in 2012. Having successfully applied this approach to revolutionise insurance pricing, Howard outlines a systematic method for connecting data science to tangible business results.<br />
The framework isn’t about building more complex algorithms -it’s about constructing systems that link predictions to decisions that drive value. If you’re struggling to translate advanced analytics into bottom-line results or finding your data science investments yield interesting insights but limited action, this framework offers a practical solution to bridge that gap.</p>

<h2 id="the-four-critical-components">The Four Critical Components</h2>

<p>The Drivetrain Framework consists of four interconnected steps that bridge the gap between data and value:</p>

<h3 id="1-define-your-objective">1. Define Your Objective</h3>

<p>Begin with absolute clarity about what you’re trying to achieve. In Howard’s insurance example, the objective was straightforward: maximise profit from each customer based on price. For Google’s search engine, it was finding the most relevant web page based on a query. For a marketing team, it might be maximising customer lifetime value.<br />
Without a clear objective, data science becomes an academic exercise. With one, it becomes a targeted tool for value creation.</p>

<h3 id="2-identify-your-levers">2. Identify Your Levers</h3>

<p>Next, determine what variables you can actually control. These are your “levers” -the actions you can take to influence outcomes:</p>

<ul>
  <li>For Google, the key lever was the ordering of search results</li>
  <li>For insurers, it was the price offered to each customer</li>
  <li>For marketers, levers include product recommendations, discount offers, and communication timing</li>
</ul>

<p>The insight here is focusing not on what you can predict, but on what you can change.</p>

<h3 id="3-collect-causal-data">3. Collect Causal Data</h3>

<p>Howard emphasises a crucial distinction: most organisations have plenty of observational data showing correlations, but lack causal data showing what happens when you pull different levers.<br />
This requires intentional experimentation:</p>

<ul>
  <li>The insurance company randomly varied prices to understand true price-response relationships</li>
  <li>A marketing team might randomly test diverse recommendations rather than showing what customers already like</li>
</ul>

<p>The counterintuitive insight: You must sometimes sacrifice short-term optimisation to collect data that enables superior long-term results. Howard convinced insurance executives to randomise pricing for six months -initially accepting potentially lower profits -to build models that later significantly increased their profitability and transformed how the entire industry approached pricing.</p>

<h3 id="4-build-an-integrated-system">4. Build an Integrated System</h3>

<p>The final step combines three elements to connect levers to objectives:</p>

<ul>
  <li><strong>Modeller</strong>: Build predictive models for key relationships (e.g. how price affects purchase probability)</li>
  <li><strong>Simulator</strong>: Combine models to predict outcomes of actions (e.g. how price changes affect profit across customer segments)</li>
  <li><strong>Optimizer</strong>: Find the best lever settings to achieve objectives (e.g. optimal price for each customer)</li>
</ul>

<p>This integrated approach replaces the need for complex “PageRank-like” algorithms with systems that combine simpler models to optimise real-world outcomes.</p>

<h2 id="application-revolutionising-marketing">Application: Revolutionising Marketing</h2>

<p>Howard suggests marketing analytics remains in the “Dark Ages” and ready for transformation through the Drivetrain approach:<br />
Consider Amazon’s recommendation system. Rather than simply suggesting more books by authors you’ve already read, a Drivetrain-based system would:</p>

<ol>
  <li>Define the objective as maximising customer lifetime value</li>
  <li>Identify recommendation content as a key lever</li>
  <li>Collect causal data by testing diverse recommendations, including unexpected ones</li>
  <li>Build an integrated system that models what customers might enjoy but don’t yet know about, and optimises for long-term value</li>
</ol>

<p>In Howard’s experience, companies implementing this approach have seen substantial improvements in customer engagement and retention while achieving meaningful reductions in marketing costs.</p>

<h2 id="drawing-from-engineering">Drawing from Engineering</h2>

<p>Howard notes that many solutions already exist in engineering disciplines, which data scientists would benefit from studying.<br />
Aircraft designers have used integrated models and optimisation for decades, combining aerodynamic models, structural analysis, and optimisation techniques to create planes that safely fly millions of passengers daily.<br />
Building construction similarly relies on systems that integrate architectural models, structural engineering, and materials science to optimize for safety, cost, and aesthetics.<br />
The most advanced example might be Google’s self-driving car, which integrates multiple predictive models (how the car responds to controls, what sensors detect) with optimisation to safely navigate real-world environments, significantly improving safety in testing environments.<br />
These engineering successes demonstrate how combining relatively simple models into integrated systems can solve extraordinarily complex problems.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The Drivetrain Framework represents a fundamental shift in how we should approach data science:</p>

<ol>
  <li>Move beyond building better predictive models in isolation</li>
  <li>Focus on connecting predictions to actions that drive real value</li>
  <li>Invest in collecting causal data through deliberate experimentation</li>
  <li>Integrate modelling, simulation, and optimisation into coherent systems</li>
</ol>

<p>By adopting this framework, organisations can bridge the gap between sophisticated analytics and meaningful results. The companies that will gain competitive advantage aren’t those with marginally better algorithms, but those that build integrated systems connecting data to decisions that create value.</p>

<h2 id="getting-started">Getting Started</h2>

<p>To begin implementing the Drivetrain approach:</p>

<ol>
  <li>Identify one high-value business objective with measurable outcomes</li>
  <li>Map the specific levers your team can control that influence this objective</li>
  <li>Design small-scale experiments to collect causal data about these relationships</li>
  <li>Start simple -build basic models for key relationships, then integrate them before attempting sophisticated optimisation</li>
</ol>

<p>The most important step is shifting your thinking from “<em>what can we predict?</em>” to “<em>what actions can we take to create value?</em>” -the essence of the Drivetrain Framework.</p>]]></content><author><name></name></author><category term="data-science" /><category term="decision-making" /><category term="machine-learning" /><category term="modelling-mindsets" /><category term="optimisation" /><category term="fast-ai" /><category term="advantage" /><category term="best-practices" /><category term="design-principles" /><category term="causal-inference" /><category term="business-value" /><category term="predictive-modelling" /><category term="integration" /><category term="deliberate-experimentation" /><category term="real-value" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">📦 From Compilation to Containerisation and Back Again</title><link href="http://0.0.0.0:4000/compilation-going-back-full-circle/" rel="alternate" type="text/html" title="📦 From Compilation to Containerisation and Back Again" /><published>2025-03-19T00:00:00+00:00</published><updated>2025-03-19T00:00:00+00:00</updated><id>http://0.0.0.0:4000/compilation-going-back-full-circle</id><content type="html" xml:base="http://0.0.0.0:4000/compilation-going-back-full-circle/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>Over the years, I’ve experimented with numerous programming languages and deployment strategies. Python has been my domain’s lingua franca -with its vast ecosystem for data science and AI applications. However, its deployment complexities have consistently been a pain point: managing dependencies, configuring containers, and setting up build pipelines.<br />
This search for a better alternative has led me through statically compiled languages like Go and Rust; JIT-compiled languages like Julia; and hosted languages like Clojure and Scala. Yet most failed to provide a good balance between ecosystem richness and deployment simplicity. Recently, however, Deno 2.0 has emerged as a compelling solution -particularly with its ability to compile TypeScript (TS) / JavaScript (JS) to standalone executables.</p>

<h2 id="the-circular-evolution-of-programming-languages">The Circular Evolution of Programming Languages</h2>

<p>Programming languages have undergone a fascinating evolution. In the beginning (the late 1950s and 1960s), languages like Fortran, COBOL, and C were ahead-of-time compiled -transformed directly into machine code executables that could run without additional dependencies.<br />
As computing evolved, the pendulum swung toward higher-level languages -interpreted languages like Python and hosted environments like the JVM- prioritising readability and developer productivity over raw performance. These languages abstracted away machine-level concerns, allowing developers to focus on solving business problems.<br />
Yet this shift introduced new challenges. Python applications often require managing complex dependency trees, virtual environments, and platform-specific configurations. The infamous “<em>works on my machine</em>” problem became so pervasive that containerisation emerged as a solution.<br />
While effective, containerisation introduces its own complexities: orchestration, image management, and networking configurations. What began as a solution to simplify deployment has become a complex system requiring specialised knowledge.</p>

<h2 id="deno-compilation-makes-a-comeback">Deno: Compilation Makes a Comeback</h2>

<p>Deno 2.0 represents a return to first principles. As highlighted in the <a href="https://youtube.com/watch?v=ZsDqTQs3_G0">Run JavaScript Anywhere</a> video, its <code class="language-plaintext highlighter-rouge">compile</code> command enables developers to transform JS and TS programs into standalone binaries that run across major platforms -no runtime installation or dependencies required.</p>

<div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// sample.ts</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">open</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">https://deno.land/x/open/index.ts</span><span class="dl">"</span><span class="p">;</span>

<span class="c1">// Open a URL in the default browser</span>
<span class="k">await</span> <span class="nx">open</span><span class="p">(</span><span class="dl">"</span><span class="s2">https://example.com</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div>

<p>With a simple <code class="language-plaintext highlighter-rouge">deno compile sample.ts</code> command, this code becomes a standalone executable that works on any machine without requiring Deno to be installed.<br />
This compilation process isn’t traditional transpilation to machine code -it embeds your JS and TS code into a specialized Deno runtime binary (denort). Your script and dependencies are bundled as an EZIP file and injected into the runtime binary, creating a self-contained executable that can be code-signed for distribution.</p>

<p>The key benefits include:</p>
<ol>
  <li><strong>Cross-platform compatibility</strong> without runtime requirements</li>
  <li><strong>Simplified deployment</strong> with single-binary distribution</li>
  <li><strong>Bundled assets</strong> for complete portability</li>
  <li><strong>Improved startup times</strong> compared to interpreter-based approaches</li>
</ol>

<p>Deno 2.0 enhances these capabilities further with support for npm packages, web workers, cross-compilation, smaller binary sizes, and code signing with custom icons—making it viable for complete applications, not just scripts.</p>

<h2 id="the-single-language-advantage">The Single Language Advantage</h2>

<p>Beyond deployment simplicity, using a single language across an entire project stack creates significant organisational benefits. I’ve experienced first-hand how using different languages for front-end, back-end, and data science work can create silos within teams.<br />
<a href="https://dockyard.com/blog/2024/02/06/5-benefts-amplified-saw-switching-to-elixir">Amplified’s case study</a> demonstrates this point clearly. After switching from a React/JS front-end and Phoenix/Elixir back-end to an all-Elixir approach with LiveView, they reported:</p>

<ol>
  <li><strong>Halved server costs</strong> through more efficient resource utilisation</li>
  <li><strong>Dramatically increased development speed</strong> by eliminating cross-language silos</li>
  <li><strong>Improved team cohesion</strong> with shared tooling and knowledge</li>
  <li><strong>Enhanced maintainability</strong> through code reuse</li>
  <li><strong>Reduced team size requirements</strong> from 12 developers to just 2</li>
</ol>

<p>TS with Deno provides a similar single language opportunity -allowing teams to build front-end interfaces, back-end services, and data processing workflows with the same toolchain. The JS/TS ecosystem is rapidly maturing for AI, ML, and data science applications, as I noted in my previous article on <a href="/deno/">Modern Data Science and AI Engineering with Deno 2.0</a>.<br />
One often overlooked benefit is the reduced cognitive load when developers don’t need to context-switch between different language paradigms, package managers, testing frameworks, and debugging approaches.</p>

<h2 id="practical-applications">Practical Applications</h2>

<p>Deno’s compilation capabilities shine in several real-world scenarios:</p>

<ol>
  <li><strong>CLI Tools</strong>: Creating self-contained executables that “just work” across platforms without complex installation instructions</li>
  <li><strong>Offline Environments</strong>: Deploying to systems without internet access, where package resolution at runtime isn’t possible</li>
  <li><strong>Cross-Platform Applications</strong>: Building desktop applications that leverage web technologies without requiring a browser runtime</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>We’ve come full circle in programming language evolution -from compiled languages like Fortran in the 1950s, to interpreted languages for improved developer experience, to containerisation for managing deployment complexities, and now back to compilation with Deno<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.<br />
Deno’s approach represents a compelling blend—combining deployment simplicity with the ecosystem richness of modern TS/JS. For AI engineering, this addresses many pain points of Python deployment while maintaining access to growing ecosystem of data science tools.<br />
While Elixir offers similar single language benefits, its distribution story remains a work in progress with projects like <a href="https://github.com/burrito-elixir/burrito">Burrito</a> showing promise but not yet fully mature. Until then, Deno stands out as a viable alternative for simplified deployment without sacrificing ecosystem benefits.<br />
The future of deployment may look surprisingly like its past, just with better languages and tools at our disposal -offering a path toward more cohesive, efficient software development that reduces complexity without sacrificing capability.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Go, Zig, Rust, C/C++ D, Nim, Common Lisp are some prominent examples of ahead-of-time compiled languages that -with the exception of Common Lisp- excel in systems programming. However, Deno allows a ubiquitous, higher-level language like JS and its superset TS to join the club of languages that can easily package code to a cross-platform single binary. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="deno" /><category term="typescript" /><category term="deployment" /><category term="cross-platform" /><category term="evolution" /><category term="toolchain" /><category term="best-practices" /><category term="code-quality" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">🧠 RAG vs CAG: Understanding Knowledge Augmentation in LLMs</title><link href="http://0.0.0.0:4000/rag-or-cag/" rel="alternate" type="text/html" title="🧠 RAG vs CAG: Understanding Knowledge Augmentation in LLMs" /><published>2025-03-18T00:00:00+00:00</published><updated>2025-03-18T00:00:00+00:00</updated><id>http://0.0.0.0:4000/rag-or-cag</id><content type="html" xml:base="http://0.0.0.0:4000/rag-or-cag/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>Large Language Models (LLMs) face a fundamental knowledge problem: they’re limited to information present in their training data. This creates challenges when dealing with recent events that occurred after training or proprietary information specific to an organization.<br />
To address these limitations, two primary augmentation techniques have emerged: Retrieval Augmented Generation (RAG) and Cache Augmented Generation (CAG). This article breaks down both approaches based on  <a href="https://www.youtube.com/channel/UCKWaEZ-_VweaEx1j62do_vQ">IBM Technology</a>’s comprehensive explanation from their <a href="https://youtube.com/watch?v=HdafI0t3sEY">video on RAG vs CAG</a>, examining how they work, their capabilities, and when to use each one.</p>

<h2 id="understanding-rag-and-cag">Understanding RAG and CAG</h2>

<h3 id="retrieval-augmented-generation-rag">Retrieval Augmented Generation (RAG)</h3>

<p>RAG operates through a two-phase system:</p>

<ol>
  <li><strong>Offline Phase (Preparation)</strong>
    <ul>
      <li>Documents are broken into manageable chunks.</li>
      <li>Vector embeddings are created for each chunk using an embedding model.</li>
      <li>These embeddings are stored in a vector database, creating a searchable knowledge index.</li>
    </ul>
  </li>
  <li><strong>Online Phase (Query &amp; Response)</strong>
    <ul>
      <li>The user submits a query.</li>
      <li>The RAG retriever converts this query to a vector using the same embedding model.</li>
      <li>The system performs a similarity search in the vector database.</li>
      <li>It retrieves the most relevant document chunks (typically 3-5 passages).</li>
      <li>These chunks and the user’s query are placed in the LLM’s context window.</li>
      <li>The LLM generates an answer based on both the query and the retrieved context.</li>
    </ul>
  </li>
</ol>

<p>For example, if asked <em>“What film won Best Picture this year?”</em>, the system might retrieve information about <em>“Anora”</em> winning the award, even if this occurred after the model’s original training.</p>

<p>A key advantage of RAG is its modularity—components like the vector database, embedding model, or LLM can be swapped independently without rebuilding the entire system.</p>

<h3 id="cache-augmented-generation-cag">Cache Augmented Generation (CAG)</h3>

<p>CAG takes a fundamentally different approach:</p>

<ul>
  <li>Instead of retrieving knowledge on demand, CAG preloads all available information into the model’s context window</li>
  <li>The entire knowledge corpus is formatted into one massive prompt that fits within the model’s context limits</li>
  <li>The LLM processes this extensive input in a single forward pass</li>
  <li>The model’s internal state is captured in what’s called a “KV cache” (key-value cache)</li>
  <li>When a user query arrives, it’s added to this pre-existing KV cache</li>
  <li>The model can access any relevant information from the cache without reprocessing the entire knowledge base</li>
</ul>

<p>The fundamental distinction: RAG fetches only what it predicts is needed, while CAG loads everything upfront and remembers it for later use.</p>

<h2 id="comparing-capabilities">Comparing Capabilities</h2>

<h3 id="accuracy">Accuracy</h3>
<ul>
  <li><strong>RAG</strong>: Accuracy depends heavily on the retriever component. If the retriever fails to fetch relevant documents, the LLM won’t have the facts needed to answer correctly.</li>
  <li><strong>CAG</strong>: Guarantees that all information is available (assuming it exists in the knowledge base), but places the burden on the LLM to extract the right information from a large context.</li>
</ul>

<h3 id="latency">Latency</h3>
<ul>
  <li><strong>RAG</strong>: Higher latency due to additional steps of embedding the query, searching the index, and processing retrieved text.</li>
  <li><strong>CAG</strong>: Lower latency once knowledge is cached, as answering queries requires only one forward pass without retrieval lookup time.</li>
</ul>

<h3 id="scalability">Scalability</h3>
<ul>
  <li><strong>RAG</strong>: Can scale to millions of documents as only a small portion is retrieved per query.</li>
  <li><strong>CAG</strong>: Limited by the model’s context window size (typically ~32k-100k tokens), restricting it to a few hundred documents at most.</li>
</ul>

<h3 id="data-freshness">Data Freshness</h3>
<ul>
  <li><strong>RAG</strong>: Easy to update incrementally as you add new document embeddings or remove outdated ones.</li>
  <li><strong>CAG</strong>: Requires recomputation when data changes, making it less suitable for frequently updated information.</li>
</ul>

<h2 id="when-to-use-each-approach">When to Use Each Approach</h2>

<p>The video presents several scenarios to illustrate when each approach is more appropriate:</p>

<ol>
  <li><strong>IT Help Desk Bot with Static Manual (200 pages, rarely updated)</strong>
    <ul>
      <li><strong>Best Choice</strong>: CAG</li>
      <li><strong>Rationale</strong>: Knowledge base is small enough to fit in most LLM context windows, information is static, and caching enables faster query responses.</li>
    </ul>
  </li>
  <li><strong>Legal Research Assistant (Thousands of constantly updated documents)</strong>
    <ul>
      <li><strong>Best Choice</strong>: RAG</li>
      <li><strong>Rationale</strong>: Knowledge base is massive and dynamic, precise citations are required, and incremental updates are essential.</li>
    </ul>
  </li>
  <li><strong>Clinical Decision Support System (Patient records, treatment guides, drug interactions)</strong>
    <ul>
      <li><strong>Best Choice</strong>: Hybrid Approach</li>
      <li><strong>Rationale</strong>: Use RAG to retrieve relevant subsets from the massive knowledge base, then load that retrieved content into a long-context model using CAG for follow-up questions.</li>
    </ul>
  </li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>The choice between RAG and CAG ultimately depends on your specific use case. Consider RAG when dealing with large or frequently updated knowledge sources, when citations are necessary, or when resources for running long-context models are limited. CAG is preferable when working with a fixed knowledge set that fits within your model’s context window, when low latency is crucial, or when you want to simplify deployment.<br />
As LLM technology evolves with expanding context windows and improved retrieval mechanisms, we may see these approaches converge or new hybrid solutions emerge. For now, understanding the strengths and limitations of both RAG and CAG allows AI engineers to make informed decisions about knowledge augmentation strategies that best suit their specific applications.</p>]]></content><author><name></name></author><category term="rag" /><category term="llm" /><category term="ai" /><category term="machine-learning" /><category term="prompt-engineering" /><category term="nlp" /><category term="data-processing" /><category term="best-practices" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">🤖 The State of AI Agents in 2025</title><link href="http://0.0.0.0:4000/navigating-ais-frontier-2025/" rel="alternate" type="text/html" title="🤖 The State of AI Agents in 2025" /><published>2025-03-15T00:00:00+00:00</published><updated>2025-03-15T00:00:00+00:00</updated><id>http://0.0.0.0:4000/navigating-ais-frontier-2025</id><content type="html" xml:base="http://0.0.0.0:4000/navigating-ais-frontier-2025/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>The AI landscape has evolved at a breathtaking pace over the past few years, with autonomous AI agents being positioned as the next revolutionary frontier. At the 2025 AI Engineer Summit, Grace Isford, a partner at Lux Capital, delivered an <a href="https://www.youtube.com/watch?v=HS5a8VIKsvA">insightful keynote</a> on “The State of the AI Frontier” that challenged the prevailing narrative about AI agents. While many industry players proclaim that 2025 marks the “perfect storm” for AI agents, Isford’s presentation offered a more nuanced view, highlighting both the tremendous progress and the significant challenges that remain. This article summarises the key insights from her keynote, examining the current state of AI agents and the strategies developers can employ to overcome persistent limitations.</p>

<h2 id="the-perfect-storm-for-ai-agents">The Perfect Storm for AI Agents</h2>

<p>The speaker began by acknowledging the remarkable progress in AI over the past two and a half years. The industry has seen exponential advancements since the release of Stable Diffusion in August 2022, with the pace of innovation only accelerating. 2025 has already witnessed several landmark developments:</p>

<ul>
  <li>The announcement of the $500 billion Stargate project collaboration between the U.S. government, OpenAI, SoftBank, and Oracle</li>
  <li>OpenAI’s o3 model exceeding human performance in the Arc AGI challenge</li>
  <li>DeepSeq’s R1 model launch causing market disruptions and reaching the top of the App Store</li>
  <li>France’s new AI initiative announced at the France AI Summit, bringing Europe back into the global AI race</li>
</ul>

<p>These developments, alongside other factors, have created what many call the “perfect storm” for AI agents:</p>

<ol>
  <li>Reasoning models (like OpenAI’s o1 and o3, DeepSeq’s R1, and Grok’s latest offering) now outperform humans in various benchmarks</li>
  <li>Increased test-time compute (more resources allocated to inference rather than just training)</li>
  <li>Engineering and hardware optimisations driving efficiency</li>
  <li>Cheaper inference and hardware costs</li>
  <li>A narrowing gap between open-source and closed-source models</li>
  <li>Massive infrastructure investments from governments and corporations worldwide</li>
</ol>

<h2 id="the-reality-gap-why-ai-agents-arent-quite-working-yet">The Reality Gap: Why AI Agents Aren’t Quite Working Yet</h2>

<p>Despite this promising landscape, Isford argued that truly autonomous AI agents aren’t functioning as seamlessly as industry hype suggests. To illustrate this point, she shared a real-world example of trying to use OpenAI’s operator to book a flight from New York to San Francisco with specific requirements. Despite seemingly straightforward criteria (departure time after 3 PM, avoiding rush hour, specific airlines, budget constraints, seat preferences), the agent failed to deliver a satisfactory result.</p>

<p>The presenter identified five categories of cumulative errors that prevent AI agents from delivering consistent, reliable results:</p>

<ol>
  <li><strong>Decision Errors</strong>: Choosing incorrect facts or overthinking/exaggerating scenarios</li>
  <li><strong>Implementation Errors</strong>: Encountering access issues or integration failures (like CAPTCHA challenges)</li>
  <li><strong>Heuristic Errors</strong>: Applying wrong criteria or missing critical contextual information</li>
  <li><strong>Taste Errors</strong>: Failing to account for personal preferences not explicitly stated</li>
  <li><strong>Perfection Paradox</strong>: User expectations heightened by AI’s capabilities in some areas lead to frustration when agents perform at merely human speed or make basic errors</li>
</ol>

<p>These errors compound dramatically in complex multi-agent systems with multi-step tasks. Isford presented a compelling visual example showing how even agents with impressive 99% and 95% accuracy rates drop to 60% and 8% reliability respectively after just 50 consecutive steps.</p>

<h2 id="five-strategies-for-building-better-ai-agents">Five Strategies for Building Better AI Agents</h2>

<p>The keynote then shifted to offering concrete strategies for mitigating these challenges and building more effective AI agents:</p>

<h3 id="1-data-curation">1. Data Curation</h3>
<ul>
  <li>Recognise that data is increasingly diverse (text, images, video, audio, sensor data)</li>
  <li>Curate proprietary data, including data generated by the agent itself</li>
  <li>Design “data flywheels” that automatically improve agent performance through user interactions</li>
  <li>Recycle and adapt to user preferences in real-time</li>
</ul>

<h3 id="2-robust-evaluation-systems">2. Robust Evaluation Systems</h3>
<ul>
  <li>Move beyond evaluations for verifiable domains (math, science) to develop frameworks for subjective assessments</li>
  <li>Collect signals about human preferences</li>
  <li>Build personalised evaluation systems that reflect actual user needs</li>
  <li>Sometimes the best evaluation is direct human testing rather than relying solely on benchmarks</li>
</ul>

<h3 id="3-scaffolding-systems">3. Scaffolding Systems</h3>
<ul>
  <li>Implement safeguards to prevent cascading failures when errors occur</li>
  <li>Build complex compound systems that can work together harmoniously</li>
  <li>Incorporate human intervention at critical junctures</li>
  <li>Develop self-healing agents that can recognise their own mistakes and correct course</li>
</ul>

<h3 id="4-user-experience-as-a-competitive-moat">4. User Experience as a Competitive Moat</h3>
<ul>
  <li>Recognise that UX differentiation is crucial when most applications are using the same foundation models</li>
  <li>Deeply understand user workflows to create elegant human-machine collaboration</li>
  <li>Integrate seamlessly with existing systems to deliver tangible ROI</li>
  <li>Focus on industries with proprietary data sources and specialised workflows (robotics, manufacturing, life sciences)</li>
</ul>

<h3 id="5-multimodal-approaches">5. Multimodal Approaches</h3>
<ul>
  <li>Move beyond basic chatbot interfaces to create more human-like experiences</li>
  <li>Incorporate multiple sensory capabilities (vision, voice, and potentially touch or smell)</li>
  <li>Build personal memory systems that understand users on a deeper level</li>
  <li>Transform inconsistent but visionary products into experiences that exceed expectations through novel interfaces</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>While 2025 has created what appears to be a perfect storm for AI agents with advanced reasoning models, increased compute efficiency, and massive infrastructure investments, the reality is that autonomous AI agents still face significant challenges. The cumulative effect of small errors across decision-making, implementation, heuristics, and user preferences creates substantial reliability issues in complex agent systems.</p>

<p>However, as this keynote emphasised, these challenges are not insurmountable. By focusing on meticulous data curation, developing sophisticated evaluation frameworks, implementing robust scaffolding systems, prioritising distinctive user experiences, and embracing multimodal approaches, developers can build AI agents that deliver on their transformative potential. The lightning strike of truly autonomous, reliable AI agents may not have happened yet, but with these strategies, the industry is moving steadily toward that breakthrough moment.</p>]]></content><author><name></name></author><category term="ai" /><category term="machine-learning" /><category term="llm" /><category term="best-practices" /><category term="evaluation" /><category term="prompt-engineering" /><category term="decision-making" /><summary type="html"><![CDATA[]]></summary></entry></feed>