<!DOCTYPE html>
<html lang="en"><head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>üí° TIL: The Matrix Equation That Makes Linear Regression Work | Just-in-Time learning</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="üí° TIL: The Matrix Equation That Makes Linear Regression Work" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Inquisitive. Learning. Sharing. Simplicity = Reliability" />
<meta property="og:description" content="Inquisitive. Learning. Sharing. Simplicity = Reliability" />
<link rel="canonical" href="http://0.0.0.0:4000/TIL-lin-alg-applied-to-stats/" />
<meta property="og:url" content="http://0.0.0.0:4000/TIL-lin-alg-applied-to-stats/" />
<meta property="og:site_name" content="Just-in-Time learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-01-08T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="üí° TIL: The Matrix Equation That Makes Linear Regression Work" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-01-08T00:00:00+00:00","datePublished":"2025-01-08T00:00:00+00:00","description":"Inquisitive. Learning. Sharing. Simplicity = Reliability","headline":"üí° TIL: The Matrix Equation That Makes Linear Regression Work","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/TIL-lin-alg-applied-to-stats/"},"url":"http://0.0.0.0:4000/TIL-lin-alg-applied-to-stats/"}</script>
<!-- End Jekyll SEO tag -->
<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://0.0.0.0:4000/feed.xml" title="Just-in-Time learning" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
delimiters: [
{left: "$$", right: "$$", display: true},
{left: "[%", right: "%]", display: true},
{left: "$", right: "$", display: false}
]}
);
        });
</script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
                if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
                }
                });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>











<link rel="stylesheet" href="/assets/css/style.css">
<script src="/assets/js/theme.js"></script>

</head>
<body><script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js"></script>
<script src="/assets/js/search.js"></script>

<header class="site-header">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/">Just-in-Time learning</a>
    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path
              d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,12.031,18,12.696,18,13.516L18,13.516z" />
          </svg>
        </span>
      </label>

      <div class="trigger">
        <div class="search-container">
          <input type="text" id="search-input" placeholder="Search..." aria-label="Search posts" class="search-input">
          <div id="search-results" class="search-results"></div>
        </div>
        <a class="page-link" href="/aihub">AI Hub</a>
        <a class="page-link" href="/about">About</a>
        <button class="theme-toggle" onclick="toggleTheme()" id="theme-toggle">
  üåô Dark
</button>

<script>
function toggleTheme() {
    const body = document.querySelector('body');
    const button = document.getElementById('theme-toggle');
    const isDark = body.classList.toggle('dark-theme');
    
    button.innerHTML = isDark ? '‚òÄÔ∏è Light' : 'üåô Dark';
    localStorage.setItem('darkTheme', isDark);
}

document.addEventListener('DOMContentLoaded', function() {
    if (localStorage.getItem('darkTheme') === 'true') {
        document.querySelector('body').classList.add('dark-theme');
        document.getElementById('theme-toggle').innerHTML = '‚òÄÔ∏è Light';
    }
});
</script>

      </div>
    </nav>
  </div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">
  <h1>üí° TIL: The Matrix Equation That Makes Linear Regression Work</h1>

  <div class="post-meta">
    <span class="post-date">Jan 8, 2025</span>
    <span class="reading-time">





  3 minutes read

</span>
  </div>
  
  <!-- Debug info -->
  <!-- <div style="color: red;"> -->
  <!--   TOC enabled: true -->
  <!--   Layout: post -->
  <!-- </div> -->
  
  
    <nav class="toc">
  <div class="toc-header">
    <h2>Contents</h2>
    <button class="toc-toggle" aria-label="Toggle table of contents">
      <svg class="toc-icon" viewBox="0 0 24 24" width="24" height="24">
        <path class="chevron-down" d="M6 9l6 6 6-6"></path>
      </svg>
    </button>
  </div>
  <div class="toc-content">
    <ul>
      
      
        
      
        
          
          <li class="toc-level-2">
            <a href="#introduction">Introduction</a>
            
            
            
          </li>
        
      
        
          
          <li class="toc-level-2">
            <a href="#the-theory-an-elegant-mathematical-solution">The Theory: An Elegant Mathematical Solution</a>
            
            
            
          </li>
        
      
        
          
          <li class="toc-level-2">
            <a href="#the-real-world-catch">The Real-World Catch</a>
            
            
            
          </li>
        
      
        
          
          <li class="toc-level-2">
            <a href="#conclusion">Conclusion</a>
            
            
            
          </li>
        
      
    </ul>
  </div>
</nav>

  
  
  <!--more-->

<h2 id="introduction">Introduction</h2>
<p>This morning <a href="https://xcancel.com/andrew_n_carr/status/1876855682529480844">an interesting interview question</a> motivated me to remind myself how it‚Äôs possible to solve linear regression through matrix algebra. Below is what I learned:</p>

<h2 id="the-theory-an-elegant-mathematical-solution">The Theory: An Elegant Mathematical Solution</h2>
<p>Linear regression finds the best-fit line through data points by finding optimal coefficients ($\beta$) that minimise squared errors. The equation $\beta = (X^TX)^{-1}X^Ty$ elegantly solves this optimisation problem using matrix algebra.</p>

<p>The solution involves these key components:</p>
<ol>
  <li>$X$ is our feature matrix (n samples √ó p features)</li>
  <li>$y$ is our target values (n √ó 1)</li>
  <li>$X^T$ is the transpose of X</li>
  <li>$\beta$ is our solution vector (p √ó 1) of coefficients</li>
</ol>

<p>Here‚Äôs how this elegant solution works:</p>
<ol>
  <li>$X^TX$ creates a $(p \times p)$ matrix of feature products:
    <ul>
      <li>Each element $(i,j)$ contains the dot product between features $i$ and $j$</li>
      <li>When features are centred, these products are proportional to covariances<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></li>
      <li>When features are also standardised, it yields correlations scaled by $n$</li>
    </ul>
  </li>
  <li>$(X^TX)^{-1}$ computes the inverse of this matrix:
    <ul>
      <li>Compensates for feature correlations in coefficient calculations<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></li>
      <li>Required for solving the normal equations $X^TX\beta = X^Ty$</li>
      <li>Exists only when no feature is a linear combination of others</li>
    </ul>
  </li>
  <li>$X^Ty$ creates a $(p \times 1)$ vector of feature-target products:
    <ul>
      <li>Each element $i$ contains the dot product of feature $i$ with target $y$</li>
      <li>Represents raw feature-target relationships before adjustment</li>
      <li>When centred, proportional to feature-target covariances<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></li>
    </ul>
  </li>
  <li>Final multiplication $(X^TX)^{-1}X^Ty$:
    <ul>
      <li>Solves the normal equations $X^TX\beta = X^Ty$</li>
      <li>Accounts for inter-feature correlations in determining coefficients</li>
      <li>Mathematically guarantees minimum squared error</li>
    </ul>
  </li>
</ol>

<p>For more information, check Hastie, Tibshirani &amp; Friedman‚Äôs ‚Äú<a href="https://archive.org/details/elementsofstatis0000hast">Elements of Statistical Learning</a>‚Äù seminal book.</p>

<h2 id="the-real-world-catch">The Real-World Catch</h2>
<p>While mathematically elegant, this direct solution has practical limitations in real-world applications:</p>
<ol>
  <li><em>Computational Complexity</em>: Computing $(X^TX)^{-1}$ requires $\Omicron(n^3)$ operations, becoming prohibitively expensive for large feature sets. This is why gradient descent, with its $\Omicron(n^2)$  per-iteration complexity, often proves more practical.</li>
  <li><em>Numerical Instability</em>: When features are highly correlated (like monthly and annual income), $X^TX$ becomes nearly singular<sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>. Even small rounding errors in the computation of its inverse can lead to large errors in $\beta$. In extreme cases, when features are perfectly correlated, the inverse doesn‚Äôt exist at all. Gradient descent avoids this matrix inversion entirely.</li>
  <li><em>Memory Constraints</em>: Large datasets require holding the entire $X^TX$ matrix in memory, while gradient descent can work with mini-batches, making it more memory-efficient.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>
<p>While this equation brilliantly demonstrates the power of linear algebra in statistics, real-world machine learning often favours gradient descent‚Äôs iterative approach. Think of it as choosing between a perfect GPS route through heavy traffic (direct solution) versus taking smaller, adaptable steps through clear side streets (gradient descent). Both reach the same destination, but the practical path often wins in real-world conditions.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>When features are centred (mean = 0), each product becomes $n$ times the covariance. This means $X^TX$ captures how features vary together, which is crucial because correlated features can lead to unstable coefficients if not accounted for. The relationship between $X^TX$ and covariance comes from the definition of sample covariance: $cov(X_i, X_j) = \frac{1}{n-1}\sum_{k=1}^n (x_{ki} - \bar{x_i})(x_{kj} - \bar{x_j})$. When data is centred, this simplifies to $\frac{1}{n-1}(X^TX)_{ij}$.  $\frac{X^TX}{n-1}$ returns the sample covariance matrix. This matters because a) when features are uncentred, $(X^TX)$ gives the sum of products, b) when centred $\frac{X^TX}{n-1}$ gives covariances, c) when also standardised (std = 1), $\frac{X^TX}{n-1}$ gives correlations.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Adjusts coefficient estimates to account for shared information between features. For example, if height and weight are correlated, we need to determine each variable‚Äôs unique contribution to the prediction, not their overlapping effect.¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>A matrix is singular (or non-invertible) when its determinant is zero. In practical terms, this means one or more columns can be expressed as linear combinations of other columns.¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>

</article>

<!-- Theme toggle -->
<script>
// Initialize theme from localStorage
if (localStorage.getItem('darkTheme') === 'true') {
    document.body.classList.add('dark-theme');
}
</script>

<!-- Collapsible TOC -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  const tocHeader = document.querySelector('.toc-header');
  const toc = document.querySelector('.toc');
  
  if (tocHeader && toc) {
    tocHeader.addEventListener('click', function() {
      toc.classList.toggle('collapsed');
      
      // Optional: Save state to localStorage
      localStorage.setItem('tocCollapsed', toc.classList.contains('collapsed'));
    });
    
    // Optional: Restore state from localStorage
    const isCollapsed = localStorage.getItem('tocCollapsed') === 'true';
    if (isCollapsed) {
      toc.classList.add('collapsed');
    }
  }
});
</script>

<div class="post-tags">
  Tags: 
  
    <a href="/">data-science</a>, 
  
    <a href="/">machine-learning</a>, 
  
    <a href="/">statistics</a>, 
  
    <a href="/">ai</a>, 
  
    <a href="/">linear-algebra</a>, 
  
    <a href="/">til</a>, 
  
    <a href="/">modelling-mindsets</a>, 
  
    <a href="/">data-modeling</a>
  
</div>


      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Just-in-Time learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Just-in-Time learning</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/ai-mindset"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">ai-mindset</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Inquisitive. Learning. Sharing. Simplicity = Reliability</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
