{
  "posts": [
    
      {
        "title": "üîß A 10-Minute Guide to Engineering Machine Learning Systems",
        "content": "\n\nIntroduction\nThis is a concise reference guide distilling Martin Zinkevich‚Äôs influential Google article on machine learning best practices. While the original spans 43 detailed rules, this 10-minute summary captures the essential principles for building production ML systems. Whether you‚Äôre starting a new project or reviewing an existing one, this summary can be used as a practical checklist for engineering-focused machine learning.\n\nCore Philosophy\n\n  Do machine learning like the great engineer you are, not like the great machine learning expert you aren‚Äôt.\n\n\nMost ML gains come from great features, not algorithms. The basic approach should be:\n\n  Ensure solid end-to-end pipeline\n  Start with reasonable objective\n  Add common-sense features simply\n  Maintain pipeline integrity\n\n\nPhase I: Before Machine Learning (Rules #1-3)\n\n  Don‚Äôt be afraid to launch without ML\n    \n      Simple heuristics get you 50% of the way\n      Launch with heuristics when data is insufficient\n      Example: Use install rate for app ranking\n    \n  \n  First, design and implement metrics\n    \n      Track everything possible in current system\n      Get early permission from users\n      Design systems with metric instrumentation\n      Implement experiment framework\n    \n  \n  Choose ML over complex heuristics\n    \n      Simple heuristics for launching\n      Complex heuristics become unmaintainable\n      ML models are easier to maintain long-term\n    \n  \n\n\nPhase II: First Pipeline (Rules #4-11)\n\n  Keep first model simple, get infrastructure right\n    \n      Focus on data pipeline integrity\n      Define clear evaluation metrics\n      Plan model integration carefully\n    \n  \n  Pipeline Health is Critical\n    \n      Test infrastructure independently\n      Monitor freshness requirements\n      Watch for silent failures\n      Give feature columns owners\n      Document feature expectations\n    \n  \n  Starting Your ML System\n    \n      Test getting data into algorithm\n      Test getting models out correctly\n      Monitor data statistics continuously\n      Build alerting system\n    \n  \n\n\nYour First Objective (Rules #12-15)\n\n  Choose Objectives Wisely\n    \n      Don‚Äôt overthink initial objective choice\n      Start with simple, observable metrics\n      Use directly observed user behaviours\n      Example: clicks, downloads, shares\n    \n  \n  Model Selection Guidelines\n    \n      Start with interpretable models\n      Separate spam filtering from quality ranking\n      Use simple linear models initially\n      Make debugging easier\n    \n  \n\n\nPhase III: Feature Engineering (Rules #16-22)\n\n  Plan to launch and iterate\n    \n      Expect regular model updates\n      Design for feature flexibility\n      Keep infrastructure clean\n    \n  \n  Feature Engineering Principles\n    \n      Start with directly observed features\n      Use cross-product features wisely\n      Clean up unused features\n      Scale feature complexity with data\n    \n  \n  Feature Coverage and Quality\n    \n      Features that generalize across contexts\n      Monitor feature coverage\n      Document feature ownership\n      Regular feature clean-up\n    \n  \n\n\nHuman Analysis (Rules #23-28)\n\n  Testing and Validation\n    \n      Use crowdsourcing or live experiments\n      Measure model deltas explicitly\n      Look for error patterns\n      Consider long-term effects\n    \n  \n  Common Pitfalls\n    \n      Engineers aren‚Äôt typical users\n      Beware of confirmation bias\n      Quantify undesirable behaviours\n    \n  \n\n\nTraining-Serving Skew (Rules #29-37)\n\n  Prevent Skew\n    \n      Save serving-time features\n      Weight sampled data properly\n      Reuse code between training/serving\n      Test on future data\n    \n  \n  Monitor Everything\n    \n      Track performance metrics\n      Watch data distributions\n      Monitor feature coverage\n      Check prediction bias\n    \n  \n\n\nPhase IV: Optimization and Complex Models (Rules #38-43)\n\n  When to Add Complexity\n    \n      After simple approaches plateau\n      When objectives are well-aligned\n      If maintenance cost justifies gains\n    \n  \n  Advanced Techniques\n    \n      Keep ensembles simple\n      Look for new information sources\n      Balance complexity vs. benefits\n    \n  \n\n\nFinal Recommendations\n\n  Launch Decisions\n    \n      Consider multiple metrics\n      Use proxies for long-term goals\n      Balance simple vs. complex\n    \n  \n  System Evolution\n    \n      Start simple, add complexity gradually\n      Monitor consistently\n      Keep infrastructure clean\n      Document everything\n    \n  \n\n",
        "url": "/ml-best-practices/",
        "date": "January 21, 2025"
      },
    
      {
        "title": "ü§ñ Understanding AI Agents: Tools, Planning, and Evaluation",
        "content": "\n\nIntroduction\nThis article summarises Chip Huyen‚Äôs comprehensive blog post ‚ÄúAgents‚Äù adapted from her upcoming book AI Engineering (2025). The original piece provides an in-depth examination of intelligent agents, which represent a fundamental concept in AI, defined by Russell and Norvig in their seminal 1995 book Artificial Intelligence: A Modern Approach as anything that can perceive its environment through sensors and act upon it through actuators. Huyen explores how the unprecedented capabilities of foundational models have transformed theoretical possibilities into practical applications, enabling agents to operate in diverse environments -from digital workspaces for coding to physical settings for robotics. These agents can now assist with tasks ranging from website creation to complex negotiations.\n\nUnderstanding Agents and Their Tools\nAn agent‚Äôs effectiveness is determined by two key factors: its environment and its tool inventory. The environment defines the scope of possible actions, while tools enable the agent to perceive and act within this environment. Modern agents leverage three distinct categories of tools.\nKnowledge augmentation tools, including text retrievers and web browsing capabilities, prevent model staleness by enabling access to current information. However, web browsing tools require careful API selection to protect against unreliable or harmful content. Capability extension tools address inherent model limitations -for instance, providing calculators for precise arithmetic or code interpreters for programming tasks. These interpreters demand robust security measures to prevent code injection attacks.\nWrite actions represent the most powerful and potentially risky category, enabling agents to modify databases or send emails. These tools are distinguished from read-only actions by their ability to affect the environment directly. The Chameleon system demonstrated the power of tool augmentation, achieving an 11.37% improvement on ScienceQA (a science question answering task) and 17% on TabMWP (a tabular math problem-solving task) through strategic tool combination.\n\n\n    \n           \n        A tool transition tree by Chameleon\n    \n\n\nPlanning and Execution Strategies\nEffective planning requires balancing granularity and flexibility. While Toolformer managed with 5 tools and Chameleon with 13, Gorilla attempted to handle 1,645 APIs, illustrating the complexity of tool selection. Plans can be expressed either in natural language or specific function calls, each approach offering different advantages in maintainability and precision.\nFoundational Model planners require minimal training but need careful prompting, while Reinforcement Learning planners demand extensive training for robustness. Modern planning systems support multiple control flows: sequential, parallel, conditional, and iterative patterns. The ReAct framework successfully combines reasoning with action,\n\n    \n        \n        ReAct agent\n    \n\n\nwhile Reflexion separates evaluation and self-reflection for improved performance.\n\n    \n        \n        Reflexion agent\n    \n\n\nReflection and Error Management\nContinuous reflection and error correction form the backbone of reliable agent systems. The process begins with query validation, continues through plan assessment, and extends to execution monitoring. Chameleon‚Äôs tool transition analysis shows how tools are commonly used together, while Voyager‚Äôs skill manager builds on this by tracking and reusing successful tool combinations.\n\nEvaluation Framework\nAgent evaluation requires a comprehensive approach to failure mode analysis. Planning failures might involve invalid tools or incorrect parameters, while tool-specific failures demand targeted analysis. Efficiency metrics must consider not just step count and costs, but also completion time constraints. When comparing AI and human agents, it‚Äôs essential to recognise their different operational patterns -what‚Äôs efficient for one may be inefficient for the other. Working with domain experts helps identify missing tools and validate performance metrics.\n\nConclusion\nHuyen‚Äôs analysis demonstrates that successful AI agents emerge from the careful orchestration of three key elements: strategic tool selection, sophisticated planning mechanisms, and robust evaluation frameworks. While tools dramatically enhance agent capabilities -as evidenced by Chameleon‚Äôs significant performance improvements- their effectiveness depends on thoughtful curation, balancing between Toolformer‚Äôs minimal approach and Gorilla‚Äôs extensive API integration. The integration of planning frameworks like ReAct and Reflexion shows how combining reasoning with action and incorporating systematic reflection can enhance agent performance. However, as an emerging field without established theoretical frameworks, significant challenges remain in tool selection, planning efficiency, and error management. Future developments will focus on agent framework evaluation and memory systems for handling information beyond context limits, while maintaining the delicate balance between capability and control that Huyen emphasizes throughout her analysis.\n",
        "url": "/agents-chip-huyen/",
        "date": "January 14, 2025"
      },
    
      {
        "title": "üí° TIL: A Simple Yet Effective Ensemble Technique called Model Soup üç≤",
        "content": "\n\nIntroduction\nWhile most ensemble methods in machine learning combine model predictions, thanks to Chris Albon I recently learned about an alternative approach called ‚Äúmodel soups‚Äù that works directly with model parameters. Instead of aggregating outputs, model soups blend the actual weights and biases of neural networks, showing promising results in computer vision and language tasks.\n\n\n   \n\n\nMain Concept\nModel soups are created by averaging the parameters (weights and biases) of multiple independently trained neural networks that share the same architecture and training setup. For example, if we have three models with weights 2.32, 4.21, and 1.23 for a particular parameter, the ‚Äúsouped‚Äù model would use (2.32 + 4.21 + 1.23) / 3 = 2.587 for that parameter. This process is repeated across all parameters in the network. However, not all parameter combinations lead to improvements -models typically need similar training datasets, optimisation methods, and hyperparameters (like learning rate and batch size) to blend effectively. When done right, parameter-averaged models can outperform both individual networks and traditional prediction-averaging ensembles, while maintaining the inference speed of a single model.\n\nConclusion\nModel soups challenge our intuitions about neural networks by showing that directly averaging weights can produce better results than averaging predictions. While the technique requires careful consideration of training conditions, it provides a computationally efficient way to combine multiple models into a single network, making it particularly valuable for resource-constrained production environments where running multiple models in parallel isn‚Äôt feasible.\n",
        "url": "/TIL-model-soups/",
        "date": "January 10, 2025"
      },
    
      {
        "title": "üìê Sparse Autoencoders: A Technical Overview",
        "content": "\n\nIntroduction\nSupervised learning has achieved remarkable successes in areas ranging from computer vision to genomics. However, as Andrew Ng points out in his CS294A lecture notes, it faces a fundamental limitation: the need for manually engineered features. While researchers have spent years crafting specialised features for vision, audio, and text processing, this approach neither scales nor generalises well.\nSparse autoencoders offer an elegant solution to this challenge by automatically learning features from unlabelled data. These neural networks are distinguished by two key characteristics:\n\n  They attempt to reconstruct their input, forcing them to capture essential data patterns\n  They employ a sparsity constraint that mimics biological neural systems, where neurons fire infrequently and selectively\n\n\nWhile simple implementations may not outperform hand-engineered features in specific domains like computer vision, their strength lies in their generality and biological plausibility. The sparse coding principle has proven effective across diverse domains including audio, text, and visual processing.\nThe mathematical framework combines reconstruction error, regularisation, and sparsity penalties to learn efficient, interpretable representations. This approach not only advances machine learning capabilities but also provides insights into how biological neural networks might learn and process information.\nThis overview examines the mathematical foundations, practical implementation, and emergent properties of sparse autoencoders, following the framework presented in Stanford‚Äôs CS294A course notes.\n\nSparse Autoencoders\nAn autoencoder is a neural network that learns to reconstruct its input. In a sparse autoencoder, we add a critical biological constraint: neurons should be ‚Äúinactive‚Äù most of the time, mimicking how biological neurons exhibit low average firing rates.\nThe basic architecture is:\nInput (x) -&gt; Hidden Layer (sparse activation) -&gt; Output (xÃÇ)\n\nWhere:\n\n  Input and output dimensions are equal $(x, \\hat{x} \\in \\R^n)$\n  Hidden layer learns a sparse representation\n  Network uses sigmoid activation: $f(z) = \\frac{1}{1+e^{-z}}$\n\n\nMathematical Framework\n\n\n  \n    Base Cost Function (single training example):\n\n\\[J(W,b; x,y) = \\frac{1}{2}||h_{W,b}(x) - y||^2\\]\n\n    For a single training example:\n     - Measures reconstruction error between network output $h_{W,b}(x)$ and target $y$\n     - For autoencoders: $y = x$ (we reconstruct the input)\n     - $\\frac{1}{2}$ factor simplifies gradient computations\n     - Squared L2 norm penalises larger reconstruction errors quadratically\n  \n  \n    Full Cost Function with Weight Decay:\n\n    The cost function $J(W,b)$ combines the average reconstruction error\n $\\frac{1}{m}\\sum_{i=1}^m \\frac{1}{2}||h_{W,b}(x^{(i)}) - x^{(i)}||^2$\n\n    with the weight decay regularisation, to prevent overfitting by penalising large weights:\n $\\frac{\\lambda}{2}\\sum_{l=1}^{n_l-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(W_{ji}^{(l)})^2$\n\n\\[J(W,b) = \\left[\\frac{1}{m}\\sum_{i=1}^m \\frac{1}{2}||h_{W,b}(x^{(i)}) - y^{(i)}||^2\\right] + \\frac{\\lambda}{2}\\sum_{l=1}^{n_l-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(W_{ji}^{(l)})^2\\]\n\n    Key points:\n    \n      For autoencoders, output $y^{(i)}$ equals input $x^{(i)}$\n      Weight decay applies only to weights $W$, not biases $b$\n      $\\lambda$ balances reconstruction accuracy vs. weight magnitude\n      The $\\frac{1}{2}$ factor simplifies derivative calculations in backpropagation\n      This regularisation is distinct from the sparsity constraint (KL divergence term)\n    \n  \n  \n    Sparsity Measurement:\n\n    The average activation $\\hat{\\rho}_j$ measures how frequently hidden unit $j$ fires across the training set:\n\n\\[\\hat{\\rho}_j = \\frac{1}{m}\\sum_{i=1}^m[a_j^{(2)}(x^{(i)})]\\]\n\n    Key points:\n    \n      $a_j^{(2)}(x^{(i)})$ is hidden unit $j$‚Äôs activation for input $x^{(i)}$\n      With sigmoid activation:\n        \n          Values near 1 mean ‚Äúactive‚Äù or ‚Äúfiring‚Äù\n          Values near 0 mean ‚Äúinactive‚Äù\n        \n      \n      We constrain $\\hat{\\rho}_j \\approx \\rho$ where $\\rho$ is small (typically 0.05)\n      This enforces selective firing: each neuron responds strongly to specific input patterns\n    \n  \n  \n    Sparsity Penalty (using KL divergence):\n\n    The sparsity penalty uses KL divergence to enforce \\(\\hat{\\rho}_j \\approx \\rho\\):\n\n\\[\\sum_{j=1}^{s_2}\\rho\\log\\frac{\\rho}{\\hat{\\rho}_j} + (1-\\rho)\\log\\frac{1-\\rho}{1-\\hat{\\rho}_j}\\]\n\n    Properties of this penalty:\n    \n      Minimised (zero) when $\\hat{\\rho}_j = \\rho$\n      Monotonically increases as $\\hat{\\rho}_j$ deviates from $\\rho$\n      Becomes infinite as $\\hat{\\rho}_j$ approaches 0 or 1\n    \n  \n  \n    Final Cost Function:\n\n\\[J_{sparse}(W,b) = J(W,b) + \\beta\\sum_{j=1}^{s_2}KL(\\rho||\\hat{\\rho}_j)\\]\n\n    Components:\n    \n      $J(W,b)$: Standard autoencoder cost (reconstruction error + weight decay)\n      Sparsity term: KL divergence penalty summed over $s_2$ hidden units\n    \n\n    $\\beta$ controls:\n    \n      Balance between accurate reconstruction and sparse representation\n      Strength of sparsity enforcement\n      Higher $\\beta$ ‚Üí stronger sparsity constraint\n    \n\n    This formulation naturally penalises both over- and under-activation of hidden units relative to target sparsity $\\rho$.\n  \n\n\nTraining Process\n\nThe key modification to standard backpropagation occurs in the hidden layer:\n\n\\[\\delta_i^{(2)} = \\left(\\sum_{j=1}^{s_3}W_{ji}^{(3)}\\delta_j^{(3)}\\right)f'(s_i^{(2)}) + \\beta\\left(-\\frac{\\rho}{\\hat{\\rho}_i} + \\frac{1-\\rho}{1-\\hat{\\rho}_i}\\right)\\]\n\nWhere:\n\n  First term: Standard backpropagation gradient through the network\n  Second term: Gradient of KL-divergence sparsity penalty\n  $s_i^{(2)}$ is weighted input sum to hidden unit $i$\n  $\\hat{\\rho}_i$ must be pre-computed using full training set\n\n\nThis modification ensures gradient descent optimises both reconstruction accuracy and sparsity.\n\nPractical Guidelines\n\n\n  $\\rho$ ‚âà 0.05 (5% target activation rate)\n  $\\beta$ controls sparsity penalty strength\n  Initialise weights randomly near zero\n  Must compute forward pass on all examples first to calculate $\\hat{\\rho}$\n\n\nResults\nWhen trained on images, the network naturally learns edge detectors at different orientations, similar to what is found in the visual cortex. This emergence of biologically plausible features validates the sparsity approach.\n\nConclusion\n\nSparse autoencoders represent a mathematically principled approach to unsupervised feature learning, combining biological inspiration with rigorous optimisation techniques. Their key innovation lies in the sparsity constraint, implemented through KL divergence, which forces hidden units to develop specialised, interpretable features.\n\nThe mathematical framework achieves this through three key components:\n\n  A reconstruction cost that ensures faithful data representation\n  A weight decay term that prevents overfitting\n  A sparsity penalty that enforces selective neural activation\n\n\nThis formulation has proven successful in practice, typically leading to:\n\n  Edge and feature detectors emerging naturally from visual data\n  Interpretable representations comparable to biological neural coding\n  Robust feature learning even with overcomplete hidden layers\n\n\nThe practical value of sparse autoencoders extends beyond their theoretical elegance -they provide a foundation for understanding how neural networks can learn meaningful data representations without supervision. Their success in learning biologically plausible features validates both their design principles and their potential for advanced machine learning applications. Their main limitation lies in hyperparameter sensitivity, particularly to the sparsity target œÅ and weight Œ≤, requiring careful tuning for optimal performance.\n",
        "url": "/sparse-autoencoders/",
        "date": "January 9, 2025"
      },
    
      {
        "title": "üîç Understanding LLM Interpretability",
        "content": "\n\nIntroduction\nLarge Language Models (LLMs) have become increasingly sophisticated, yet understanding their inner workings remains a critical challenge for AI safety and development. This blog post summarises concepts and research presented in Welch Labs‚Äô video on mechanistic interpretability, examining how LLMs process information and recent advances in making their decision-making processes more transparent.\n\nHow LLMs Think\nLLMs process text through a sophisticated pipeline:\n\n  Text is converted into tokens and mapped to vectors\n  These vectors flow through multiple layers via ‚Äúresidual streams‚Äù\n  Each layer transforms the information through attention mechanisms\n  Final outputs emerge from probability distributions across possible tokens\n\n\nThis process, while mathematically precise, creates a black box of neural connections that resist simple interpretation.\n\nThe Challenge of Model Transparency\nGoogle Gemma models‚Äô analysis of the sentence ‚Äúthe reliability of Wikipedia is very‚Äù demonstrates this complexity. The model assigns varying probabilities to different completions:\n\n  ‚Äúimportant‚Äù (20.21%)\n  ‚Äúhigh‚Äù (11.16%)\n  ‚Äúquestionable‚Äù (9.48%)\n\n\nThese probabilities emerge from intricate interactions between neurons, leading to a phenomenon called superposition1.\n\nSuperposition and Its Solution\nUnlike vision models where neurons correspond to specific concepts, LLMs exhibit polysemanticity -individual neurons respond to multiple, unrelated concepts. This occurs because LLMs encode more concepts than available neurons by using specific neuron combinations.\n\nThis complexity necessitated the development of sparse autoencoders, which:\n\n  Map complex neuron combinations to specific concepts\n  Extract interpretable features from LLMs\n  Enable direct manipulation of model behaviour\n\n\nPractical Implications\nUnderstanding LLM internals has crucial implications:\n\n  AI Safety: Better control over model behaviours and outputs\n  Development: More targeted improvements in model capabilities\n  Deployment: Enhanced ability to predict and prevent unwanted behaviours\n  Trust: Greater transparency in AI decision-making processes\n\n\nConclusions\nWhile tools like sparse autoencoders have provided unprecedented insights into model behaviour, they‚Äôve also revealed the vast complexity of LLM internal mechanisms -the ‚Äúdark matter‚Äù of AI. As these models become more integral to society, advancing our ability to interpret and control them becomes increasingly critical for responsible AI development.\nThis improved understanding represents not just academic progress, but a crucial step toward safer, more reliable AI systems.\n\n\n\n  \n    \n      superposition in the context of neural networks is the ability of a single neuron to represent multiple features simultaneously.  https://hdl.handle.net/1721.1/157073¬†&#8617;\n    \n  \n\n",
        "url": "/interpreting-llms/",
        "date": "January 9, 2025"
      },
    
      {
        "title": "üí° TIL: The Matrix Equation That Makes Linear Regression Work",
        "content": "\n\nIntroduction\nThis morning an interesting interview question motivated me to remind myself how it‚Äôs possible to solve linear regression through matrix algebra. Below is what I learned:\n\nThe Theory: An Elegant Mathematical Solution\nLinear regression finds the best-fit line through data points by finding optimal coefficients ($\\beta$) that minimise squared errors. The equation $\\beta = (X^TX)^{-1}X^Ty$ elegantly solves this optimization problem using matrix algebra.\n\nThe solution involves these key components:\n\n  $X$ is our feature matrix (n samples √ó p features)\n  $y$ is our target values (n √ó 1)\n  $X^T$ is the transpose of X\n  $\\beta$ is our solution vector (p √ó 1) of coefficients\n\n\nHere‚Äôs how this elegant solution works:\n\n  $X^TX$ creates a $(p \\times p)$ matrix of feature products:\n    \n      Each element $(i,j)$ contains the dot product between features $i$ and $j$\n      When features are centred, these products are proportional to covariances1\n      When features are also standardised, it yields correlations scaled by $n$\n    \n  \n  $(X^TX)^{-1}$ computes the inverse of this matrix:\n    \n      Compensates for feature correlations in coefficient calculations2\n      Required for solving the normal equations $X^TX\\beta = X^Ty$\n      Exists only when no feature is a linear combination of others\n    \n  \n  $X^Ty$ creates a $(p \\times 1)$ vector of feature-target products:\n    \n      Each element $i$ contains the dot product of feature $i$ with target $y$\n      Represents raw feature-target relationships before adjustment\n      When centred, proportional to feature-target covariances3\n    \n  \n  Final multiplication $(X^TX)^{-1}X^Ty$:\n    \n      Solves the normal equations $X^TX\\beta = X^Ty$\n      Accounts for inter-feature correlations in determining coefficients\n      Mathematically guarantees minimum squared error\n    \n  \n\n\nFor more information, check Hastie, Tibshirani &amp; Friedman‚Äôs ‚ÄúElements of Statistical Learning‚Äù seminal book.\n\nThe Real-World Catch\nWhile mathematically elegant, this direct solution has practical limitations in real-world applications:\n\n  Computational Complexity: Computing $(X^TX)^{-1}$ requires $\\Omicron(n^3)$ operations, becoming prohibitively expensive for large feature sets. This is why gradient descent, with its $\\Omicron(n^2)$  per-iteration complexity, often proves more practical.\n  Numerical Instability: When features are highly correlated (like monthly and annual income), $X^TX$ becomes nearly singular3. Even small rounding errors in the computation of its inverse can lead to large errors in $\\beta$. In extreme cases, when features are perfectly correlated, the inverse doesn‚Äôt exist at all. Gradient descent avoids this matrix inversion entirely.\n  Memory Constraints: Large datasets require holding the entire $X^TX$ matrix in memory, while gradient descent can work with mini-batches, making it more memory-efficient.\n\n\nConclusions\nWhile this equation brilliantly demonstrates the power of linear algebra in statistics, real-world machine learning often favours gradient descent‚Äôs iterative approach. Think of it as choosing between a perfect GPS route through heavy traffic (direct solution) versus taking smaller, adaptable steps through clear side streets (gradient descent). Both reach the same destination, but the practical path often wins in real-world conditions.\n\n\n\n  \n    \n      When features are centred (mean = 0), each product becomes $n$ times the covariance. This means $X^TX$ captures how features vary together, which is crucial because correlated features can lead to unstable coefficients if not accounted for. The relationship between $X^TX$ and covariance comes from the definition of sample covariance: $cov(X_i, X_j) = \\frac{1}{n-1}\\sum_{k=1}^n (x_{ki} - \\bar{x_i})(x_{kj} - \\bar{x_j})$. When data is centred, this simplifies to $\\frac{1}{n-1}(X^TX)_{ij}$.  $\\frac{X^TX}{n-1}$ returns the sample covariance matrix. This matters because a) when features are uncentred, $(X^TX)$ gives the sum of products, b) when centred $\\frac{X^TX}{n-1}$ gives covariances, c) when also standardised (std = 1), $\\frac{X^TX}{n-1}$ gives correlations.¬†&#8617;\n    \n    \n      Adjusts coefficient estimates to account for shared information between features. For example, if height and weight are correlated, we need to determine each variable‚Äôs unique contribution to the prediction, not their overlapping effect.¬†&#8617;\n    \n    \n      A matrix is singular (or non-invertible) when its determinant is zero. In practical terms, this means one or more columns can be expressed as linear combinations of other columns.¬†&#8617;¬†&#8617;2\n    \n  \n\n",
        "url": "/TIL-lin-alg-applied-to-stats/",
        "date": "January 8, 2025"
      },
    
      {
        "title": "üí° TIL: How Different Societies View and Value Choice",
        "content": "\n\nIntroduction\nToday I revisited a talk on the art of choosing by Sheena Iyengar. A humourous and informative presentation, it reminded me that our assumptions about choice ‚Äìas studied by Prof. Iyengar through research spanning American, European and Asian populations‚Äì reveals fascinating cultural differences in how we perceive and respond to choice. Her research reveals some eye-opening insights that I‚Äôll briefly summarise below.\n\nPerceiving Choice\nFirst, while Americans believe individual choice is sacred (think ‚Äúhave it your way‚Äù), research shows this isn‚Äôt universal. When studying children solving puzzles, Asian-American children actually performed better when their mothers chose for them, while Anglo-American children did better choosing for themselves. This reveals how deeply cultural context shapes not just our preferences, but the actual effectiveness of our choices.\n\nSecond, remember how overwhelming it feels staring at 50 different breakfast cereals? Turns out, people from post-communist countries often saw seven different sodas as just one choice: ‚Äúsoda or no soda.‚Äù This isn‚Äôt because they‚Äôre less sophisticated, it‚Äôs because the ability to spot tiny differences between products is a learned skill -not a natural one.\n\nMost striking was the research on medical decisions. When comparing American and French parents making end-of-life decisions for infants, American parents had more negative emotions and guilt despite insisting on having the choice, while French parents, whose doctors made the decisions, coped better. This challenges the core American belief that having choice is always better.\n\nConcluding with a personal story, Prof. Iyengar -who is blind- shared how she once brought two ‚Äúclearly different‚Äù shades of pink nail polish to her lab. When she removed the labels, half the participants couldn‚Äôt tell them apart. Those who could, chose differently when the labels were present versus absent, showing how marketing narratives shape what we think we‚Äôre choosing.\n\nConclusions\nThe TL;DR is: Through cross-cultural research, Prof. Iyengar shows that how we understand and value choice varies dramatically across cultures. Sometimes, having fewer choices or letting others choose for us might actually lead to better outcomes. \nAs a technologist, inundated with a very wide choice of tools that often offer similar results, I have made the conscious decision to reduce my tooling footprint to the minimum viable toolstack possible. I‚Äôm happy to let more knowledgeable professionals choose, with adequate justification, tools for my line of work but I do disagree with the zealotry that‚Äôs occasionally observed in tech and complemented by big egos.\n",
        "url": "/TIL-the-art-of-choice/",
        "date": "January 8, 2025"
      },
    
      {
        "title": "üíä Lessons for Modern Drug Development from the Golden Age of Antibiotics",
        "content": "\n\nIntroduction\n\nIn a thought-provoking analysis1, Our World in Data reveals a striking paradox in medical progress: the most productive period in antibiotic development occurred in the two decades following World War II, with scientific capabilities far more limited than today. This ‚ÄúGolden Age of Antibiotics‚Äù (1940s-1960s) produced nearly two-thirds of the antibiotic drug classes we still rely on2.\nEven more surprisingly, since 1970 -despite exponential advances in computing power and biotechnology- only eight new classes of antibiotics have been approved2. This indicates a stark decline that threatens the foundation of modern medicine. Traditional screening methods now rediscover existing compounds most of the time rather than finding new ones3.\nModern tools like genome sequencing and systematic screening methods offer unprecedented capabilities. We‚Äôve only identified a small fraction of bacterial species, many of which could harbour new antibiotic compounds2. Yet despite these capabilities, development has stagnated due to fundamental market failures and fragmented research efforts.\nThis article examines this paradoxical inverse relationship between technological capability and antibiotic development: How did the Golden Age achieve such remarkable success with limited tools? Why has progress slowed as our capabilities have grown? Most importantly, what combinations of economic incentives and modern technology could spark a new era of antibiotic discovery?\n\nWhen Urgency Met Innovation\n\nThe Golden Age of Antibiotics stands as medicine‚Äôs most productive period in antimicrobial discovery, yielding over 20 new antibiotic classes -more than double what we‚Äôve developed in the 50 years since2. Three pivotal breakthroughs, coupled with unprecedented coordination, drove this remarkable success.\nThe foundation was laid by Paul Ehrlich‚Äôs systematic approach to drug discovery. By methodically testing hundreds of compounds, he discovered salvarsan in 1910 -the first synthetic antibiotic that effectively treated syphilis2. A second milestone emerged when Alexander Fleming discovered penicillin in 1928. However, the real innovation came through coordinated wartime effort. With infections being the second-most common cause of hospital admissions in the US Army, the U.S. Office of Scientific Research and Development (OSRD) launched a global search for more productive penicillin strains, ultimately finding a high-yielding strain on a cantaloupe4.\nThe third breakthrough came from Selman Waksman‚Äôs insight into soil bacteria. His discovery that soil-dwelling actinomycetes bacteria naturally produce antibiotics led to streptomycin‚Äôs development and opened an entirely new avenue for antibiotic discovery5.\nWhat transformed these breakthroughs into a ‚Äúgolden age‚Äù was unprecedented coordination. The U.S. War Production Board orchestrated collaboration between government, academia, and industry -removing patent restrictions, sharing data, and streamlining clinical trials6. The results were remarkable: some antibiotics, like tetracyclines and macrolides, went from discovery to clinical use within the same year.\n\nScientific Progress and Market Failure\n\nThe contrast between the Golden Age and our current era reflects a fundamental misalignment between public health needs and market incentives2. The market structure fundamentally disfavours antibiotics in two ways:\n\n  Revenue Structure: While chronic disease medications can generate billions in annual revenue over decades, new antibiotics typically generate only tens of millions annually7, by comparison. This revenue gap has driven many large pharmaceutical companies away from antibiotic development8.\n  Conservation Requirements: New antibiotics must be reserved for severe drug-resistant infections, reaching less than 1% of hospitalised patients7. This necessary conservation practice severely limits market potential.\n\n\nMeanwhile, our technological capabilities offer three particularly promising approaches:\n\n  Genome mining: a breakthrough technique that identifies hidden antibiotic genes in microbes that remain dormant under standard laboratory conditions. This computational approach has already yielded promising candidates like humimycins9.\n  Advanced bacterial exploration: research into extreme environments like deep oceans and deserts, where previously ‚Äúunculturable‚Äù bacteria might harbour entirely new antibiotic classes3.\n  Smart combination strategies: exploiting the observation that bacterial resistance to one antibiotic can increase vulnerability to others, opening new therapeutic possibilities10.\n\n\nYet these powerful tools remain underutilised due to insufficient investment and coordination. The challenge isn‚Äôt scientific capability -it‚Äôs the failure to create systems that effectively deploy these technologies within sustainable economic frameworks.\n\nIntegrating Economics and Technology\n\nDrawing from evidence in antibiotic development research, several promising approaches could help overcome current market failures while leveraging modern technological capabilities7.\n\nEconomic Solutions to Market Failures\n\n\n  Subscription Models: The UK has pioneered a system where healthcare systems pay annual fees for antibiotic access rather than per-volume pricing. This addresses both the revenue challenge and conservation requirements by providing stable income while supporting appropriate antibiotic use7.\n  Advance Market Commitments: These provide guaranteed payments to companies that successfully develop new antibiotics, similar to successful vaccine development programs. This directly addresses the revenue uncertainty that has driven companies away from antibiotic development11.\n  Collaborative Funding Initiatives: Organisations like CARB-X and GARDP help smaller companies navigate costly clinical trials, distributing development risks that large pharmaceutical companies are unwilling to bear7.\n\n\nLeveraging Modern Technology\n\nTo maximise the impact of these economic incentives, three technological approaches show particular promise:\n\n  Systematic Genome Mining: Using computational power to identify promising antibiotic-producing genes in bacterial genomes, revealing compounds that traditional screening would miss9.\n  Environmental Exploration: Research into extreme environments could unlock entirely new antibiotic classes, enabled by modern sequencing technologies3.\n  Smart Combination Strategies: Systematic exploration of how resistance to one antibiotic can increase vulnerability to others, offering new therapeutic possibilities10.\n\n\nConclusion\n\nThe story of antibiotic development demonstrates that scientific capability alone cannot drive progress. The Golden Age succeeded through a powerful combination of systematic approaches, unprecedented collaboration, and removal of institutional barriers -even with limited technological tools2.\nToday‚Äôs challenge is fundamentally different. We possess sophisticated tools -from genome mining to advanced screening methods- yet development has stalled. This paradox reveals that progress requires three key elements working in concert: economic incentives, institutional coordination, and technological application7.\nThe evidence-based solutions presented in the original Our World in Data article1 offer a path forward. Market reforms like subscription models and advance market commitments could help correct the fundamental economic misalignment in antibiotic development8. Meanwhile, systematic application of computational tools, genomic analysis, and bacterial exploration could help unlock new classes of antibiotics that traditional methods miss9.\nThe urgency is clear. Antimicrobial resistance threatens to undermine many advances in modern medicine12. However, by combining proven coordination approaches from the Golden Age with modern capabilities and sustainable economic frameworks, we can revitalise antibiotic development for the challenges ahead.\n\n\n\n  \n    \n      Our World in Data (2024). ‚ÄúWhat was the Golden Age of Antibiotics, and how can we spark a new one?‚Äù https://ourworldindata.org/golden-age-antibiotics¬†&#8617;¬†&#8617;2\n    \n    \n      Hutchings, M. I., Truman, A. W., &amp; Wilkinson, B. (2019). Antibiotics: Past, present and future. Current Opinion in Microbiology, 51, 72‚Äì80. https://doi.org/10.1016/j.mib.2019.10.008¬†&#8617;¬†&#8617;2¬†&#8617;3¬†&#8617;4¬†&#8617;5¬†&#8617;6¬†&#8617;7\n    \n    \n      Kolter, R., &amp; Van Wezel, G. P. (2016). Goodbye to brute force in antibiotic discovery? Nature Microbiology, 1(2), 15020. https://doi.org/10.1038/nmicrobiol.2015.20¬†&#8617;¬†&#8617;2¬†&#8617;3\n    \n    \n      Gaynes, R. (2017). The Discovery of Penicillin -New Insights After More Than 75 Years of Clinical Use. Emerging Infectious Diseases, 23(5), 849‚Äì853. https://doi.org/10.3201/eid2305.161556¬†&#8617;\n    \n    \n      Waksman, S. A., &amp; Schatz, A. (1945). Streptomycin‚ÄìOrigin, Nature, and Properties. Journal of the American Pharmaceutical Association, 34(11), 273‚Äì291. https://doi.org/10.1002/jps.3030341102¬†&#8617;\n    \n    \n      Sampat, B. N. (2023). Second World War and the Direction of Medical Innovation. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.4422261¬†&#8617;\n    \n    \n      √Ördal, C., et al. (2020). Antibiotic development -economic, regulatory and societal challenges. Nature Reviews Microbiology, 18(5), 267-274. https://doi.org/10.1038/s41579-019-0293-3¬†&#8617;¬†&#8617;2¬†&#8617;3¬†&#8617;4¬†&#8617;5¬†&#8617;6\n    \n    \n      Renwick, M. J., Brogan, D. M., &amp; Mossialos, E. (2016). A systematic review and critical assessment of incentive strategies for discovery and development of novel antibiotics. The Journal of Antibiotics, 69(2), 73-88. https://doi.org/10.1038/ja.2015.98¬†&#8617;¬†&#8617;2\n    \n    \n      Chu, J., et al. (2016). Discovery of MRSA active antibiotics using primary sequence from the human microbiome. Nature Chemical Biology, 12(12), 1004-1006. https://doi.org/10.1038/nchembio.2207¬†&#8617;¬†&#8617;2¬†&#8617;3\n    \n    \n      Baym, M., Stone, L. K., &amp; Kishony, R. (2016). Multidrug evolutionary strategies to reverse antibiotic resistance. Science, 351(6268), aad3292. https://doi.org/10.1126/science.aad3292¬†&#8617;¬†&#8617;2\n    \n    \n      Kremer, M., Levin, J., &amp; Snyder, C. M. (2020). Advance Market Commitments: Insights from Theory and Experience. AEA Papers and Proceedings, 110, 269-273. https://www.aeaweb.org/articles?id=10.1257/pandp.20201017¬†&#8617;\n    \n    \n      World Health Organization (2024). Antimicrobial Resistance Fact Sheet. https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance¬†&#8617;\n    \n  \n\n",
        "url": "/golden-age-of-antibiotics/",
        "date": "January 7, 2025"
      },
    
      {
        "title": "üí° TIL: Test-Driven Development Is Key to Better LLM System Prompts",
        "content": "\n\nIntroduction\n\n2024 has made clear that writing good automated evaluations for LLM-powered systems is the most critical skill for building useful applications. This insight parallels Anthropic‚Äôs internal approach to system prompt development. As usual, Simon Willison‚Äôs recent insightful 2024 LLM overview was a treasure trove. One item I picked up on was evaluating system prompts using a test-driven approach.\n\nThe Evaluation-First Approach\n\nAmanda Askell, leading fine-tuning at Anthropic, outlines a test-driven process for system prompts:\n\n  Create a test set of messages where the model‚Äôs default behaviour fails to meet requirements\n  Develop a system prompt that passes these tests\n  Identify cases where the system prompt is misapplied and refine it\n  Expand the test set and repeat\n\n\nThis methodology‚Äôs importance extends beyond prompt engineering. Companies with strong evaluation suites can adopt new models faster and build more reliable features than competitors. As Vercel‚Äôs experience demonstrates, moving from complex prompt protection to robust testing enables rapid iteration and development.\n\nConclusion\n\nWhile everyone acknowledges evals‚Äô importance, implementing them effectively remains challenging. The key insight is clear: robust automated evaluation isn‚Äôt just a quality check, it‚Äôs the foundation for building reliable LLM-powered systems.\n",
        "url": "/TIL-tdd-good-system-prompts/",
        "date": "January 2, 2025"
      },
    
      {
        "title": "üìù From Vim to VSCode to Neovim",
        "content": "\n\nIntroduction\nVim‚Äôs portable .vimrc embodies software minimalism at its best. One file, one minute to setup, resulting in a complete development environment. This simplicity served me well until Azure development motivated the use of VSCode.\nWhile VSCode worked reasonably well on macOS, Fedora revealed its constraints: keyboard input failures, heavy resource usage, and complex environment portability compared to Vim‚Äôs vim +PlugInstall. These limitations drove my search for tools that could maintain simplicity while meeting my development requirements with simplicity and portability in mind.\n\nVim -&gt; VSCode -&gt; Neovim\nAzure development initially pulled me into VSCode‚Äôs ecosystem. While stable on macOS, Fedora revealed deal-breakers: random keyboard input failures that only responded to command palette (Ctrl+Shift+P). No amount of configuration resets or reinstalls resolved these issues.\n\nThis instability, coupled with VSCode‚Äôs resource footprint, led me to Neovim. The timing aligned with my exploration of Clojure, where Neovim‚Äôs Conjure plugin offered a compelling Lisp development experience that rivaled Emacs.\n\nMy requirements were specific:\n\n  A lightweight Python IDE\n  A lightweight Deno IDE\n  A lightweight Clojure IDE\n\n\nThrough Dialogue Engineering, I crafted a complete IDE using a single configuration file. Neovim‚Äôs mixed ecosystem of package managers and dual Vimscript/Lua support presents a learning curve, but the resulting environment is fast, stable, and precisely tailored to my needs. One minor drawback is the complexity of adding colour to Conjure‚Äôs output, especially when compared to the rich REPL experiences offered by IPython, Deno, and Clojure with rebel-readline.\n\nConclusions\nThe journey from Vim to VSCode and finally to Neovim reflects a common pattern in software development: sometimes we need to step backward to move forward. While VSCode offered modern IDE features, its stability and resource issues on Linux highlighted the enduring value of minimal, portable tools.\nNeovim strikes an elegant balance: it preserves Vim‚Äôs philosophy of simplicity and portability while providing modern IDE capabilities. Despite minor challenges with REPL colourisation, its single configuration file approach and robust plugin ecosystem make it a powerful choice for polyglot development. For developers who value both minimal tooling and modern features, Neovim proves that we don‚Äôt always have to choose between the two.\n",
        "url": "/vscode-to-neovim/",
        "date": "December 24, 2024"
      },
    
      {
        "title": "üí° TIL: Exploring OpenAI's API with Swagger",
        "content": "\n\nIntroduction\nOpenAI maintains a comprehensive OpenAPI specification that documents their entire API surface. While browsing through their GitHub repository, Simon Willison1 discovered you can easily explore this spec using Swagger‚Äôs web interface.\n\nThe Discovery\nWillison recently highlighted a neat trick: you can browse OpenAI‚Äôs full API documentation by loading their OpenAPI YAML file directly into Swagger‚Äôs web UI.\n\nWhy This Matters\nThis approach offers several advantages:\n\n  Interactive exploration of all API endpoints\n  Complete request/response schemas\n  Built-in testing capability\n  Detailed parameter documentation\n\n\nFor developers working with AI APIs, this provides a valuable reference point - especially when building services that need to maintain compatibility with OpenAI‚Äôs API structure.\n\nTry It Yourself\nVisit the Swagger UI and paste this URL: \nhttps://raw.githubusercontent.com/openai/openai-openapi/refs/heads/master/openapi.yaml\n\n\n\n  \n    \n      Co-founder of Lanyrd, co-creator of Django and Datasette and a prolific independent AI researcher¬†&#8617;\n    \n  \n\n",
        "url": "/TIL-openai-openapi/",
        "date": "December 23, 2024"
      },
    
      {
        "title": "üéõÔ∏è A Practical Guide to Fine-tuning LLMs with InstructLab",
        "content": "\n\nIntroduction\n\nThe explosion of Large Language Models (LLMs) has created a pressing need for domain-specific adaptations. While base models like GPT-4, Claude, and Llama demonstrate impressive general capabilities, organisations often need models that excel in specific domains or exhibit particular behavioural traits. This customisation typically requires fine-tuning, a process that has historically demanded significant expertise, computational resources, and sophisticated infrastructure.\n\nThe Fine-tuning Challenge\n\nTraditional LLM fine-tuning presents a complex web of interconnected challenges that organisations must navigate. At its core lies the need for sophisticated infrastructure, often requiring specialised hardware and carefully orchestrated software stacks. This infrastructure challenge is compounded by substantial computational costs, making experimentation and iteration expensive.\nThe data challenge is equally significant. Fine-tuning demands large, high-quality datasets that are both rare and expensive to create. Even when such datasets exist, organisations face the risk of catastrophic forgetting, where models lose their general capabilities while acquiring new ones. Moreover, validating improvements remains a complex task, requiring careful benchmarking and evaluation frameworks.\nThese challenges have historically restricted fine-tuning to well-resourced organisations, creating a significant barrier to entry for smaller teams and organisations seeking to adapt LLMs to their specific needs.\n\nReal-world Challenges\n\nThe adaptation of LLMs to specific domains presents organisations with a multifaceted set of practical challenges. In healthcare, medical institutions grapple with the need for models that can accurately process and generate content using complex medical terminology while maintaining strict clinical protocols. This domain expertise challenge extends beyond mere vocabulary; it encompasses understanding of medical procedures, drug interactions, and diagnostic reasoning.\nThe financial sector faces equally demanding requirements, particularly around compliance and regulation. Banks and financial institutions must ensure their models operate within specific regulatory frameworks, making decisions that are not only accurate but also auditable and explainable to regulatory bodies.\nData quality emerges as a persistent challenge across sectors. Organisations typically struggle with historical datasets that exhibit inconsistent formatting, missing values, and inherent biases. The challenge extends to maintaining proper version control and data lineage tracking, crucial for both compliance and model improvement cycles.\nRegulatory constraints add another layer of complexity. Healthcare organisations must ensure strict HIPAA compliance in their model development and deployment processes. Similarly, any organisation handling European data must adhere to GDPR requirements, while specific industries often face additional certification needs. These regulatory requirements must be considered not just in the final deployment but throughout the entire fine-tuning process.\n\nThe Role of InstructLab\n\nInstructLab emerges as a systematic solution to these challenges, offering a novel approach to LLM fine-tuning that combines:\n\n  Synthetic data generation for high-quality training examples\n  Efficient QLoRA-based training pipelines\n  Comprehensive evaluation frameworks\n  Hardware-adaptive processing\n\n\nThe rest of this article will elaborate on InstructLab‚Äôs architecture, workflow, and practical considerations, demonstrating how it makes LLM fine-tuning accessible while maintaining rigorous quality standards. It will explore how organisations can leverage this tool to enhance their AI capabilities efficiently and systematically.\n\nFrom Principles to Practice\n\nInstructLab is built around the LAB (Large-Scale Alignment for ChatBots) methodology, leveraging [QLoRA(https://arxiv.org/abs/2305.14314) (Quantized Low-Rank Adaptation) for efficient fine-tuning. The system requires Python 3.10/3.11 and approximately 500GB of disc space for full operation.\n\nArchitectural Components\n\nThe system operates through three primary components:\n\n  Taxonomy Repository: A structured collection of knowledge and skills, organised in YAML files (max 2300 words per Q&amp;A pair)\n  Synthetic Data Generator: Uses a teacher model (default: Mixtral/Mistral instruct for full pipeline, Merlinite 7b for simple) to transform taxonomy entries into diverse training examples\n  Training Pipeline System: QLoRA-based training options optimised for different hardware configurations\n\n\nTraining Pipelines\n\nInstructLab offers three specialised training pipelines:\n\n\n  Simple Pipeline\n    \n      Fast training (~1 hour)\n      Uses SFT Trainer (Linux) or MLX (MacOS)\n      Great for initial experiments and validation\n    \n  \n  Full Pipeline\n    \n      Custom QLoRA training loop optimised for CPU/MPS\n      Enhanced data processing functions\n      Memory requirement: 32GB RAM\n      Balanced performance and accessibility\n    \n  \n  Accelerated Pipeline\n    \n      GPU-accelerated distributed QLoRA training\n      Supports NVIDIA CUDA and AMD ROCm\n      Requires 18GB+ GPU memory\n      Ideal for production-grade fine-tuning\n    \n  \n\n\nHardware Support and Quantisation\n\nInstructLab supports various hardware configurations with automatic quantisation:\n\n  Apple M-series chips: MLX optimisation, MPS acceleration\n  NVIDIA GPUs: CUDA support, 4-bit quantisation available\n  AMD GPUs: ROCm support, similar quantisation options\n  Standard CPUs: Optimised quantisation for memory efficiency\n\n\nPractical Workflow\n\nWith the architectural foundation established, InstructLab provides a systematic approach to implementing these components through a straightforward command-line interface. The following sections detail the practical steps to leverage this architecture effectively.\n\nSetup and Installation\n\npip install instructlab\nilab config init\n\n\nKey requirements:\n\n  Python 3.10 or 3.11 (&gt;=3.12 not supported1)\n  500GB recommended disc space\n  16GB RAM minimum, 32GB recommended\n\n\nCore Workflow Steps\n\n\n  Model Acquisition\n    ilab model download\n    \n    \n      Downloads pre-trained base models\n      Supports GGUF (4-bit to 16-bit) and Safetensors formats\n      Automatic quantisation with configurable parameters\n    \n  \n  Synthetic Data Generation\n    ilab model serve\nilab data generate --pipeline [simple|full]\n    \n    Common issues and solutions:\n    \n      Server conflicts: Use different ports with --port\n      Memory errors: Reduce batch size or use --pipeline simple\n      Teacher model issues: Verify model checksum and try re-downloading\n    \n  \n  Training\n    ilab model train\n    \n    Hyperparameters (configurable in config.yaml):\n    \n      Max epochs: 10\n    \n  \n  Evaluation\n    ilab model evaluate\n    \n    Benchmarks and typical scores:\n    \n      MMLU: Knowledge (0.0-1.0 scale)\n      MMLUBranch: Delta improvements\n      MTBench: Skills (0.0-10.0 scale)\n      MTBenchBranch: Skill improvements\n    \n  \n\n\nModel Deployment\n\nilab model serve --model-path &lt;new-model-path&gt;\nilab model chat -m &lt;new-model-path&gt; # Optionally, chat with the model\n\nDeployment considerations:\n\n  Verify quantisation level matches hardware capabilities\n  Monitor memory usage during serving\n  Consider temperature settings for inference (default: 1.0)\n\n\nConclusion\n\nInstructLab represents a significant advancement in democratising LLM fine-tuning, bridging the gap between research capabilities and practical deployment. Through its innovative LAB methodology and QLoRA-based implementation, it makes sophisticated model adaptation accessible to practitioners across different hardware configurations.\n\nKey Advantages\n\n\n  Accessibility: From laptops to data centres, InstructLab scales with available resources\n  Flexibility: Multiple training pipelines accommodate different needs and constraints\n  Systematic: Structured approach to knowledge and skill injection through taxonomy\n  Verifiable: Comprehensive evaluation suite ensures quality of fine-tuned models\n\n\nPractical Impact\n\nInstructLab enables organisations to:\n\n  Create domain-specialised models without massive compute resources\n  Systematically inject new capabilities through structured knowledge representation\n  Validate improvements through quantitative benchmarks\n  Deploy fine-tuned models with minimal operational overhead\n\n\nLimitations and Considerations\n\n\n  Model Constraints: Currently supports models up to 7B parameters effectively\n  Resource Timeline: Typical deployment cycle from setup to production:\n    \n      Initial setup: a few hours\n      Synthetic Data generation: 15 minutes to 1+ hours depending on computing resources\n      Training: several hours on consumer hardware\n      Evaluation and deployment: a few hours\n    \n  \n  Maintenance Requirements:\n    \n      Regular model evaluations against new benchmarks\n      Periodic retraining with updated taxonomy\n      System updates and dependency management\n      Storage management for checkpoints and datasets\n    \n  \n\n\nRAG vs Fine-tuning\n\nIt‚Äôs important to recognise that fine-tuning isn‚Äôt always the optimal solution. For dynamic, frequently changing knowledge bases, Retrieval-Augmented Generation (RAG) often provides a more practical and maintainable solution. Fine-tuning through InstructLab is most valuable for:\n\n  Stable knowledge domains (e.g., natural sciences, engineering)\n  Consistent skill enhancement needs\n  Cases where inference latency is critical\n\n\nThe system‚Äôs architecture strikes a careful balance between computational efficiency and training effectiveness, making it a practical tool for both experimentation and production use. While not eliminating the complexity of LLM fine-tuning entirely, InstructLab significantly reduces the technical barriers to entry in this crucial domain.\n\n\n\n  \n    \n      Python version compatibility remains a significant consideration in the ML ecosystem. While newer versions (‚â•3.12) offer improved performance, they often lack compatibility with essential ML frameworks. This constraint informs InstructLab‚Äôs current version requirements.¬†&#8617;\n    \n  \n\n",
        "url": "/instructlab-and-rag/",
        "date": "December 19, 2024"
      },
    
      {
        "title": "üí° TIL: Understanding GGUF Model Quantisation",
        "content": "\n\nIntroduction\nWhen experimenting with larger language models (12B, 30B, 70B etc.), choosing the right quantisation format becomes crucial for striking a good balance i.e. running them on consumer hardware while maintaining reasonably good performance. I wrote this guide after spending time looking up different GGUF quantisation types to optimise model selection for my machine‚Äôs constraints. This guide explains quantisation methods and their practical tradeoffs to help the reader select the optimal format for their setup.\nThe quantisation formats discussed here are implemented in popular frameworks like llama.cpp. Q4_K_S is typically the default format due to its good balance of size, speed, and quality, while Q2_K and Q3_K variants are offered for more constrained systems.\n\nWhat is Quantisation?\nQuantisation converts model weights from 16-bit floating point (F16) to lower precision formats using fixed-size blocks. Each block contains multiple weights that share scaling parameters. \nPerplexity is the key metric used to measure model quality after quantisation. It indicates how well the model predicts text, the lower the perplexity the better the predictions. For example, a change from 5.91 to 6.78 perplexity represents a noticeable but often acceptable drop in prediction quality. A model with perplexity 6.78 is slightly less certain about its predictions than one with perplexity 5.91.\n\nBasic Quantisation Types and K-Quantisation\nK-quantisation is a way to make AI models smaller using two methods to store weights (the model‚Äôs numbers):\n\n\n  Type-0 (simpler): reconstructs weight as weight = scale √ó quant\n  Type-1 (more precise): reconstructs weight as weight = scale √ó quant + minimum\n\n\nThe ‚Äúblock minimum‚Äù minimum is the smallest value found in a group of weights. By tracking this minimum, we can represent the other values more precisely relative to it, rather than having to represent their full absolute values.\n\nEach format groups weights into ‚Äúsuper-blocks‚Äù to save space. Specifically:\n\nQ2_K (2-bit):\n\n  Uses Type-1 formula\n  Organises weights in groups of 256 (16 blocks √ó 16 weights)\n  Uses 4 bits to store both scales and minimums\n  Takes exactly 2.5625 bits per weight\n  Result: Shrinks a 13GB model to 2.67GB, but quality drops (perplexity increases from 5.91 to 6.78)\n\n\nQ3_K (3-bit):\n\n  Uses Type-0 formula (simpler one)\n  Same organization: 16 blocks √ó 16 weights\n  Uses 6 bits to store scales\n  Takes exactly 3.4375 bits per weight\n  Better quality than Q2_K but bigger file size\n\n\nQ4_K (4-bit):\n\n  Uses Type-1 formula\n  Different organisation: 8 blocks √ó 32 weights = 256 total\n  Uses 6 bits for both scales and minimums\n  Takes exactly 4.5 bits per weight\n  Much better quality, file size around 3.56GB\n\n\nQ5_K (5-bit):\n\n  Uses Type-1 formula\n  Same organisation as Q4_K\n  Also uses 6 bits for scales and minimums\n  Takes exactly 5.5 bits per weight\n  Quality getting very close to original\n\n\nQ6_K (6-bit):\n\n  Uses Type-0 formula\n  Back to 16 blocks √ó 16 weights\n  Uses 8 bits for scales\n  Takes exactly 6.5625 bits per weight\n  Almost perfect quality, file size 5.15GB\n\n\nThe main tradeoff: Fewer bits means smaller files but lower quality. More bits means better quality but larger files. This lets users choose what works best for their needs.\nWhen compressing numbers in Type-1 quantisation, each block keeps track of its smallest value (the minimum). When reconstructing the weights, this minimum is added back after multiplication. This helps preserve the range of values more accurately than just using scaling alone.\n\nA simple way to think of this concept is:\n\n  Type-0 just stretches/shrinks values using a scale\n  Type-1 first shifts all numbers by subtracting the minimum (making them smaller), then scales them for storage, and when reconstructing adds the minimum back\n\n\nThis is why Type-1 generally gives better quality results but needs more storage space. It has to keep track of both the scale and minimum for each block.\n\nMixed Precision Strategies\nK-quantisations use different precision levels for different model components. From llama.cpp documentation, there are three variants:\n\n\n  S (Small): Uses single quantisation throughout\nExample using Q3_K_S:\n    \n      All model tensors ‚Üí Q3_K (3-bit)\nResult: 2.75GB size, 6.46 perplexity (7B model)\n    \n  \n  M (Medium): Strategic mixed precision\nExample using Q3_K_M:\n    \n      attention.wv1, attention.wo2, feed_forward.w23 ‚Üí Q4_K (4-bit)\nAll other tensors ‚Üí Q3_K (3-bit)\nResult: 3.06GB size, 6.15 perplexity (7B model)\n    \n  \n  L (Large): Higher precision mix\nExample using Q3_K_L:\n    \n      attention.wv1, attention.wo2, feed_forward.w23 ‚Üí Q5_K (5-bit)\nAll other tensors ‚Üí Q3_K (3-bit)\nResult: 3.35GB size, 6.09 perplexity (7B model)\n    \n  \n\n\nThese strategies target attention and feed-forward layers with higher precision because they directly impact text processing quality, as demonstrated by the perplexity improvements in benchmarks: Q3_K_S (6.46) ‚Üí Q3_K_M (6.15) ‚Üí Q3_K_L (6.09).\nThe improvement in perplexity scores demonstrates why mixed precision strategies are effective, though they require more storage space.\n\nPerformance Comparison (7B model)\nFormat | Size(GB) | Reduction | BPW  | Perplexity | RTX4080  | M2Max   \nF16    | 13.0     | 1.0x      | 16.0 | 5.91       | 60.0ms   | 116ms\nQ2_K   | 2.67     | 4.9x      | 2.56 | 6.78       | 15.5ms   | 56ms\nQ3_K_S | 2.75     | 4.7x      | 3.44 | 6.46       | 18.6ms   | 81ms\nQ4_K_S | 3.56     | 3.7x      | 4.50 | 6.02       | 15.5ms   | 50ms\nQ6_K   | 5.15     | 2.5x      | 6.56 | 5.91       | 18.3ms   | 75ms\n\n*BPW = Bits Per Weight, Speed in milliseconds per token\n\nPractical Recommendations:\n\n  Balanced Performance: Q4_K_S\n  Maximum Compression: Q2_K\n  Best Quality: Q6_K (matches F16)\n  Limited RAM: Q2_K or Q3_K\n  GPU Inference: Q4_K (optimal speed/quality)\n\n\nAll data are from recent llama.cpp performance benchmarks and GGML implementation details.\n\nMemory Requirements for Inference\nWhen running quantised models, more RAM is required than the model size alone for inference overhead. Memory requirements depend on several factors:\n\n  Model architecture and size\n  Batch size for inference\n  Number of layers loaded at once\n  Operating system and framework overhead\n\n\nFor 7B models (verified from benchmarks):\nFormat | Model Size | Note\nF16    | 13.0GB    | Base format\nQ4_K_S | 3.56GB    | Common choice\nQ3_K_S | 2.75GB    | Minimum size\nQ6_K   | 5.15GB    | Highest quality\n\n\nFor larger models scale the memory requirements proportionally and ensure additional overhead memory is available for inference. Test with smaller models first to gauge the system‚Äôs capabilities.\nActual RAM/VRAM requirements will be higher than the model size. Consider monitoring memory usage during inference to determine exact requirements for a specific setup.\nHere is an example memory usage scenario for a Q4_K_S 7B model:\n\n  Model size: 3.56GB\n  Inference overhead: ~2GB for standard settings\n  Operating system buffer: ~1GB recommended\n  Total recommended free memory: ~7GB\n\n\nThis explains why a model that‚Äôs ‚Äú3.56GB‚Äù might need 6-7GB of free RAM/VRAM to run smoothly. The exact overhead varies based on your settings and system.\n\nConclusion\nModern quantisation techniques offer multiple ways to run large language models on consumer hardware. Here‚Äôs what we need to remember:\n\n\n  K-quantisation provides the best balance through super-blocks and mixed precision strategies\n  Q4_K_S (4-bit) represents the current sweet spot for most users, offering:\n    \n      3.7x size reduction\n      Good perplexity (6.02)\n      Excellent inference speed on both GPU and CPU\n    \n  \n  For more constrained setups, Q2_K/Q3_K variants can run larger models with acceptable quality loss\n  Higher bits (Q5_K, Q6_K) approach F16 quality but require more memory\n  The _S/_M/_L variants let the user fine-tune the quality-size tradeoff by adjusting precision where it matters most\n\n\nBefore downloading a quantised model, check the system‚Äôs available RAM and choose a format that leaves enough memory for comfortable operation. For most users with modern GPUs, Q4_K variants will provide the best experience.\n\n\n\n  \n    \n      In llama.cpp, attention.wv refers to a tensor that holds the weights for the value vectors in the self-attention mechanism of the model. This tensor is crucial for determining how much focus the model places on different parts of the input when generating responses.¬†&#8617;¬†&#8617;2\n    \n    \n      attention.wo refers to the weight matrix used in the output layer of the attention mechanism within a transformer model. It plays a crucial role in transforming the attention output into the final representation that is used for generating predictions.¬†&#8617;¬†&#8617;2\n    \n    \n      feed_forward.w1 projects input to a higher-dimensional space, enabling the capture of complex features. feed_forward.w2 projects transformed input back to the original dimension with a non-linear activation function, whereas feed_forward.w3 applies an additional transformation to enhance the learning of complex patterns. These matrices collectively enable the feed-forward network to transform and learn from the input effectively, contributing to the overall performance of the transformer model.¬†&#8617;¬†&#8617;2\n    \n  \n\n",
        "url": "/TIL-llm-quantisation/",
        "date": "December 7, 2024"
      },
    
      {
        "title": "üí° TIL: LLM Evaluation using Critique Shadowing",
        "content": "\n\nIntroduction\nAs LLMs increasingly drive critical business decisions, ensuring their reliability becomes paramount. Many teams struggle with complex metrics and scoring systems that lead to confusion rather than clarity. Hamel Husain‚Äôs Critique Shadowing methodology1 offers a systematic path from drowning in metrics to developing reliable evaluation systems.\n\nThe Critique Shadowing Method\nThe key insight behind Critique Shadowing is deceptively simple: start with binary (pass/fail) expert judgements and detailed critiques before building automated evaluation systems. This approach solves two critical challenges: capturing domain expertise and scaling evaluation processes.\n\nThis expert-centric approach echoes knowledge engineering practices from the 1970-80s, when AI researchers first recognized the necessity of systematically capturing domain expertise. Just as MYCIN‚Äôs creators worked closely with medical doctors to encode diagnostic knowledge, Critique Shadowing similarly structures the process of extracting expert judgement for LLM evaluation. While the technology has evolved from rule-based systems to large language models, the fundamental challenge of effectively capturing and operationalising expert knowledge remains central.\n\nImplementation Process\nThe methodology follows a structured, iterative process:\n\n\n    \n\n\n\n  Identify a principal domain expert as the arbiter of quality\n  Create a diverse dataset covering different scenarios and user types\n  Expert conducts binary pass/fail judgements with detailed critiques\n  Address discovered issues and verify fixes\n  Develop LLM-based judges using expert critiques as few-shot examples\n  Analyze error patterns and root causes\n  Create specialized judges for persistent issues\n\n\nThe process is continuous, repeating periodically or when material changes occur. For simpler applications or when manual review is feasible, teams can adapt or streamline these steps while maintaining the core principle of systematic data examination.\n\nBeyond Automation\nHusain‚Äôs most striking observation is that the process of developing evaluation systems often provides more value than the resulting automated judges. The systematic collection of expert feedback reveals product insights, user needs, and failure modes that might otherwise remain hidden. This understanding drives improvements in the core system, not just its evaluation.\n\nConclusion\nThe Critique Shadowing methodology succeeds by prioritizing expert knowledge and systematic data collection over premature automation. For teams building LLM applications, this approach offers a clear path to reliable evaluation systems while simultaneously deepening their understanding of their product and users.\nLLM evaluation is an active area of interest and research both in academia and industry. Here is a short list of resources to look into:\n\n  IBM LLM Evaluation\n  Mistral AI - Evaluation\n  Mistral Evals\n  Anthropic - Using the Evaluation Tool\n  Top 5 Open-Source LLM Evaluation Frameworks in 2024\n\n\n\n\n  \n    \n      Husain, H. (2024). ‚ÄúCreating a LLM-as-a-Judge That Drives Business Results‚Äù https://hamel.dev/blog/posts/llm-judge/¬†&#8617;\n    \n  \n\n",
        "url": "/TIL-llm-eval-critique-shadowing/",
        "date": "December 5, 2024"
      },
    
      {
        "title": "‚úç A Path to Maintainable AI Systems using Norman's Design Principles",
        "content": "\n\nIntroduction\nDon Norman‚Äôs principles of good design, outlined in The Design of Everyday Things, are particularly relevant to Data Science and AI Engineering, where systems often suffer from unnecessary complexity. This article presents a minimalist approach to implementing these principles using a carefully selected set of tools that maximise impact while reducing operational overhead. Norman‚Äôs insights about visibility, feedback, constraints, and mappings translate powerfully to AI system design, where abstract interfaces and complex workflows can easily become overwhelming. Just as Norman observed that poorly designed physical objects lead to user frustration and errors, poorly architected AI systems can result in maintenance nightmares, hidden failure modes, and costly debugging cycles. By applying his principles - making system states visible, providing clear feedback, implementing appropriate constraints, and creating natural mappings between components, we can build AI systems that are not only more intuitive to use but also easier to maintain, debug, and evolve over time.\n\nDesign Principles Implementation\n1. Visibility\nImplement comprehensive system observability using MLflow as your central platform:\n\n\n  Track experiments, parameters, and metrics\n  Version models and artefacts\n  Log production predictions and outcomes\n  Monitor model performance metrics\n\n\nFor system-level metrics, use Prometheus/Grafana to:\n\n  Track resource utilisation (CPU, memory, latency)\n  Monitor prediction throughput\n  Create dashboards for key performance indicators\n\n\nImplement adaptive sampling for high-volume systems:\ndef should_log(request_id, sampling_rate=0.1):\n    return hash(request_id) % 100 &lt; (sampling_rate * 100)\n\n\n2. Feedback\nUse Prometheus/Grafana for real-time monitoring and alerting:\n\n\n  Set up alerts for model performance degradation\n  Monitor data distribution shifts\n  Track system health metrics\n  Configure tiered alerting based on severity\n\n\nExample metric collection:\nfrom prometheus_client import Counter, Histogram\n\nPREDICTIONS = Counter('model_predictions_total', 'Total predictions made')\nLATENCY = Histogram('prediction_latency_seconds', 'Time spent processing prediction')\n\ndef predict(features):\n    with LATENCY.time():\n        prediction = model.predict(features)\n        PREDICTIONS.inc()\n        return prediction\n\n\n3. Constraints\nImplement data and model guardrails using Great Expectations:\n\n\n  Define data quality expectations\n  Set distribution bounds for features\n  Monitor for data drift\n  Generate validation reports\n\n\nExample constraint implementation:\nfrom great_expectations.dataset import Dataset\n\ndef validate_features(df):\n    dataset = Dataset(df)\n    dataset.expect_column_values_to_be_between(\"age\", 0, 120)\n    dataset.expect_column_values_to_not_be_null(\"critical_feature\")\n    validation_result = dataset.validate()\n    return validation_result.success\n\n\n4. Mappings\nUse MLflow to maintain clear relationships between:\n\n\n  Experiments and business objectives\n  Models and their training data\n  Predictions and outcomes\n  Performance metrics and business KPIs\n\n\nExample mapping structure:\nwith mlflow.start_run(run_name=\"production_model_v1\"):\n    mlflow.log_param(\"business_objective\", \"customer_churn\")\n    mlflow.log_param(\"data_version\", data_hash)\n    mlflow.log_metric(\"business_impact\", revenue_improvement)\n    mlflow.log_artifact(\"feature_importance.json\")\n\n\n5. Error Prevention and Recovery\nIntegrate safeguards using your core toolset:\n\nMLflow:\n\n  Version control for models and artefacts\n  Rollback capabilities\n  Experiment tracking for reproducibility\n\n\nPrometheus/Grafana:\n\n  Early warning system for issues\n  Performance degradation detection\n  Resource exhaustion prevention\n\n\nGreat Expectations:\n\n  Data quality validation\n  Schema enforcement\n  Distribution monitoring\n\n\nExample error prevention:\ndef safe_predict(features):\n    if not validate_features(features):\n        return fallback_prediction()\n    \n    try:\n        with LATENCY.time():\n            prediction = model.predict(features)\n            PREDICTIONS.inc()\n            return prediction\n    except Exception as e:\n        ERROR_COUNTER.inc()\n        return fallback_prediction()\n\n\nImplementation Strategy\n\n  Start with MLflow\n    \n      Set up experiment tracking\n      Implement model versioning\n      Configure basic logging\n    \n  \n  Add Prometheus/Grafana\n    \n      Deploy basic monitoring\n      Set up key alerts\n      Create essential dashboards\n    \n  \n  Integrate Great Expectations\n    \n      Define core data quality rules\n      Implement validation pipelines\n      Monitor data distributions\n    \n  \n\n\nConclusions\nBy focusing on a minimal set of powerful tools (MLflow, Prometheus/Grafana, and Great Expectations), you can implement Norman‚Äôs design principles effectively while maintaining system simplicity. This approach provides:\n\n\n  Comprehensive visibility through unified logging and monitoring\n  Immediate feedback via real-time alerts\n  Strong constraints through data validation\n  Clear mappings between components\n  Robust error prevention and recovery\n\n\nThe key is to fully utilise these core tools rather than adding complexity with additional solutions. This creates maintainable, observable, and reliable AI systems that can scale with your needs.\n",
        "url": "/design-principles-ds-ai/",
        "date": "December 3, 2024"
      },
    
      {
        "title": "üêº Pandas or üêª‚Äç‚ùÑÔ∏è Polars?",
        "content": "\n\nIntroduction\nThe world of Python data processing has long revolved around the well established Pandas library, but in recent years, a new contender has emerged in the form of Polars. This post aims to provide a comparison of these two powerful data processing tools, that empowers the reader to make an informed choice on a case-by-case basis.\n\nArchitecture and Design Comparison\nAt the core, Pandas and Polars differ in their underlying implementation and design philosophies.\n\nImplementation and Performance\nThe Pandas library is written in Python/Cython, with a focus on single-threaded operations. In contrast, Polars is built upon the Rust programming language, leveraging its performance and concurrency capabilities to enable parallel processing by default.\nThis distinction in implementation has significant implications for memory management and query optimization. Pandas typically works with multiple copies of data, while Polars utilizes the Arrow data format, which allows for more efficient memory usage. Additionally, Polars offers automatic query optimization, whereas Pandas users must rely on a more sequential, manual approach to optimizing their data processing pipelines.\n\n\n  \n    \n      Feature\n      Pandas\n      Polars\n    \n  \n  \n    \n      Implementation\n      Python/Cython\n      Rust\n    \n    \n      Processing\n      Single-threaded\n      Parallel by default\n    \n    \n      Memory Management\n      Multiple copies\n      Arrow format\n    \n    \n      Query Optimization\n      Sequential\n      Automatic\n    \n  \n\n\nAPI and Language Support\nThe API and language support differences between Pandas and Polars are quite notable. Pandas -being a Python-only library- offers a mix of method chaining and attribute access approaches. In contrast, Polars takes a more expansive approach, providing implementations in Python, Node.js, and the Rust programming language itself.\nThis language versatility of Polars enables seamless JavaScript and TypeScript integration, allowing data scientists and developers to leverage the same performance benefits regardless of their preferred language. Additionally, Polars maintains a consistent method chaining syntax across these different language environments, simplifying the learning curve for users who may work with the library in multiple contexts.\n\n\n  \n    \n      Feature\n      Pandas\n      Polars\n    \n  \n  \n    \n      Language Support\n      Python-only\n      Python, Node.js, Rust\n    \n    \n      API Style\n      Mixed method chaining and attribute access\n      Consistent method chaining\n    \n    \n      Language Integration\n      N/A\n      JavaScript/TypeScript\n    \n  \n\n\nUse Cases and Trade-offs\nWhile both Pandas and Polars excel in the realm of data processing, each library has distinct strengths and weaknesses that make them better suited for different use cases and scenarios.\n\nWhen to Choose Pandas\nPandas shines when it comes to interactive data exploration and working with smaller datasets, typically under 1GB in size. The library‚Äôs deep integration with the broader scientific computing ecosystem in Python, along with its intuitive syntax and extensive documentation, make it an excellent choice for rapid prototyping, educational contexts, and projects that require seamless compatibility with the Python-centric data science toolchain.\n\nWhen to Choose Polars\nOn the other hand, Polars emerges as the preferred choice for large-scale data processing, particularly for datasets exceeding 1GB. The library‚Äôs Rust-based implementation and parallel processing capabilities make it a more suitable option for production environments with demanding performance requirements. Polars also excels in memory-constrained systems, thanks to its efficient use of the Arrow data format, and it is an attractive choice for cross-language development teams due to its implementations in Python, Node.js, and Rust.\nFurthermore, Polars demonstrates strengths in handling complex data transformations and time series processing at scale, areas where its optimized query engine and parallel processing features can truly shine.\n\nTo summarize the key differences:\n\n\n  \n    \n      Consideration\n      Pandas\n      Polars\n    \n  \n  \n    \n      Dataset Size\n      Small to medium (&lt;1GB)\n      Scales to larger datasets\n    \n    \n      Performance\n      Suitable for interactive exploration\n      Excels at large-scale processing\n    \n    \n      Memory Efficiency\n      Works with multiple data copies\n      Utilizes Arrow format for efficiency\n    \n    \n      Query Optimization\n      Sequential, manual approach\n      Automatic optimization\n    \n    \n      Language Support\n      Python-only\n      Python, Node.js, Rust\n    \n    \n      Ecosystem Integration\n      Strong in Python scientific computing\n      Limited cross-language integration\n    \n    \n      Learning Resources\n      Extensive documentation and community support\n      Younger ecosystem, less comprehensive resources\n    \n  \n\n\nUltimately, the choice between Pandas and Polars should be guided by the specific requirements of your project, such as data volume, performance needs, language preferences, and ecosystem integration requirements. Both libraries offer powerful data processing capabilities, and selecting the right one can significantly impact the success and efficiency of your data-driven initiatives.\n\nConclusions\nAfter carefully evaluating the key differences between Pandas and Polars, the choice between the two data processing libraries ultimately comes down to the specific requirements of your project and use case.\nFor projects focused on interactive data exploration and working with smaller datasets (under 1GB), Pandas remains the go-to choice. Its deep integration with the broader Python scientific computing ecosystem, extensive documentation, and large community make it a reliable and familiar option for many data scientists and developers.\nHowever, for large-scale data processing, production environments, and cross-language teams, Polars presents a compelling alternative. Its performance advantages, memory efficiency, and multi-language support (Python, Node.js, Rust) make it an increasingly attractive choice for modern data-intensive applications.\nWhen deciding between Pandas and Polars, consider factors such as dataset size, performance requirements, memory constraints, language preferences, and the level of ecosystem integration needed. Pandas may be the better fit for projects focused on rapid prototyping and educational use, while Polars can shine in mission-critical, large-scale data processing tasks.\nUltimately, both Pandas and Polars are powerful data processing tools, and the choice between them should be guided by the specific needs and constraints of your project. As the data processing landscape continues to evolve, it‚Äôs valuable to stay informed about the trade-offs and emerging alternatives to ensure you make the most informed decision for your team and organization.\n",
        "url": "/pandas-polars/",
        "date": "December 2, 2024"
      },
    
      {
        "title": "üìä Ten Ways to Model Data",
        "content": "\n\nIntroduction\nAs a practitioner looking to work effectively with real-world data and generate meaningful insights, I face a crucial decision: which modelling approaches should I invest my time and energy in learning? After discovering Christoph Molnar‚Äôs Modeling Mindsets, I realised this isn‚Äôt about picking the ‚Äúbest‚Äù approach. It‚Äôs about becoming what he calls a ‚ÄúT-shaped modeller‚Äù.\nThe concept is elegantly simple: rather than trying to master every possible approach (impossible) or limiting myself to just one (ineffective), I should aim to develop:\n\n  Deep expertise in one or two mindsets that align with my goals and problems\n  Working knowledge of other approaches to recognise when my primary tools aren‚Äôt optimal\n\n\nThis systematic exploration serves two purposes:\n\n  To understand the landscape: What are the main modelling mindsets available today? What are their core premises, strengths, and limitations?\n  To make an informed choice: Which mindset(s) should I focus on mastering, given my goals and constraints?\n\n\nEach mindset represents a different way of approaching problems through data. From the probability-focused world of statistical modelling to the interactive realm of reinforcement learning, from the causality-oriented approach to the pattern-finding nature of unsupervised learning, each offers unique tools and perspectives.\nBy examining these mindsets systematically, I aim to make an informed decision about where to focus my learning efforts while maintaining enough breadth to recognize when I should switch approaches. This isn‚Äôt just about theoretical understanding, it‚Äôs about practical effectiveness in solving real-world problems.\n\nLet‚Äôs explore each mindset in turn, focusing on their fundamental premises, key strengths, and limitations to guide this decision.\n\nStatistical Modelling: The Foundation of Data-Driven Inference\nThis mindset sees the world through probability distributions. At its core, it‚Äôs about modelling how data is generated and making inferences under uncertainty.\n\nKey Aspects:\n\n  Everything has a distribution, from dice rolls to customer behaviours\n  Models encode assumptions about how data is generated\n  Models are evaluated by both checking if their assumptions make sense and measuring how well they match the data\n  Uses same data for fitting and evaluation, unlike machine learning approaches\n\n\nPrimary Strengths:\n\n  Provides rigourous mathematical framework for handling uncertainty\n  Strong theoretical foundation spanning decades of research\n  Forces explicit consideration of data-generating processes\n  Versatile for decisions, predictions, and understanding\n\n\nNotable Limitations:\n\n  Manual and often tedious modelling process\n  Struggles with complex data types like images and text\n  Good model fit doesn‚Äôt guarantee good predictions\n  Less automatable than modern machine learning approaches\n\n\nThis mindset serves as the foundation for three important sub-approaches: Frequentism, Bayesianism, and Likelihoodism, each with its own interpretation of probability and evidence. For someone starting in data science, understanding statistical modelling provides crucial groundwork for understanding both traditional statistics and modern machine learning approaches.\n\nFrequentism: Making Decisions Through Repeated Experiments\nFrequentism views probability as long-run frequency and assumes that parameters in the world are fixed but unknown. It‚Äôs the dominant approach in many scientific fields, particularly in medicine and psychology.\n\nKey Aspects:\n\n  Interprets probability as frequency in infinite repetitions\n  Makes decisions through hypothesis tests and confidence intervals\n  Relies on ‚Äúimagined experiments‚Äù to draw conclusions\n  Focuses on estimating fixed, true parameters\n\n\nPrimary Strengths:\n\n  Enables clear, binary decisions\n  Computationally fast compared to other approaches\n  No need for prior information\n  Widely accepted in scientific research\n\n\nNotable Limitations:\n\n  Often oversimplifies complex questions into yes/no decisions\n  Vulnerable to p-hacking (searching for significant results)\n  Interpretation can be counterintuitive, especially for confidence intervals\n  Results depend on the experimental design, not just the data\n\n\nFor practitioners, Frequentism offers a well-established framework with clear decision rules and strong scientific acceptance. However, its limitations in handling uncertainty and tendency toward oversimplification have led to growing interest in alternative approaches like Bayesian inference.\n\nBayesianism: Learning Through Updated Beliefs\nBayesianism stands out by treating parameters themselves as random variables with distributions, fundamentally different from Frequentism‚Äôs fixed-parameter view. It focuses on updating beliefs about parameters as new data arrives.\n\nKey Aspects:\n\n  Requires prior distributions before seeing data\n  Updates beliefs through Bayes‚Äô theorem\n  Produces complete posterior distributions, not just point estimates\n  Naturally propagates uncertainty through all calculations1\n\n\nPrimary Strengths:\n\n  Can incorporate prior knowledge and expert opinions\n  Provides complete probability distributions for parameters\n  More intuitive interpretation of uncertainty\n  Cleanly separates inference (getting posteriors) from decisions (using them)\n\n\nNotable Limitations:\n\n  Choosing priors can be difficult and controversial\n  Computationally intensive, especially for complex models\n  Mathematically more demanding than frequentist approaches\n  Can seem like overkill for simple decisions\n\n\nBayesianism offers a more complete and intuitive framework for handling uncertainty, but requires more computational resources and mathematical sophistication. It‚Äôs particularly valuable when prior knowledge is important or when understanding full uncertainty is crucial.\n\nLikelihoodism: Pure Evidence Through Likelihood\nLikelihoodism attempts to reform statistical inference by focusing solely on likelihood as evidence, avoiding both Frequentism‚Äôs imagined experiments and Bayesianism‚Äôs subjective priors.\n\nKey Aspects:\n\n  Uses likelihood ratios to compare hypotheses\n  Adheres strictly to the likelihood principle\n  Rejects both prior probabilities and sampling distributions\n  Compares models based on their relative evidence\n\n\nPrimary Strengths:\n\n  More coherent than Frequentism‚Äôs mixed toolkit\n  Avoids subjective elements of Bayesianism\n  Ideas work well within other statistical mindsets\n  Adheres to likelihood principle (evidence depends only on observed data)\n\n\nNotable Limitations:\n\n  Cannot make absolute statements, only relative comparisons\n  No clear mechanism for making final decisions\n  Lacks tools for expressing beliefs or uncertainty\n  Less practical than other statistical approaches\n\n\nLikelihoodism offers interesting theoretical insights but may be less immediately useful than Frequentist or Bayesian approaches. It‚Äôs more valuable for understanding the foundations of statistical inference than for day-to-day data analysis.\n\nCausal Inference: From Association to Causation\nCausal inference moves beyond correlation to understand what actually causes observed effects, providing a framework for analysing interventions and their impacts.\n\nKey Aspects:\n\n  Uses Directed Acyclic Graphs (DAGs) to visualize relationships\n  Distinguishes between association and causation\n  Requires explicit encoding of causal assumptions\n  Can work with both statistical models and machine learning\n\n\nPrimary Strengths:\n\n  Addresses fundamental questions about cause and effect\n  Makes assumptions explicit through DAGs\n  Models tend to be more robust than pure association-based approaches\n  Provides clear framework for analysing interventions\n\n\nNotable Limitations:\n\n  Requires identifying all relevant confounders\n  Cannot verify all causal assumptions from data alone\n  Multiple competing frameworks can confuse newcomers\n  May sacrifice predictive performance for causal understanding\n\n\nFor practitioners, causal inference is essential when decisions about interventions are needed, though it requires careful consideration of assumptions and domain knowledge. It‚Äôs particularly valuable in fields like medicine, policy-making, and business strategy where understanding cause-effect relationships is crucial.\n\nMachine Learning: Algorithms Learning from Data\nMachine learning approaches problems by having computers learn algorithms from data, focusing on task performance rather than theoretical underpinning.\n\nKey Aspects:\n\n  Computer-first approach to learning from data\n  External evaluation based on task performance\n  Less constrained by statistical assumptions\n  Includes supervised, unsupervised, reinforcement, and deep learning\n\n\nPrimary Strengths:\n\n  Task-oriented and pragmatic approach\n  Highly automatable\n  Well-suited for building digital products\n  Strong industry adoption and career opportunities\n\n\nNotable Limitations:\n\n  Less principled than statistical approaches\n  Many competing approaches can be overwhelming\n  Models often prioritize performance over interpretability\n  Usually requires substantial data and computation\n\n\nFor practitioners, machine learning offers powerful tools for automation and prediction, particularly valuable in industry settings. It‚Äôs especially useful when theoretical understanding is less important than practical performance.\n\nSupervised Learning: The Art of Prediction\nSupervised learning frames everything as a prediction problem, using labelled data to learn mappings from inputs to outputs.\n\nKey Aspects:\n\n  Learning is optimization and search in hypothesis space\n  Models evaluated on unseen data, not training data\n  Focuses on generalizing to new cases\n  Highly automatable and competition-friendly\n\n\nPrimary Strengths:\n\n  Clear evaluation metrics\n  Highly automatable\n  Strong performance on prediction tasks\n  Well-defined optimization objectives\n\n\nNotable Limitations:\n\n  Requires labelled data\n  Models often black-box (uninterpretable)\n  Not hypothesis-driven\n  May miss causal relationships\n  Can fail in unexpected ways when patterns change\n\n\nFor practitioners, supervised learning excels in prediction tasks where good labelled data exists and interpretability isn‚Äôt crucial. It‚Äôs particularly valuable in industry settings for automation and decision support.\n\nUnsupervised Learning: Discovering Hidden Patterns\nThis mindset focuses on finding inherent structures in data without labelled outcomes, making it ideal for exploratory analysis and pattern discovery.\n\nKey Aspects:\n\n  Discovers patterns in data distributions\n  Includes clustering, dimensionality reduction, anomaly detection\n  No ground truth for validation\n  More open-ended than supervised learning\n\n\nPrimary Strengths:\n\n  Finds patterns other approaches might miss\n  Excellent for initial data exploration\n  Flexible for undefined problems\n  Can reveal natural groupings in data\n\n\nNotable Limitations:\n\n  Hard to validate results objectively\n  Feature weighting is often arbitrary\n  Suffers from curse of dimensionality2\n  No guarantee of finding meaningful patterns\n\n\nFor practitioners, unsupervised learning is valuable for initial data exploration and when labelled data isn‚Äôt available. It‚Äôs particularly useful in customer segmentation, anomaly detection, and dimension reduction.\n\nReinforcement Learning: Learning Through Interaction\nThis mindset models an agent interacting with an environment, making decisions and learning from rewards.\n\nKey Aspects:\n\n  Agent learns by taking actions and receiving rewards\n  Handles delayed and sparse rewards\n  Balances exploration and exploitation\n  Creates its own training data through interaction\n\n\nPrimary Strengths:\n\n  Models dynamic real-world interactions\n  Excellent for sequential decision-making\n  Can discover novel strategies\n  Learns through direct experience\n  Combines well with deep learning\n\n\nNotable Limitations:\n\n  Not all problems fit agent-environment framework\n  Often unstable or difficult to train\n  May perform poorly in real-world conditions\n  Requires careful reward design\n  Complex implementation choices\n\n\nFor practitioners, reinforcement learning is valuable for problems involving sequential decisions or control, particularly in robotics, game playing, and resource management.\n\nDeep Learning: End-to-End Neural Networks\nThis mindset approaches problems through deep neural networks, letting the model learn both features and relationships.\n\nKey Aspects:\n\n  Models tasks end-to-end through neural networks\n  Learns hierarchical representations automatically\n  Highly modular architecture design\n  Benefits from transfer learning and pre-trained models\n\n\nPrimary Strengths:\n\n  Excels at complex data (images, text, speech)\n  Learns useful feature representations\n  Highly modular and customizable\n  Strong tooling and community support\n  Can handle multiple inputs/outputs seamlessly\n\n\nNotable Limitations:\n\n  Underperforms on tabular data versus tree methods\n  Requires large amounts of data\n  Computationally intensive\n  Hard to train and tune effectively\n  Results can be difficult to interpret\n\n\nFor practitioners, deep learning is essential for complex data types but may be overkill for simpler problems. Most valuable in computer vision, natural language processing, and other complex pattern recognition tasks.\n\nConclusions\naka Choosing Your Modelling Path\n\nFor developing T-shaped expertise in modelling, the practitioner‚Äôs choice should align with their primary domain while maintaining broader awareness. Here‚Äôs how to approach this decision:\n\n\n  \n    Scientific Research demands Statistical Modelling for its rigorous uncertainty quantification and established peer review frameworks.\n  \n  \n    Business Predictions benefit most from Supervised Learning, optimising prediction accuracy while enabling automation and scalability.\n  \n  \n    Complex Data (images/text) requires Deep Learning to handle unstructured data and learn hierarchical features effectively.\n  \n  \n    Interventions/Policies need Causal Inference to distinguish correlation from causation and understand intervention effects.\n  \n  \n    Control Systems thrive with Reinforcement Learning for sequential decisions and environment interaction.\n  \n\n\nFor practical applications, certain combinations prove particularly effective:\n\n\n  \n    Industry/Business combines Supervised Learning with Unsupervised Learning, enabling accurate predictions while discovering valuable patterns in customer data.\n  \n  \n    Research pairs Statistical Modelling with Machine Learning, balancing academic rigour with modern capabilities.\n  \n  \n    Product Development merges Deep Learning with Supervised Learning for end-to-end features with clear metrics.\n  \n  \n    Medical Diagnostics unites Supervised Learning with Statistical Modelling, crucial for evidence-based decisions with proper uncertainty quantification.\n  \n\n\nThe choice should be based on the practitioner‚Äôs domain requirements, computational resources, interpretability needs, and available time for mastery. Remember: Mastery of one mindset with broad awareness surpasses superficial knowledge of many.\n\n\n\n  \n    \n      Because Bayesian models treat everything as probability distributions (rather than fixed values), any predictions or conclusions automatically include their associated uncertainty. For example, if you predict someone‚Äôs future income using multiple uncertain factors, the final prediction comes as a range of possibilities with their probabilities, rather than just a single number.¬†&#8617;\n    \n    \n      Here is a nice digital flashcard by Chris Albon, on the concept of curse of dimensionality¬†&#8617;\n    \n  \n\n",
        "url": "/modelling-mindsets/",
        "date": "November 27, 2024"
      },
    
      {
        "title": "üí° TIL: Pydantic, Python's Data Validation Guard",
        "content": "\n\nIntroduction\nToday I started using Pydantic, a Python library that handles data validation through Python type annotations. Pydantic brings runtime type checking and data validation that catches errors before they cause mysterious bugs in an application. It uses type hints to validate data at runtime, automatically converting and validating data types, preventing bugs, and reducing boilerplate code. It‚Äôs essential for robust API development, configuration management, and data processing pipelines.\n\nUnderstanding Pydantic and Its Value\nPydantic leverages Python‚Äôs type hints to validate data structures. It converts your type hints from mere documentation into active runtime checks, ensuring data consistency throughout your application. Here are Pydantic‚Äôs key features:\n\nType Enforcement\n\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n    email: str\n\n# This raises a ValidationError\nuser = User(name=\"John\", age=\"not_a_number\", email=\"john@example.com\")\n\n\nAutomatic Type Coercion\n\nclass Order(BaseModel):\n    quantity: int\n    price: float\n\n# Pydantic automatically converts valid strings to numbers\norder = Order(quantity=\"3\", price=\"9.99\")\nprint(order.quantity)  # 3 (int)\nprint(order.price)    # 9.99 (float)\n\n\nReal-World Benefits\n\n  API Development: Validates incoming JSON data automatically\n  Configuration Management: Ensures config files meet your specifications\n  Database Operations: Validates data before insertion\n  Data Parsing: Converts between JSON, dictionaries, and model instances seamlessly\n\n\nWhy It Matters\n\n  Error Prevention: Catches data issues at system boundaries\n  Clean Code: Reduces validation boilerplate\n  Self-Documenting: Type hints serve as both validation rules and documentation\n  Performance: Compiled validation code runs efficiently\n\n\nConclusions\nPydantic transforms Python‚Äôs type hints from passive documentation into active data validators, significantly reducing runtime errors and improving code reliability.\n",
        "url": "/TIL-pydantic/",
        "date": "November 26, 2024"
      },
    
      {
        "title": "üè∫ Historical Evolution of AI",
        "content": "\n\nAI‚Äôs Historical Evolution\n\nIntroduction\nThe field of Artificial Intelligence has undergone several paradigm shifts since its inception, each representing a distinct approach to creating intelligent systems. Drawing from Pedro Domingos‚Äô framework in The Master Algorithm we can trace how different schools of thought have shaped our understanding and implementation of AI technologies.\n\nHistorical Evolution\nEarly Foundations: Symbolic AI and Expert Systems (1950s-1970s)\nThe pioneers of AI began with symbolic reasoning, believing intelligence could be reduced to symbol manipulation. This symbolist approach offered explicit reasoning chains and interpretability but struggled with real-world complexity. Expert Systems followed, successfully applying rule-based reasoning to narrow domains while revealing the challenges of scaling knowledge-based systems.\n\nThe Rise of Neural Approaches (1980s-1990s)\nThe connectionist movement emerged with neural networks, drawing inspiration from biological systems. This era introduced pattern recognition capabilities and learning from examples. Simultaneously, the Bayesian school brought statistical methods to the forefront, offering principled approaches to handling uncertainty but requiring significant data and computational resources.\n\nThe Data Revolution (2000s-2010s)\nBig Data and Deep Learning foundations emerged as the analogiser school gained prominence. This period saw the convergence of massive datasets, computational power, and sophisticated architectures. Deep Learning breakthrough demonstrated the power of automatic feature learning, though at the cost of increased computational demands and reduced interpretability.\n\nContemporary AI: The Era of Integration (2020s)\nCurrent AI systems, particularly large language models, represent a synthesis of multiple schools. They combine symbolic reasoning1, neural architectures2, and statistical learning3, achieving impressive generative capabilities and few-shot learning. However, they face challenges in resource requirements, reliability, and alignment with human values. A nicely distilled overview of what is today‚Äôs AI, comes from an Andrej Karpathy Tweet.\n\nConclusions\nThe evolution of AI reveals a field shaped by competing philosophies, each contributing essential insights. As Domingos argues, the future likely lies not in the dominance of any single approach but in their unification. While recent advances demonstrate the potential of synthesising different methods, significant challenges remain in creating truly intelligent systems that are both powerful and reliable.\n\nThe path forward requires building on these foundations while addressing core challenges in efficiency, interpretability, and alignment. Rather than choosing between different schools of thought, the field must continue to integrate their strengths while mitigating their individual weaknesses.\n\n\n\n  \n    \n      Symbolic reasoning in modern AI manifests through attention mechanisms and transformers‚Äô ability to process structured input like code or mathematical expressions. While not explicitly rule-based like early AI, these systems can learn and apply symbolic patterns.¬†&#8617;\n    \n    \n      Neural architectures in contemporary AI primarily use the transformer architecture, where self-attention layers process information in parallel, allowing the model to weigh the importance of different inputs contextually.¬†&#8617;\n    \n    \n      Statistical learning appears in the form of probabilistic token prediction and the use of large-scale statistical patterns learned during training. Models learn probability distributions over sequences, enabling them to generate coherent outputs.¬†&#8617;\n    \n  \n\n",
        "url": "/ai-evolution/",
        "date": "November 23, 2024"
      },
    
      {
        "title": "üîÑ Considering Iterative Refinement Over Unit Testing",
        "content": "\n\nIntroduction\nIn the realm of software development and related fields, three influential figures -Peter Norvig (former Director of Research at Google), Jeremy Howard (founder of fast.ai), and Grant Sanderson (creator of 3Blue1Brown)- demonstrate the power of iterative refinement over rigid test-driven development. Their approaches, while applied in different domains, share common principles that challenge traditional development practices.\n\nIterative Refinement\nPeter Norvig‚Äôs Software Development\nNorvig‚Äôs approach, demonstrated in both his original docex module and his spell corrector implementation, emphasises tests that are tightly coupled with the code they verify. Before Python‚Äôs doctests1 were officially supported, he created the docex module specifically to write tests in docstrings using a concise syntax like\ndef factorial(n):\n    \"\"\"Return the factorial of n, an exact integer &gt;= 0.\n       &gt;&gt;&gt; [factorial(n) for n in range(6)]\n       [1, 1, 2, 6, 24, 120]\n       It must also not be ridiculously large:\n       &gt;&gt;&gt; factorial(1e100)\n       Traceback (most recent call last):\n       ...\n       OverflowError: n too large\n    \"\"\"\n    ...\n\nif __name__ == \"__main__\":\n    import doctest\n    doctest.testmod()\n\n\n$ python fact.py -v\nTrying:\n    [factorial(n) for n in range(6)]\nExpecting:\n    [1, 1, 2, 6, 24, 120]\nok\nTrying:\n    factorial(1e100)\nExpecting:\n    Traceback (most recent call last):\n        ...\n    OverflowError: n too large\nok\n2 items passed all tests:\n   1 test in __main__\n   6 tests in __main__.factorial\n7 tests in 2 items.\n7 passed.\nTest passed.\n$\n\nEven in his spell corrector, Norvig uses simple functions with in-line test cases rather than separate test files. This approach keeps tests close to the code they verify, making them part of the living documentation rather than separate artefacts that can drift out of sync.\nUpdate: While randomly skimming through PyTorch code, it was good to stumble across examples of code containing doctests.\n\nJeremy Howard‚Äôs Machine Learning Development\nHoward‚Äôs methodology, evidenced in fast.ai‚Äôs development and his book ‚ÄúDeep Learning for Coders‚Äù advocates for rapid prototyping in notebooks. His emphasis lies in getting end-to-end solutions working quickly, then iteratively improving them based on actual usage patterns. In his latest course SolveIt, Howard extends this iterative philosophy to Dialogue Engineering, i.e. using Large Language Models in an iterative conversation to develop solutions, demonstrating how modern AI can be integrated into the development workflow while maintaining the principles of continuous refinement.\n\nGrant Sanderson‚Äôs Visual Mathematics\nThis iterative philosophy extends to mathematical animations. In Grant Sanderson‚Äôs How I animate video, he demonstrates how he builds visualisations incrementally, starting with basic shapes and gradually refining them while continuously previewing the results. This approach allows for creative exploration while maintaining momentum.\n\nThe Problem with Traditional Testing\nTraditional unit testing often fragments development workflow by requiring separate test maintenance and can lead to ossified code structures. When tests aren‚Äôt exercised regularly, they become outdated, creating false confidence. This is particularly problematic in rapidly evolving domains like AI, where interfaces and requirements frequently change.\n\nConclusions\nInstead of extensive unit test suites, it‚Äôs worth considering:\n\n  Writing working code first\n  Using doctests for critical functions\n  Relying on end-to-end validation\n  Refactoring based on actual usage patterns\n  Keeping tests focused on stable interfaces\n\n\nThis approach reduces maintenance burden while ensuring code remains reliable where it matters most, that is in production.\n\n‚ÄúPrograms must be written for people to read, and only incidentally for machines to execute‚Äù - Abelson &amp; Sussman. The same applies to tests.\n\n\n\n  \n    \n      Python has supported doctests natively since v2.6.9¬†&#8617;\n    \n  \n\n",
        "url": "/iterative-refinement/",
        "date": "November 22, 2024"
      },
    
      {
        "title": "„Ç∑ Back to Basics: A Modern, Minimal Python Toolchain",
        "content": "\n\nIntroduction\nPython‚Äôs ecosystem for Data Science and AI is unmatched in its depth and maturity. Yet, its fragmented tooling landscape often leads to decision paralysis and opinions galore: virtualenv or venv? pip or conda? black or flake8? These choices, while providing flexibility, can create unnecessary cognitive load and often foster dogmatic opinions about ‚Äúthe right way‚Äù to do things.\nAfter exploring alternative stacks, I‚Äôm returning to Python. Not least because it‚Äôs perfect, but because it‚Äôs productive. The challenge isn‚Äôt Python‚Äôs capabilities; it‚Äôs the abundance and complexity of its tooling. This article presents a carefully curated, minimal toolkit that leverages Python‚Äôs ecosystem while avoiding its common setup pitfalls.\n\nMotivation\nThe appeal of integrated toolchains like Deno 2.0 is undeniable. Zero setup, immediate productivity, and a cohesive development experience. My recent exploration of alternative stacks revealed the value of unified tools that just work. While JavaScript‚Äôs ecosystem for Data Science and AI is growing rapidly, it still lacks the depth and maturity of Python‚Äôs scientific computing stack.\nThis exploration led to an important realisation: aside from an expansive Data and AI ecosystem, Python development can be achieved with a streamlined workflow that increases productivity and decreases complexity. Rather than accepting the cognitive overhead of multiple competing tools, I decided to create my own compact toolchain that meets most Data Science and AI requirements with minimalism, simplicity, and clarity in mind.\nThe goal isn‚Äôt to prescribe another ‚Äúright way‚Äù of doing things, but rather to demonstrate how a carefully chosen set of modern tools can create a development experience that rivals the integrated approaches of newer platforms while leveraging Python‚Äôs mature ecosystem.\n\nMy Approach\nLocal Development\nMy toolchain starts with the following foundational choices that eliminate common Python setup headaches:\n\n\n  PEP8: Let‚Äôs start with a style guide, so that the team is on the same page\n  uv: A blazing-fast Python package and project manager, written in Rust. It replaces pip, pip-tools, pipx, poetry, pyenv, twine, virtualenv, and more, providing:\n    \n      Consistent dependency resolution\n      Lightning-fast package installations\n      Built-in virtual environment management\n      Direct integration with pyproject.toml\n    \n  \n  pyproject.toml: The single source of truth for project configuration. For example:\n     [project]\n name = \"my-ds-project\"\n version = \"0.1.0\"\n dependencies = [\n     \"polars\",\n     \"tensorflow\",\n     \"plotly\"\n ]\n\n [tool.ruff]\n line-length = 90 \n select = [\"E\", \"F\", \"I\"]\n\n # Required only if you use pytest for unit testing\n [tool.pytest.ini_options]\n testpaths = [\"tests\"]\n    \n  \n  Ruff: A Rust-based tool that combines formatting and linting, replacing the need for black, flake8, isort etc.:\n    \n      Single-tool code quality enforcement\n      Configurable through pyproject.toml\n      Significantly faster than Python-based alternatives\n    \n  \n  pyright: Static Type Checker for Python\n    \n      Static type checker\n      Standards compliant\n      Configurable within pyproject.toml\n    \n  \n  iterative refinement: An approach that tightly couples (doc)tests with code, ensuring up-to-dateness\npytest: Handles testing with minimal boilerplate and rich assertions\n\n\nCross-Platform Distribution\n\n  PyInstaller for creating stand-alone executables\n  GitHub Actions workflow for automated builds:\n    - name: Build executables\n  run: |\n pyinstaller --onefile src/main.py\n    \n  \n  Local cross-compilation using Podman:\n    FROM python:3.13-slim\nCOPY . /app\nWORKDIR /app\nRUN pip install pyinstaller\nCMD pyinstaller --onefile src/main.py\n    \n  \n\n\nData Science\nA carefully selected set of powerful libraries that minimize overlap:\n\n\n  Polars: Fast DataFrame operations with a cohesive API. Why?\n\n\n    import polars as pl\n\n    def analyze_customer_behavior(path: str):\n        return (\n            pl.scan_parquet(path)\n            .with_columns([\n                pl.col(\"purchase_date\").str.to_datetime(),\n                (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total_spend\")\n            ])\n            .group_by([\n                pl.col(\"customer_id\"),\n                pl.col(\"purchase_date\").dt.month().alias(\"month\")\n            ])\n            .agg([\n                pl.col(\"total_spend\").sum().alias(\"monthly_spend\"),\n                pl.col(\"product_id\").n_unique().alias(\"unique_products\"),\n                pl.col(\"purchase_date\").count().alias(\"purchase_frequency\")\n            ])\n            .sort([\"customer_id\", \"month\"])\n            .collect()\n        )\n\n\n\n  TensorFlow 2: Deep learning when needed\n\n\n    import tensorflow as tf\n    mnist = tf.keras.datasets.mnist\n\n    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n    x_train, x_test = x_train / 255.0, x_test / 255.0\n\n    model = tf.keras.models.Sequential([\n      tf.keras.layers.Flatten(input_shape=(28, 28)),\n      tf.keras.layers.Dense(128, activation='relu'),\n      tf.keras.layers.Dropout(0.2),\n      tf.keras.layers.Dense(10, activation='softmax')\n    ])\n\n    model.compile(optimiser='adam',\n      loss='sparse_categorical_crossentropy',\n      metrics=['accuracy'])\n\n    model.fit(x_train, y_train, epochs=5)\n    model.evaluate(x_test, y_test)\n\n\n\n  XGBoost: Gradient boosting for structured data\n\n\n    from xgboost import XGBClassifier\n    # read data\n    from sklearn.datasets import load_iris\n    from sklearn.model_selection import train_test_split\n    data = load_iris()\n    X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=.2)\n    # create model instance\n    bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n    # fit model\n    bst.fit(X_train, y_train)\n    # make predictions\n    preds = bst.predict(X_test)\n\n\n\n  Plotly: Interactive visualizations\n\n\n    import plotly.express as px\n    df = px.data.iris()\n    fig = px.scatter(df, x=\"sepal_width\", y=\"sepal_length\", color=\"species\", symbol=\"species\")\n    fig.show()\n\n\n  MLFlow: Managing the Machine Learning Lifecycle\n\n\n      \n\n\n\nAI Engineering\nWith hybrid solutions becoming more prevalent nowadays, we can use a combination of tools.\n\n\n  Ollama: Local model deployment and inference\n      import ollama\n\n  def technical_advisor():\n      messages = [\n          {\n              \"role\": \"system\",\n              \"content\": \"You are a technical advisor specializing in Python architecture.\"\n          },\n          {\n              \"role\": \"user\",\n              \"content\": \"What's the best way to handle database migrations?\"\n          }\n      ]\n        \n      response = ollama.chat(model='llama2', messages=messages)\n      messages.append(response['message'])\n        \n      # Follow-up question with context\n      messages.append({\n          \"role\": \"user\",\n          \"content\": \"How would that work with SQLAlchemy specifically?\"\n      })\n        \n      return ollama.chat(model='llama2', messages=messages)\n    \n  \n  LlamaIndex: RAG pipeline construction using local LLMs or external APIs\n      from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n  from llama_index.core.node_parser import SentenceSplitter\n  from llama_index.core.retrievers import VectorIndexRetriever\n  from llama_index.core.query_engine import RetrieverQueryEngine\n\n  def create_custom_rag():\n      # Load and parse documents\n      documents = SimpleDirectoryReader(\"technical_docs\").load_data()\n      parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n      nodes = parser.get_nodes_from_documents(documents)\n        \n      # Create index with custom settings\n      index = VectorStoreIndex(nodes)\n        \n      # Custom retriever with similarity threshold\n      retriever = VectorIndexRetriever(\n          index=index,\n          similarity_top_k=3,\n          filters=lambda x: float(x.get_score()) &gt; 0.7\n      )\n        \n      # Create query engine with custom retriever\n      query_engine = RetrieverQueryEngine(retriever=retriever)\n      return query_engine\n    \n  \n  MongoDB: A distributed document DB that supports vector storage and graph operations\n      from pymongo import MongoClient\n  import numpy as np\n\n  def vector_search(text_embedding: np.ndarray, threshold: float = 0.8):\n      client = MongoClient(\"mongodb://localhost:27017/\")\n      db = client.vector_db\n        \n      pipeline = [\n          {\n              \"$search\": {\n                  \"index\": \"vector_index\",\n                  \"knnBeta\": {\n                      \"vector\": text_embedding.tolist(),\n                      \"path\": \"embedding\",\n                      \"k\": 5\n                  }\n              }\n          },\n          {\n              \"$match\": {\n                  \"score\": {\"$gt\": threshold}\n              }\n          },\n          {\n              \"$project\": {\n                  \"_id\": 0,\n                  \"text\": 1,\n                  \"score\": {\"$meta\": \"searchScore\"}\n              }\n          }\n      ]\n        \n      return list(db.documents.aggregate(pipeline))\n    \n  \n\n\nUpdate: Looking into Weaviate as an all-in-one DB solution.\n\nThis stack provides everything needed for modern Data Science and AI work while maintaining clarity and minimising tool overlap.\n\nConclusions\nReturning to Python with this minimal, modern toolchain has proven to be a pragmatic choice. The combination of uv, Ruff, and Pytest creates a more unified development workflow, while retaining access to Python‚Äôs mature scientific computing ecosystem.\n\nKey benefits of this approach:\n\n  Reduced Cognitive Load: One tool per task eliminates decision fatigue\n  Modern Performance: Rust-based tools (uv, Ruff) provide near-instant feedback\n  Simplified Configuration: Single pyproject.toml as source of truth\n  Production Ready: Direct path from development to cross-platform deployment\n  Full Feature Set: Complete Data Science and AI capabilities without bloat\n  Flexible AI Stack: Seamless integration between local models (Ollama), RAG pipelines (LlamaIndex), and vector storage (MongoDB)\n  Production AI: Easy transition from experimentation to production AI systems with consistent tooling\n\n\nWhile Python‚Äôs ecosystem will likely remain fragmented, we don‚Äôt have to accept the complexity. By carefully choosing modern tools that prioritise speed, simplicity, and clarity, we can create a development environment that‚Äôs both powerful and pleasant to use.\n\nThe beauty of this approach lies not in its prescriptiveness, but in its principles: minimize tooling, maximise capability, and maintain clarity. Whether you adopt this exact stack or use it as inspiration for your own, the goal remains the same: bring the focus back to solving problems rather than managing tools.\n",
        "url": "/bring-it-back-to-basics/",
        "date": "November 21, 2024"
      },
    
      {
        "title": "üí° TIL: TF-IDF vs BM25",
        "content": "\n\nIntroduction\nWhen building search engines or document retrieval systems, two algorithms often come up: TF-IDF and Okapi BM25. While both aim to rank documents by relevance, they differ significantly in their approach and effectiveness. Today, I learned the key differences between these techniques and when to use each one.\n\nTF-IDF: The Classic Approach\nTF-IDF (Term Frequency-Inverse Document Frequency) ranks documents based on how frequently terms appear in a document, weighted by how rare those terms are across all documents. It‚Äôs straightforward: if a word appears often in a document but is rare across the corpus, it‚Äôs probably important1. $idf$ is calculated as follows:\n\n\\[idf(t) = \\log\\frac{N}{n_t}\\]\n\nwhere:  \n$N$ : Total number of documents in corpus\n$n_t$ : Number of documents containing term $t$\n\nTF-IDF is derived by the following calculation:\n\n\\[TF\\text{-}IDF(t,d) = tf(t,d) \\cdot idf(t)\\]\n\nwhere:  \n$tf(t,d)$ : Frequency of term $t$ in document $d$\n\nAdvantages\n\n  Simple to understand and implement\n  Computationally efficient\n  Works well for documents of similar length\n  Great for basic document classification\n\n\nDisadvantages\n\n  No term frequency saturation (more occurrences always mean higher scores)\n  Doesn‚Äôt handle varying document lengths well\n  Can overemphasise common terms in long documents\n\n\nBM25: The Modern Evolution\nBM25 (Best Match 25) builds upon TF-IDF‚Äôs foundation but adds two crucial improvements: term frequency saturation and document length normalisation. Note how the $idf_{BM25}$ component differs from TF-IDF‚Äôs:\n\n\\[idf_{BM25}(t) = \\log\\frac{N - n_t + 0.5}{n_t + 0.5}\\]\n\nThis modification provides smoother IDF weights and better handles edge cases.\n\n\\[BM25(t,d) = \\frac{tf(t,d) \\cdot (k_1 + 1)}{tf(t,d) + k_1 \\cdot (1 - b + b \\cdot \\frac{|d|}{avgdl})} \\cdot idf_{BM25}\\]\n\nwhere:  \n$tf(t,d)$ : Frequency of term $t$ in document $d$\n$|d|$ : Length of document $d$ (in words)\n$avgdl$ : Average document length in corpus\n$k_1$ : Term frequency saturation parameter (typically 1.2-2.0)\n$b$ : Length normalization parameter (typically 0.75)\n$N$ : Total number of documents in corpus\n$n_t$ : Number of documents containing term $t$\n\nAdvantages\n\n  Better handles varying document lengths\n  Prevents term frequency from dominating scores\n  More nuanced relevance rankings\n  Industry standard for search engines\n\n\nDisadvantages\n\n  More complex implementation\n  Requires parameter tuning\n  Slightly higher computational cost\n  Less interpretable than TF-IDF\n\n\nWhich to Choose?\n\nChoose TF-IDF when:\n\n  Building basic document classification systems\n  Working with uniformly-sized documents\n  Needing interpretable results\n  Prioritising implementation simplicity\n\n\nChoose BM25 when:\n\n  Building a search engine\n  Handling documents of varying lengths\n  Requiring state-of-the-art retrieval performance\n  Working with user queries\n\n\nConclusions\nWhile TF-IDF remains valuable for simpler tasks and educational purposes, BM25 is generally superior for serious search applications. The choice between them often comes down to the trade-off between simplicity and sophistication. For modern search engines, BM25 is the clear winner, but TF-IDF‚Äôs simplicity makes it perfect for learning and basic applications.\n\nRemember: the best algorithm is the one that meets your specific needs. Don‚Äôt automatically reach for BM25 just because it‚Äôs more advanced ‚Äì sometimes, simpler is better.\n\n\n  \n    \n      This is why TF-IDF is effective at identifying characteristic terms in documents. It automatically downweights common words like ‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúis‚Äù while highlighting distinctive terms that appear frequently in specific documents.¬†&#8617;\n    \n  \n\n",
        "url": "/TIL-BM25-TFIDF/",
        "date": "November 20, 2024"
      },
    
      {
        "title": "üÜô Level Up With Dialogue Engineering",
        "content": "\n\nIntroduction\nDialogue Engineering is transforming how we interact with AI1. Rather than relying on one-shot prompts, it‚Äôs an iterative approach where we engage in structured, multi-turn conversations with LLMs (Large Language Models) to achieve complex goals. While I first encountered the term through Jeremy Howard2 3, the concept has deeper roots in human-AI interaction research. Though Howard popularized it recently through fast.ai and answer.ai, the concept has been discussed since 19864.\nDialogue engineering dramatically improves productivity by breaking down complex tasks, maintaining context across interactions, and guiding AI through iterative refinement. This systematic approach helps produce better results while reducing the cognitive load of prompt crafting. A nice overview of Dialogue Engineering comes from the Medium article Dialog Engineering: AI as Your Research Assistant.\nBelow, I‚Äôll summarise what I inferred from that article.\n\nHow Dialogue Engineering Works\n\n  Setting the Scenario\nThis first step involves defining clear objectives and research questions before engaging with AI. Rather than diving into broad topics, we frame specific goals and provide relevant context. For example, when starting a research project, we outline exactly what we need to investigate and any important background information the AI should consider.\nBest Practice: Be clear and specific about goals, provide relevant background information to help AI understand context.\n  Gathering Information\nOnce the scenario is set, we guide the AI in collecting and organising relevant data. This could involve creating annotated bibliographies, summarising key sources, or compiling research findings. The AI helps structure this information in a way that‚Äôs useful for the next steps.\nBest Practice: Request structured formats like annotated bibliographies, ask for citations and evidence to ensure accuracy.\n  Structuring the Outline\nBefore diving into content creation, we work with the AI to develop a clear roadmap. This outline breaks down the task into logical sections, ensuring a coherent flow and manageable chunks of work.\nBest Practice: Break the task into clear sections, ensure logical connections between parts that reflect overall goals.\n  Generating Content Iteratively\nWith the outline in place, we tackle each section individually through iterative refinement. Rather than expecting perfect content immediately, we provide feedback and guide the AI to improve its outputs progressively.\nBest Practice: Work on sections individually to maintain focus, use feedback loops to guide AI toward more specific, accurate outputs.\n  Conclusion and Introduction Refinement\nThe final step involves revisiting the opening and closing sections once the main content is complete. This ensures these crucial parts accurately reflect and synthesise the entire piece.\nBest Practice: Write introduction last to accurately reflect content, craft conclusion by synthesising main takeaways from each section.\n\n\nThroughout all steps, I maintain active oversight, checking for accuracy and providing clear feedback. This systematic approach has dramatically improved my productivity while ensuring high-quality outputs.\n\nPractical Applications\nHere are the key areas where dialogue engineering proves particularly valuable:\n\n\n  Academic Research\nResearchers can leverage dialogue engineering to synthesize vast amounts of information, structure complex arguments, and ensure accurate citations. The iterative approach is particularly useful for literature reviews and thesis development.\nExample: A researcher prompts AI to generate an annotated bibliography on AI-driven diagnostics, focusing on recent studies, then iteratively refines the summaries and findings.\n  Business Strategy and Reporting\nFor corporate applications, dialogue engineering helps generate market reports, analyse trends, and produce comprehensive strategy documents. This systematic approach ensures consistency while maintaining analytical depth.\nExample: Business analysts use iterative prompts to draft sections of market reports, starting with ‚ÄúGenerate a section on e-commerce trends focusing on AI-driven personalisation‚Äù then refining based on specific data points.\n  Report Automation\nDialogue Engineering excels at automating recurring business reports, such as quarterly financial reviews or performance summaries. The structured approach ensures consistency while allowing for customisation.\nExample: Teams automate quarterly reports by structuring templates with AI, feeding relevant data, and using iterative refinement to maintain accuracy and freshness.\n  Content Creation and Media\nContent creators can streamline the production of articles, blogs, and multimedia scripts through structured dialogue with AI. This approach particularly shines in drafting and revising content iteratively.\nExample: Writers use dialogue engineering to draft introductory paragraphs, then iterate with prompts for more engaging language or additional examples.\n  Technical Writing and Documentation\nIn fields requiring precise technical documentation, dialogue engineering helps ensure clarity, accuracy, and consistency across complex documents.\nExample: Software engineers use dialogue engineering to draft technical documentation for new features, prompting ‚ÄúDraft a technical overview of the new user authentication feature‚Äù then refining for clarity and technical accuracy.\n\n\nEach of these applications benefits from dialogue engineering‚Äôs structured, iterative approach, leading to more efficient workflows and higher-quality outputs.\n\nBest Practices\nKey best practices include:\n\n  Precision in Prompts\nCraft prompts that are neither too vague nor overly specific. Focus on clear, well-structured queries that guide AI towards relevant outputs.\nExample: Instead of ‚ÄúTell me about AI in healthcare‚Äù use ‚ÄúWhat are the latest advancements in AI-driven diagnostics in healthcare, particularly in image recognition?‚Äù\n  Iterative Refinement\nBuild on each interaction, using feedback to improve outputs gradually rather than expecting perfection immediately.\nExample: Start with a draft section, then refine with follow-up prompts like ‚ÄúExpand on the use of dialogue engineering in business reporting, specifically market trend analysis.‚Äù\n  Leverage Feedback Loops\nMaintain continuous cycles of prompting, feedback, and refinement to improve output quality over time.\nExample: When creating an outline, start broad, then use feedback to add specific sections on practical examples in different domains.\n  Source and Citation Checking\nVerify AI-generated sources and citations manually, as AI models lack real-time access to databases.\nExample: Cross-reference any cited statistics or research papers with trusted external sources before including them in final outputs.\n  Structure Before Diving In\nCreate clear outlines or plans before generating detailed content to ensure logical flow and completeness.\nExample: Start with a structured outline for a Medium article, then develop each section iteratively.\n  Mind Token Limits\nBreak down long content into manageable chunks to work within AI model token limits.\nExample: Generate long-form content section by section, refining each piece before moving to the next.\n\n\nHowever, we should be aware of the limitations (and challenges) of Dialogue Engineering too.\n\nUnderstanding the Limitations\nWhile these best practices enhance the use of dialogue engineering, it‚Äôs essential to acknowledge its constraints and challenges. Like any powerful tool, dialogue engineering comes with limitations that require careful consideration and management. Here‚Äôs what we need to keep in mind:\n\nKey Limitations and Challenges\nThe foremost concern when using generative AI is accuracy and hallucinations. LLMs can sometimes generate plausible-sounding but false information, necessitating rigorous fact-checking processes. This is particularly critical in professional contexts where accuracy is paramount.\nEthical implications also demand attention. While AI can streamline work processes, maintaining authenticity and proper attribution is crucial. This connects directly to the need for consistent human oversight, that is users must actively review outputs, ensure quality control, and make ethical judgements about the content‚Äôs appropriateness and accuracy.\nAI‚Äôs current limitations in understanding context and nuance present another challenge. Models may struggle with subtle distinctions or produce oversimplified explanations, especially in specialised fields. Technical constraints, particularly token limits and handling complex, multi-layered reasoning tasks, further necessitate careful planning and task breakdown.\nThese limitations underscore a crucial point: dialogue engineering works best as a collaborative tool that enhances, rather than replaces, human expertise and judgement.\n\nConclusions\nDialogue Engineering represents a significant evolution in human-AI interaction, moving beyond simple prompt engineering to create a dynamic, iterative approach. Through structured conversations and systematic refinement, it enables us to tackle complex tasks more efficiently across academic, business, and creative domains. While the technique requires careful attention to limitations like AI hallucinations and demands consistent human oversight, its power lies in treating AI as a collaborative partner rather than a one-shot tool. By following best practices and understanding its constraints, dialogue engineering becomes a force multiplier for productivity, helping us create better outputs while maintaining human expertise at the core of the process. This balance of systematic interaction and human judgement makes dialogue engineering a valuable framework for anyone looking to maximise the potential of AI tools in their workflow.\n\n\n\n  \n    \n      AI is an umbrella term that has meant different things over the years. Since 2022, it has become a synonym of Generative AI. Here‚Äôs a short AI timeline: Symbolic AI (1950-60s), Expert Systems (1970s), Neural Networks and Knowledge Representation (1980s), Machine Learning and Statistical Methods (1990s), Big Data and Deep Learning foundations (2000s), Deep Learning (2010s), Generative AI and Large Language Models (2020s)¬†&#8617;\n    \n    \n      Answer.ai &amp; AI Magic with Jeremy Howard¬†&#8617;\n    \n    \n      How To Solve It With Code¬†&#8617;\n    \n    \n      Foundations of dialog engineering: the development of human-computer interaction. Part II (Gaines et al., 1986)¬†&#8617;\n    \n  \n\n",
        "url": "/dialogue-engineering/",
        "date": "November 15, 2024"
      },
    
      {
        "title": "‚úÖ On-boarding that works",
        "content": "\n\nIntroduction\nI recently watched a lively presentation titled opinionated on-boarding, that discussed the dramatic impact of on-boarding practices on new staff and business success. The speaker, drawing from his experience, articulated that poor on-boarding is actively harming companies while good on-boarding can transform team productivity and retention.\nHaving experienced both ends of the spectrum myself, I strongly agree with the presenter‚Äôs central thesis. Companies can‚Äôt afford to waste months getting new staff up to speed, yet that‚Äôs what‚Äôs happening very frequently. The costs are staggering, both in lost productivity and squandered talent.\n\nThe Problem\nNew employees face an overwhelming cognitive burden as they simultaneously navigate multiple learning curves: mastering the tech stack, deciphering an unfamiliar codebase, adapting to team workflows, understanding the business domain, and learning organisational structures. Many companies worsen this situation through ineffective approaches, either expecting staff to self-direct their learning with vague instructions like ‚Äúgo learn X and tell us when you‚Äôre done‚Äù, or by immediately assigning them tickets without proper context or support. This inefficiency comes at a significant cost though. According to the presenter, the average software developer staying at a company for only 20 months, taking 6 months to become productive means losing nearly a third of their effective tenure1. Rather than fixing their on-boarding processes, many companies respond by exclusively hiring senior professionals who can ‚Äúhit the ground running‚Äù - an approach that not only limits their talent pool but proves unrealistic even for experienced hires.\n\nEffective On-boarding Strategies\nAccording to the presenter, research and experience show that effective on-boarding follows clear cognitive science principles. Rather than overwhelming new hires with broad, unfocused (occasionally learning) objectives, successful on-boarding programs recognise how human learning actually works and structure their approach accordingly. Two key principles emerge as foundational to any effective on-boarding strategy:\n\n\n  Focus on building specific capabilities rather than general ‚Äúunderstanding‚Äù\n  Account for cognitive limitations:\n    \n      People can only hold ~4 concepts in working memory\n      New concepts take more mental space than familiar ones\n      Skills must be practiced close to when they‚Äôre learned\n    \n  \n\n\nBest Practices\nThese principles translate into concrete best practices that any organisation can implement. While the specific skills and technologies may vary between teams, successful on-boarding programs share common structural elements that maximize learning efficiency while minimizing cognitive overload:\n\n\n  Break down complex skills into smaller, manageable components. For example, rather than asking someone to ‚Äúlearn LiveView‚Äù2 break it down into specific tasks like creating forms, handling events, or managing state\n  Provide structured learning paths rather than self-directed exploration. When new hires must decide what to learn next, they waste valuable mental capacity on planning rather than learning. A clear, predefined path eliminates this overhead\n  Aim for 80% proficiency before moving to next skill. This threshold ensures sufficient mastery while avoiding diminishing returns from pursuing perfection\n  Define minimum productive competency for the role. Not every skill needs to be mastered immediately - identify what‚Äôs truly needed for day-one productivity and focus there first\n  Use checklists and frequent practice with feedback. Clear checkpoints provide motivation and progress tracking, while regular feedback prevents learners from developing incorrect habits\n  Focus on mechanical competency to reduce cognitive load. When basic operations become automatic, developers can focus their mental energy on solving more complex problems\n\n\nConclusions\nEffective on-boarding is not just a nice-to-have, it‚Äôs a competitive necessity in today‚Äôs software industry. While poor on-boarding practices continue to cost companies valuable time and talent, the path to improvement is clear. By breaking down complex skills, providing structured learning paths, and respecting cognitive limitations, organisations can dramatically reduce the time it takes for new hires to become productive team members. This investment in structured on-boarding not only accelerates developer productivity but also expands hiring possibilities, allowing companies to tap into a broader talent pool. The choice is simple: continue losing months of productivity to ineffective on-boarding, or implement these evidence-based practices to build stronger, more capable engineering teams.\n\n\n\n  \n    \n      Since the presentation focused on software developers, I use it here as a proxy for various technical positions including AI Engineering, Data Science and others. Also, the tenure statistic may be skewed towards the U.S. market, however it‚Äôs true that many employees job hop in pursuit of a higher salary or a better job altogether¬†&#8617;\n    \n    \n      Phoenix LiveView ‚Äúis a process that received events, updates its state and renders updates to a page as diffs‚Äù¬†&#8617;\n    \n  \n\n",
        "url": "/good-onboarding/",
        "date": "November 14, 2024"
      },
    
      {
        "title": "üí™ The Advantage",
        "content": "\n\nIntroduction\nRecently I listened to a successful company‚Äôs CEO interview, where he explained how growing sustainably contributed to the company‚Äôs success. The CEO said that their growth strategy was inspired by Patrick Lencioni‚Äôs book The Advantage. I found the book‚Äôs premise very interesting, hence I‚Äôll attempt to summarise important points as a note to self.\n\nCentral Thesis\nOrganisational health is the single greatest competitive advantage a company can achieve, yet it‚Äôs often overlooked in favour of ‚Äúsmart‚Äù business decisions.\n\nKey components of organisational health\nLeadership team structure\n\n  Optimal size: 3-10 people\n  Built on trust and vulnerability\n  Values collective success over individual achievement\n\n\nCore principles\n\n  Trust and vulnerability as foundations\n  Accountability at all levels\n  Commitment to collective goals\n  Clear communication and expectations\n\n\nOperational excellence\n\n  Regular, focused meetings with specific purposes\n  Clear distinction between strategic and tactical discussions\n  Emphasis on debate and healthy conflict resolution\n  Continuous progress monitoring\n\n\nPeople and culture\n\n  Hire for cultural fit rather than training after hiring\n  Focus on values alignment in recruitment\n  Reward behaviour that aligns with organisational values\n  Foster an environment of open communication\n\n\nCritical success factors\n\n  Minimal internal politics\n  High clarity in communication\n  Clear decision-making processes\n  Low employee turnover\n  High morale and productivity\n\n\nConclusions\nThe book‚Äôs fundamental message is that creating a healthy organisation through strong leadership, clear communication, and aligned values is more important than traditional business metrics for long-term success. It‚Äôs not about being the ‚Äúsmartest‚Äù in the market, but about creating the healthiest internal environment.\n",
        "url": "/the-advantage/",
        "date": "November 14, 2024"
      },
    
      {
        "title": "üê¢ Slow Down And Grow Smart, Not Fast",
        "content": "\n\nIntroduction\nIn an industry obsessed with ‚Äúmove fast and break things,‚Äù some companies prove that measured growth creates lasting success. I was happy to listen to the CEO of a successful company recounting their success, which was largely thanks to slow and steady growth.\n\nSustainable Development Over Explosive Growth\nIt‚Äôs safe to say that consistency and growing at a controlled pace is a recipe for a successful sustainable [insert word here]. This strategy applies to most things in life, in my view. Here is what caught my attention from this discussion:\n\n  The company grew at a pace that allowed leaders to develop alongside the business\n  They focused on reaching profitability within 18-24 months after each funding round\n  They raised capital from a position of strength, not necessity\n  Capital was then deployed strategically rather than burning through runway\n\n\nBuilding Strong Foundations\nOrganisational health was centred around clear alignment through frameworks like The Advantage1. Company values have been embedded into daily processes. Goals are integrated into regular team workflows. Finally, maintaining strong communication across all levels has been key to the company‚Äôs success.\n\nCulture And Retention\nSuccess metrics go beyond financial growth. Key employees have stayed with the company long-term. Teams have maintained autonomy while staying accountable through balanced scorecards. Leadership has focused on creating clarity and empowering teams. Regular reinforcement of values has been achieved through good on-boarding2, and daily operations.\n\nKey Takeaways\n\n  Match growth to capability: Ensure your organization can sustainably support its growth rate\n  Focus on fundamentals: Build strong processes and systems that scale gradually\n  Invest in people: Give teams time and resources to develop alongside the company\n  Deploy capital wisely: Prioritize sustainable growth over rapid cash burn\n\n\nConclusions\nThis is the TL;DR really: Building a successful tech company doesn‚Äôt require breakneck speed or unsustainable growth. Smart, measured expansion with a focus on people and processes creates resilient businesses that stand the test of time.\n\n\n\n  \n    \n      ‚Äú[the author] makes an overwhelming case that organizational health will surpass all other disciplines in business as the greatest opportunity for improvement and competitive advantage.‚Äù¬†&#8617;\n    \n    \n      Characteristics of a good on-boarding program: a) One thing at a time, b) Lots of practice (with feedback), c) Attain proficiency, then move forward, d) The learner follows rather than guides, e) ‚ÄúMinimum Productive Competency‚Äù¬†&#8617;\n    \n  \n\n",
        "url": "/slow-down/",
        "date": "November 14, 2024"
      },
    
      {
        "title": "üñ• The On-Prem Comeback (aka Cloud Repatriation)",
        "content": "\n\nIntroduction\nMore recently, a notable shift is emerging in how organisations approach their cloud infrastructure. Some companies are beginning to move their applications and data away from public cloud providers like AWS, GCP and Azure. This marks a shift in the computing landscape.\n\nWhat Does Cloud Repatriation Mean?\nCloud repatriation refers to the process of moving applications, services and data from public cloud environments back to on-premises data centres, private clouds or hybrid set-ups. This reverse migration represents a pivot from the ‚Äúcloud-first‚Äù mindset that has dominated in the last few years.\n\nWhy It‚Äôs Happening\nSeveral key factors are driving this trend. For larger companies, cost is a real consideration, with scale-ups such as 37signals projecting savings of ¬£2 million p.a. by leaving AWS. Performance issues and rising cloud costs have led major organisations like GEICO to reconsider their cloud strategy after experiencing 2.5x increases in their bills alongside reliability challenges.\nData privacy, compliance requirements and the desire to avoid vendor lock-in are also significant motivators aside from growing costs. Many organisations are finding that running certain workloads on-premises or in hybrid environments offers better control over their infrastructure and data. See optimising infrastructure for AI for a nice overview on cloud&lt;-&gt;on-prem.\n\nConclusions\nCloud repatriation isn‚Äôt about completely abandoning public clouds but rather about finding the right balance. For organisations with predictable workloads and sufficient technical expertise, a strategic combination of on-premises, private cloud and public cloud infrastructure might prove more effective than a public-cloud-only approach.\n",
        "url": "/cloud-repatriation/",
        "date": "November 14, 2024"
      },
    
      {
        "title": "üí° TIL: vLLM Is A High-Performance Engine For LLM Serving",
        "content": "\n\nIntroduction\nAs a Data Scientist / AI Engineer exploring local-first solutions1 2, deploying Large Language Models (LLMs) presents significant resource management challenges. vLLM emerges as a breakthrough solution that fundamentally reimagines how we deploy and utilize these resource-intensive models3.\n\nWhat Is vLLM?\nvLLM is an open-source serving engine that optimizes LLM deployment through virtualization techniques3. At its core, vLLM introduces PagedAttention, a novel attention algorithm that improves memory utilization through paged memory management4. Similar to how operating systems manage virtual memory, PagedAttention segments the key-value memory into non-continuous pages, enabling more efficient memory usage and request handling.\n\nKey features3:\n\n  Efficient memory management through PagedAttention\n  Continuous batching for request handling\n  Support for popular open-source models (Llama, Mistral, Falcon)\n\n\nImplementation\nHere‚Äôs a simple example of using vLLM:\n\nfrom vllm import LLM, SamplingParams\n\n# Initialize the model\nllm = LLM(model=\"meta-llama/Llama-3.1-8B\")\n\n# Define sampling parameters\nsampling_params = SamplingParams(\n    temperature=0.7,\n    max_tokens=128\n)\n\n# Generate text\noutputs = llm.generate([\"Your prompt goes here\"], sampling_params)\n\n\nApplications\nAccording to industry analysis3, vLLM‚Äôs applications span multiple domains:\n\n\n  Natural Language Processing\n    \n      Enhances chatbots and sentiment analysis\n      Improves language translation services\n    \n  \n  Healthcare\n    \n      Enables secure patient data analysis\n      Assists in medical diagnostics\n    \n  \n  Financial Services\n    \n      Powers fraud detection systems\n      Enhances automated customer service\n    \n  \n  Education\n    \n      Facilitates intelligent tutoring systems\n      Enables automated assessment tools\n    \n  \n\n\nBest Practices for Implementation3\nFor optimal vLLM deployment:\n\n  Implement model optimization techniques\n  Utilize containerization for scalable deployment\n  Maintain robust monitoring systems\n  Regular performance optimization\n\n\nConclusion\nvLLM represents a significant advancement in LLM serving technology4, offering an efficient, scalable solution for resource-constrained environments. Its innovative approach to memory management through PagedAttention and broad applicability across industries makes it an essential tool for modern AI development.\n\n\n\n  \n    \n      Cloud Repatriation: Examples, Unpacking 2024 Trends &amp; Tips for Reverse Migration¬†&#8617;\n    \n    \n      Why Companies Are Ditching the Cloud: The Rise of Cloud Repatriation¬†&#8617;\n    \n    \n      vLLM Explained¬†&#8617;¬†&#8617;2¬†&#8617;3¬†&#8617;4¬†&#8617;5\n    \n    \n      Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.H., Gonzalez, J.E., Zhang, H., and Stoica, I. (2023). ‚ÄúEfficient Memory Management for Large Language Model Serving with PagedAttention.‚Äù arXiv:2309.06180.¬†&#8617;¬†&#8617;2\n    \n  \n\n",
        "url": "/TIL-vLLM/",
        "date": "November 13, 2024"
      },
    
      {
        "title": "üîÄ Cross-Platform Builds In Python",
        "content": "\n\nIntroduction\nIn the last couple of years I‚Äôve spoken to 3-4 people who had needed a bespoke data analysis tool that could be used locally, with privacy in mind. In some cases they‚Äôd need to work in a sandboxed environment for security reasons, other times they had IP protection concerns. A desktop app or a CLI tool1 seemed to fit the bill in all those cases. \nIn the last decade+, Data and Python have become a synonym. PyInstaller seemed like an obvious choice. However, PyInstaller cannot cross-compile code as it stands. Being a Linux user, offering to help Windows users, meant I should find a workaround e.g. leveraging GitHub Actions for cross-compilation.\n\nTaking The Scenic Route\nOut of curiosity, I decided to have a poke around a couple of different languages and ecosystems that could teach me a few things while helping me understand what is a viable alternative to Python.\nJulia\nThe Julia programming language has caught my eye since 2014. It partially reminded me of MATLAB, that I used for my PhD. Familiarity aside, it‚Äôs a great language to develop with. It‚Äôs fast, interactive, with the best REPL I‚Äôve encountered, highly promising overall especially so after the release of Julia v1.9.\nTo my understanding Julia, being a JIT compiled language, wasn‚Äôt designed for static compilation per se. Community efforts have enabled the generation of compiled packages, with BinaryBuilder, PackageCompiler and StaticCompiler being the most well known compilation tools available at the time of writing. From an intermediate Julia user‚Äôs point of view, I‚Äôve found that compilation results may vary. Also, to the best of my knowledge most compilation tools actually package code rather than statically compile it, which may expose valuable IP. Therefore, I concluded that Julia probably isn‚Äôt as easy to compile as I initially thought (and hoped).\nElixir\nElixir is a fantastic hosted functional language, running on the tried and tested BEAM (Erlang VM). One of the many things Elixir has going for it, is its strong drive towards good documentation. The language‚Äôs REPL is also excellent. All in all, Elixir is rapidly evolving and it‚Äôs worth experimenting with.  \nStarting from 2021, Numerical Elixir (Nx) has progressed by leaps and bounds. The Nx community has managed to produce excellent libraries, with Livebook being the best literate programming environment I‚Äôve ever used. As far as data applications are concerned, Elixir will become a very strong contender, it‚Äôs well worth keeping a close eye on the language.\nAs for cross-compilation, to my understanding Burrito is the only tool that allows for packaged Elixir code to be truly portable albeit producing sizeable executables. Burrito is still WIP, not a guaranteed solution for the time being but a noteworthy tool that‚Äôs improving fast.\nBeing doubtful as to whether this tech stack could meet all my current needs, beside being a niche language in Data and AI, led me to search for another tech stack for fun and profit.\nDeno (TypeScript)\nMore recently, especially given many AI Engineering APIs are written both in Python and TypeScript, I started using Deno. The idea is to leverage Deno for all my computational needs, since it‚Äôs an all-in-one, straightforward to use runtime. Installation and setup were a breeze, Deno comes with all the tools a developer requires (formatter, linter, testing suite, package manager etc.), it plays very nicely with Vim, it‚Äôs lightweight, secure, compatible with NPM packages and the list goes on. Importantly, it can easily cross-compile executables. The data and AI ecosystem is not yet as mature as that of Python. However, if someone is willing to put in the effort, I‚Äôve found that it‚Äôs well worth the investment. This is why I am betting on Deno for my Data Science and AI needs.\n\nWhat About Python Cross-Compilation?\nHow hard could it be? ü§î \nTL;DR: it‚Äôs an involved process that requires access to a CI/CD platform such as GitHub Actions. Once a pipeline is in place, it‚Äôs a straightforward process that requires internet access and registering to a hosting service such as GitHub.\n\nI wrote two pipelines, one for generating a Unix build and one for Windows. The result is pretty decent, however the cumbersome process and reliance on third party tech (GitHub Actions in this case) strengthened my conviction that Deno and TypeScript are worth investing in, for a more complete solution. The JS/DS Data2 ecosystem is not as mature yet but it‚Äôs evolving pretty fast.\n\nConclusions\nThe process of cross-compiling a simple Python app was pretty instructive. The main downside I see is the reliance on a hosting service and a CI/CD platform. Frankly, having access to a hosting service and using CI/CD is almost a given in my line of work. Still, it‚Äôs nowhere near as straightforward as running $ deno compile main.ts\nI am considering attempting the same using Podman, since Windows, macOS and Linux containers are available. Stay tuned for updates!\n\n\n\n  \n    \n      One might argue that running statically compiled executables in a sandboxed environment is a security risk. Static malware analysis tools exist for this exact reason¬†&#8617;\n    \n    \n      To be fair, the Data ecosystem is pretty decent and continuously improving. It‚Äôs the ML and statistical ecosystem and specifically the lack of a native Scikit-learn and Scipy-like packages that‚Äôs still somewhat lacking¬†&#8617;\n    \n  \n\n",
        "url": "/py-cross-compile/",
        "date": "November 11, 2024"
      },
    
      {
        "title": "üí° TIL: 1.58-bit LLMs Match Full Performance @ 98.6% Energy Reduction",
        "content": "\n\nIntroduction\nBack in February 2024, a preprint titled The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits was released. Lots of people picked up on it, simply search for 1.58-bits on YouTube for instance, however it escaped me due to a busy time at work. It was only when I stumbled across this preprint again recently, that I realised what a fantastic idea it is to substitute multiplication with addition or subtraction.\n\nContributions\nThe TL;DR is that all LLM weights can be ternary i.e. {-1, 0, 1}. Ternary weights are 1.58-bits. Activations are 8-bits. This highly quantised model matches full-precision performance at 3B parameter scale.\nThis highly quantised model exhibits 2.71x faster inference, 2.55x lower memory usage at 3B scale, 71.4x lower energy consumption for matrix multiplication operations. Benefits increase with model scale e.g. 4.1x speed-up at 70B parameters, 8.9x higher throughput, 11x larger batch size.\n\nWhat Does 1.58-bits Mean?\nCarnegie Mellon University has a great reference on the basics of Information theory. Learning how to measure information content for a ternary system {-1, 0, 1}, we notice that:\nEach value {-1, 0, 1} has an equal probability $P = \\frac{1}{3}$ for each state. \nThe information content is $-(P \\log_2{P})$ summed over all states\n\n\\[-(\\frac{1}{3} \\log_2(\\frac{1}{3}) + \\frac{1}{3} \\log_2(\\frac{1}{3}) + \\frac{1}{3} \\log_2(\\frac{1}{3}))  \n= -(3 √ó (\\frac{1}{3} \\log_2(\\frac{1}{3})))  \n= -\\log_2(\\frac{1}{3})\n\\approx 1.58496... bits\\]\n\nConclusions\nLLMs can achieve comparable performance to full-precision models while using only three weight values {-1, 0, 1}, achieving up to 71.4x lower energy consumption for matrix operations and 3.55x lower memory usage at 3B scale. This breakthrough suggests a new direction for efficient LLM deployment, particularly promising for edge devices and mobile applications, while also opening opportunities for specialized hardware optimized for 1-bit operations.\n",
        "url": "/TIL-1bitLLM/",
        "date": "October 30, 2024"
      },
    
      {
        "title": "üóÉÔ∏è RAG Is Here To Stay",
        "content": "\n\nIntroduction\nThis morning I noticed that Simon Willison shared some views on RAG, Andryi Burkov criticised people who claim that RAG is obsolete, and other RAG-related discussions taking place sparked by recent longer LLM context windows. Below I‚Äôm sharing some thoughts based on personal experience.\n\nRAG\nRAG is not simply a workaround to context limits, it‚Äôs a way to carefully curate information and data. It enables provenance and visibility of the data flowing through an LLM pipeline -compared to fine-tuning which bakes knowledge into the model itself. Importantly, RAG is not a synonym of embeddings. Embedding text is a fantastic way to enable semantic search, especially if it is done in a smart way (word, sentence, paragraph, or document) given project needs. \nI have successfully reused existing infrastructure to provide one of the largest companies in the world with the ability to quickly retrieve information through Q &amp; A. To achieve this, in the context of simplicity and leveraging existing infrastructure, I opted against adding moving parts like a Vector DB. Instead, I used plain JSON objects and an agentic system to meet the client‚Äôs needs. It worked very well, with feedback from higher management being ‚Äúthank you, this is mind-blowing‚Äù.    \nA nice overview of RAG comes from Jerry Liu‚Äôs interview on Latent Space.\nUpdate: a useful open-source tool for RAGLogging just came out.\n\nU-Shaped Performance\nOne LLM behaviour that should be considered, before regarding RAG obsolete, is their tendency to attend to information from the beginning and end of the context window. See Lost in the Middle: How Language Models Use Long Contexts for an empirical analysis.\nThe paper concludes\n\n  We empirically study how language models use long input contexts via a series of controlled experiments. We show that language model performance degrades significantly when changing the position of relevant information, indicating that models struggle to robustly access and use information in long input contexts. In particular, performance is often lowest when models must use information in the middle of long input contexts.We conduct a preliminary investigation of the role of (i) model architecture, (ii) query-aware contextualization, and (iii) instruction fine-tuning to better understand how they affect how language models use context. Finally, we conclude with a practical case study of open-domain question answering,finding that the performance of language model readers saturates far before retriever recall. Our results and analysis provide a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.\nIn other words, simply dumping loads of text or embeddings into an LLM with a big context window -say 2M tokens- won‚Äôt yield great results. There‚Äôs more to it than brute forcing.\n\n\nConclusions\nExtending context length, as appealing as it may sound, neither simplifies nor solves the issue of creating a good quality AI system that is enriched by large text corpora. It seems that when it comes to larger data volumes, semantic search augmented with Graph search could be a more robust, albeit more involved, approach. Solid prompt engineering approaches, including Chain-of-Thought, Few-shot prompting etc. are also powerful tools to keep in our toolbox.\n",
        "url": "/rag-is-here-to-stay/",
        "date": "October 29, 2024"
      },
    
      {
        "title": "üí° TIL: Useful Nuggets from AI Engineers",
        "content": "\n\nIntroduction\nAs I was going through some CS25: Transformers United V4 lectures from Stanford, I stumbled across some pertinent and useful quotes from guest lecturers.\n\nüìñ\nBest AI skillset\n\n  Best AI skillset in 2018: PhD + long publication record in a specific area\nBest AI skillset in 2023: strong engineering abilities + adapting quickly to new directions without sunk cost fallacy\n\n\nAdvice on choosing a topic\n\n  [‚Ä¶] the project you choose defines the upper-bound for your success.\n\n\nStudy the change itself\n\n  AI is advancing so fast it is hard to keep up. People spend a lot of time and energy catching up with the latest developments. But not enough attention goes to the old things. It is more important to study the change itself\n\n\nWhy I‚Äôm 100% transparent with my manager\n\n  I try to open this [performance] conversation [with my line manager] by asking ‚Äúwhat can I do better‚Äù.\nI tend to use my 1-1s to talk about bigger picture stuff. [‚Ä¶] since that‚Äôs where managers can help the most.\nOf course, all this [honesty with your manager] going well is conditional on working in a healthy company and having a decent manager. [‚Ä¶] do you want to keep working for someone who doesn‚Äôt ask for feedback, or who doesn‚Äôt take your problems seriously?\n\n\nMy strengths are communication and prioritization\n\n  [‚Ä¶] a friend recently asked me what were the best skills I had. [‚Ä¶] I said prioritization and communication. These skills are relatively general but happen to be very important for AI research.\n\n\nMany great managers do IC (Individual Contributor) work\n\n  It seems to be not a coincidence that some of the strongest leaders in AI who manage large teams frequently do very low-level technical work.\n\n\nManually inspect data\n\n  [‚Ä¶] great AI researchers are willing to manually inspect lots of data. And more than that, they build infrastructure that allows them to manually inspect data quickly. Though not glamorous, manually examining data gives valuable intuitions about the problem.\n\n\nRead informal write-ups\n\n  [‚Ä¶] I like to look at the process of how they [great researchers] got there.\n\n\nAdvice from Bryan Johnson\n\n  [‚Ä¶]  having good health enables clear thinking, which is by far the biggest leverage in AI. [‚Ä¶] While it‚Äôs possible to double output by working twice as many hours, choosing a better project has the potential to 10x or even 100x output.\n\n",
        "url": "/TIL-useful-AI-nuggets/",
        "date": "October 25, 2024"
      },
    
      {
        "title": "üîÅ GitHub Actions for yt-dlp-hq",
        "content": "\nWas it worth my time?\n\nIntroduction\nThere are times where I need to use my computer offline, e.g. when I‚Äôm travelling. Having to stay offline is a good opportunity for me to study some lectures of interest, without distractions. For that, I need offline access to the videos I‚Äôm interested in.\nyt-dlp is a great open-source project that allows the user to download audio and/or video from a wide array of platforms including YouTube. Recently, I noticed that it‚Äôs no longer as straightforward to download a video with audio, using yt-dlp. One workaround is to download the audio and video streams separately, and merge them using FFmpeg. This was a good opportunity to write a small automation project in a language I‚Äôm interested in.\n\nyt-dlp And YouTube\nHere‚Äôs an example that motivates implementing this project. Imagine I‚Äôd like to download a video from the excellent IBM Technology YouTube channel, for instance What are AI Agents. Listing the video‚Äôs available formats, returns the following table\n$ yt-dlp -F https://www.youtube.com/watch\\?v\\=F8NKVhkZZWI\n[youtube] Extracting URL: https://www.youtube.com/watch?v=F8NKVhkZZWI\n[youtube] F8NKVhkZZWI: Downloading webpage\n[youtube] F8NKVhkZZWI: Downloading ios player API JSON\n[youtube] F8NKVhkZZWI: Downloading web creator player API JSON\n[youtube] F8NKVhkZZWI: Downloading m3u8 information\n[info] Available formats for F8NKVhkZZWI:\nID      EXT   RESOLUTION FPS CH ‚îÇ   FILESIZE   TBR PROTO ‚îÇ VCODEC          VBR ACODEC      ABR ASR MORE INFO\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nsb3     mhtml 48x27        0    ‚îÇ                  mhtml ‚îÇ images                                  storyboard\nsb2     mhtml 80x45        0    ‚îÇ                  mhtml ‚îÇ images                                  storyboard\nsb1     mhtml 160x90       0    ‚îÇ                  mhtml ‚îÇ images                                  storyboard\nsb0     mhtml 320x180      0    ‚îÇ                  mhtml ‚îÇ images                                  storyboard\n233     mp4   audio only        ‚îÇ                  m3u8  ‚îÇ audio only          unknown             [en] Default\n234     mp4   audio only        ‚îÇ                  m3u8  ‚îÇ audio only          unknown             [en] Default\n139-drc m4a   audio only      2 ‚îÇ    4.35MiB   49k https ‚îÇ audio only          mp4a.40.5   49k 22k [en] low, DRC, m4a_dash\n139     m4a   audio only      2 ‚îÇ    4.35MiB   49k https ‚îÇ audio only          mp4a.40.5   49k 22k [en] low, m4a_dash\n249     webm  audio only      2 ‚îÇ    4.37MiB   49k https ‚îÇ audio only          opus        49k 48k [en] low, webm_dash\n250     webm  audio only      2 ‚îÇ    5.27MiB   59k https ‚îÇ audio only          opus        59k 48k [en] low, webm_dash\n140-drc m4a   audio only      2 ‚îÇ   11.55MiB  129k https ‚îÇ audio only          mp4a.40.2  129k 44k [en] medium, DRC, m4a_dash\n140     m4a   audio only      2 ‚îÇ   11.55MiB  129k https ‚îÇ audio only          mp4a.40.2  129k 44k [en] medium, m4a_dash\n251     webm  audio only      2 ‚îÇ    9.45MiB  106k https ‚îÇ audio only          opus       106k 48k [en] medium, webm_dash\n602     mp4   256x144     15    ‚îÇ ~  7.26MiB   81k m3u8  ‚îÇ vp09.00.10.08   81k video only\n394     mp4   256x144     30    ‚îÇ    4.36MiB   49k https ‚îÇ av01.0.00M.08   49k video only          144p, mp4_dash\n269     mp4   256x144     30    ‚îÇ ~ 11.26MiB  126k m3u8  ‚îÇ avc1.4D400C    126k video only\n160     mp4   256x144     30    ‚îÇ    2.97MiB   33k https ‚îÇ avc1.4D400C     33k video only          144p, mp4_dash\n603     mp4   256x144     30    ‚îÇ ~ 13.63MiB  153k m3u8  ‚îÇ vp09.00.11.08  153k video only\n278     webm  256x144     30    ‚îÇ    7.23MiB   81k https ‚îÇ vp09.00.11.08   81k video only          144p, webm_dash\n395     mp4   426x240     30    ‚îÇ    5.90MiB   66k https ‚îÇ av01.0.00M.08   66k video only          240p, mp4_dash\n229     mp4   426x240     30    ‚îÇ ~ 14.99MiB  168k m3u8  ‚îÇ avc1.4D4015    168k video only\n133     mp4   426x240     30    ‚îÇ    4.49MiB   50k https ‚îÇ avc1.4D4015     50k video only          240p, mp4_dash\n604     mp4   426x240     30    ‚îÇ ~ 21.46MiB  241k m3u8  ‚îÇ vp09.00.20.08  241k video only\n242     webm  426x240     30    ‚îÇ    7.38MiB   83k https ‚îÇ vp09.00.20.08   83k video only          240p, webm_dash\n396     mp4   640x360     30    ‚îÇ   10.27MiB  115k https ‚îÇ av01.0.01M.08  115k video only          360p, mp4_dash\n230     mp4   640x360     30    ‚îÇ ~ 29.86MiB  335k m3u8  ‚îÇ avc1.4D401E    335k video only\n134     mp4   640x360     30    ‚îÇ    7.76MiB   87k https ‚îÇ avc1.4D401E     87k video only          360p, mp4_dash\n18      mp4   640x360     30  2 ‚îÇ   32.38MiB  363k https ‚îÇ avc1.42001E         mp4a.40.2       44k [en] 360p\n605     mp4   640x360     30    ‚îÇ ~ 39.56MiB  444k m3u8  ‚îÇ vp09.00.21.08  444k video only\n243     webm  640x360     30    ‚îÇ   12.52MiB  140k https ‚îÇ vp09.00.21.08  140k video only          360p, webm_dash\n397     mp4   854x480     30    ‚îÇ   17.06MiB  191k https ‚îÇ av01.0.04M.08  191k video only          480p, mp4_dash\n231     mp4   854x480     30    ‚îÇ ~ 37.90MiB  425k m3u8  ‚îÇ avc1.4D401F    425k video only\n135     mp4   854x480     30    ‚îÇ   11.28MiB  126k https ‚îÇ avc1.4D401F    126k video only          480p, mp4_dash\n606     mp4   854x480     30    ‚îÇ ~ 50.26MiB  564k m3u8  ‚îÇ vp09.00.30.08  564k video only\n244     webm  854x480     30    ‚îÇ   17.41MiB  195k https ‚îÇ vp09.00.30.08  195k video only          480p, webm_dash\n398     mp4   1280x720    30    ‚îÇ   31.31MiB  351k https ‚îÇ av01.0.05M.08  351k video only          720p, mp4_dash\n232     mp4   1280x720    30    ‚îÇ ~ 57.85MiB  649k m3u8  ‚îÇ avc1.4D401F    649k video only\n136     mp4   1280x720    30    ‚îÇ   20.16MiB  226k https ‚îÇ avc1.4D401F    226k video only          720p, mp4_dash\n609     mp4   1280x720    30    ‚îÇ ~ 72.26MiB  810k m3u8  ‚îÇ vp09.00.31.08  810k video only\n247     webm  1280x720    30    ‚îÇ   27.69MiB  310k https ‚îÇ vp09.00.31.08  310k video only          720p, webm_dash\n399     mp4   1920x1080   30    ‚îÇ   63.06MiB  707k https ‚îÇ av01.0.08M.08  707k video only          1080p, mp4_dash\n270     mp4   1920x1080   30    ‚îÇ ~193.27MiB 2167k m3u8  ‚îÇ avc1.640028   2167k video only\n137     mp4   1920x1080   30    ‚îÇ   96.17MiB 1078k https ‚îÇ avc1.640028   1078k video only          1080p, mp4_dash\n614     mp4   1920x1080   30    ‚îÇ ~164.68MiB 1847k m3u8  ‚îÇ vp09.00.40.08 1847k video only\n248     webm  1920x1080   30    ‚îÇ   91.43MiB 1025k https ‚îÇ vp09.00.40.08 1025k video only          1080p, webm_dash\n616     mp4   1920x1080   30    ‚îÇ ~322.53MiB 3617k m3u8  ‚îÇ vp09.00.40.08 3617k video only          Premium\n\n\nIt looks like the only ID containing a video with audio, is 18, i.e. a 420p, 640x360 video according to my media player. This might be sufficient for a video like the above, but such low resolution would make it almost impossible to read code or smaller writing.\n\nMy Solution\nGiven I have started leveraging Deno for my needs, I wrote a small tool called yt-dlp-hq. It‚Äôs certainly basic, with lots of room for improvement. However it does exactly what I need and I‚Äôm relatively happy with the result, pending some improvements1.\nDeno is great for cross-compilation. Also, GitHub Actions can be a good method for automating testing, running, compiling etc.2 Is it though? Let‚Äôs see.\n\nMy CI Pipeline\nI started off by using act, a very nice tool that allows for testing pipelines locally. The main downside I found was that for an intermediate Docker user with little act experience, sometimes GitHub Actions don‚Äôt behave the same way locally as they would online. Also, I like podman considerably better, since it‚Äôs daemonless and not as resource-hungry among others.\nPutting act aside, I focused on setting up a pipeline that‚Äôd work well enough with every new PR opened against main aside from others.\nThe pipeline ran successfully, where in theory it built and released yt-dlp-hq executables. However, when I downloaded the corresponding executable for my OS and CPU architecture, it did not run. When I locally built the same set of executables, running deno task build, the executable for my OS &amp; arch worked as expected. This made me wonder whether I‚Äôm doing something wrong, if it‚Äôs a GitHub Action intricacy or some other issue I needed to resolve. \nInspired by Medicine, I tried approaching the issue through differential diagnosis, which to my understanding works by excluding other causes in order to hone in on the actual medical condition. I.e. I first created a release directory locally. I then manually created a release on GitHub. To my dismay, the executable I manually uploaded didn‚Äôt run when I downloaded it back from GitHub. This made me wonder if there is a conversion involved when a pipeline generates executables or the user uploads them manually for release. Spoiler alert: I still don‚Äôt know if that‚Äôs the case, but I suspect that GitHub indeed doesn‚Äôt save executables without some change taking place during upload.\n\nFixing Executables GitHub Release\nInitially, I changed the following setting on my repository: \n‚ÄúSettings -&gt; Actions -&gt; General -&gt; Workflow permissions‚Äù select  ‚ÄúRead and write permissions‚Äù.\nThen, I experimented with compressing each generated executable into a .tar file. This did the trick. Simply compressing an executable is enough to maintain its function. Thus, the way to install yt-dlp-hq takes one extra step.\nFor example, if you‚Äôre a Linux user on an Intel-based machine, here‚Äôs how you can use my tool\n$ curl -L -O https://github.com/ai-mindset/yt-dlp-hq/releases/download/1.0.0/yt-dlp-hq-intel-linux.tar &amp;&amp; tar xvf yt-dlp-hq-intel-linux.tar &amp;&amp; cd release\n$ ./yt-dlp-hq-intel-linux https://www.youtube.com/watch?v=dQw4w9WgXcQ\n\n\nConclusions\nI‚Äôm glad I learned something more about GitHub and Actions, its idiosyncrasies and abilities. It took me a couple days, which made me consider the benefits of automation. Being more minimalistic, I tend to opt for simple automation when possible if it‚Äôs worth it. To quote Alan Perlis, ‚ÄúSimplicity does not precede complexity, but follows it‚Äù.\n\n\n\n  \n    \n      Some improvements I‚Äôm planning include unit testing, automatic audio &amp; video ID selection and possibly automatic FFmpeg installation when it‚Äôs not available in $PATH.¬†&#8617;\n    \n    \n      A Juiia enthusiast introduced me to Woodpecker CI and Codeberg. I‚Äôm definitely considering switching, following my recent GitHub Actions experience ü§î¬†&#8617;\n    \n  \n\n",
        "url": "/gh-actions-ytdlphq/",
        "date": "October 8, 2024"
      },
    
      {
        "title": "üìñ Python To TypeScript Cheatsheet",
        "content": "\n\nIntroduction\nI‚Äôve been curious as to how Python and TypeScript compare at a high level, for someone new to TypeScript. Below is a chatsheet I put together with the help of Claude 3.5 Sonnet. It covers basic syntax, it‚Äôs by no means complete or exhaustive. However it gives a first taste of the similarities and differences between the two languages. The reason I am looking into TypeScript is explained in my Deno article.\n\nVariables And Data Types\n\n\n  \n    \n      Concept\n      Python\n      TypeScript\n    \n  \n  \n    \n      Int\n      x = 5\n      let x: number = 5;\n    \n    \n      Float\n      y = 3.14\n      let y: number = 3.14;\n    \n    \n      Str\n      name = \"John\"\n      let name: string = \"John\";\n    \n    \n      Bool\n      is_valid = True\n      let isValid: boolean = true;\n    \n    \n      List\n      numbers = [1, 2, 3]\n      let numbers: number[] = [1, 2, 3];\n    \n    \n      Dict\n      person = {\"name\": \"John\", \"age\": 30}\n      let person: { name: string; age: number } = { name: \"John\", age: 30 };\n    \n  \n\n\nFunctions\n\n\n  \n    \n      Concept\n      Python\n      TypeScript\n    \n  \n  \n    \n      Func def\n      def greet(name: str) -&gt; str:\n      function greet(name: string): string {\n    \n    \n      Func return\n      return f\"Hello, {name}!\"\n      return `Hello, ${name}!`;\n    \n    \n      Lambda\n      lambda x: x * 2\n      (x: number): number =&gt; x * 2\n    \n  \n\n\nClasses\n\n\n  \n    \n      Concept\n      Python\n      TypeScript\n    \n  \n  \n    \n      Class def\n      class Person:\n      class Person {\n    \n    \n      Constructor\n      def __init__(self, name: str, age: int):\n      constructor(name: string, age: number) {\n    \n    \n      Instance vars\n      self.name = name\n      this.name = name;\n    \n    \n      ¬†\n      self.age = age\n      this.age = age;\n    \n    \n      Method def\n      def greet(self) -&gt; str:\n      greet(): string {\n    \n    \n      Method return\n      return f\"Hello, I'm {self.name}!\"\n      return `Hello, I'm ${this.name}!`;\n    \n  \n\n\nControl Flow\n\n\n  \n    \n      Concept\n      Python\n      TypeScript\n    \n  \n  \n    \n      If\n      if x &gt; 0:\n      if (x &gt; 0) {\n    \n    \n      Else if\n      elif x &lt; 0:\n      } else if (x &lt; 0) {\n    \n    \n      Else\n      else:\n      } else {\n    \n    \n      For loop\n      for i in range(5):\n      for (let i = 0; i &lt; 5; i++) {\n    \n    \n      While loop\n      while x &gt; 0:\n      while (x &gt; 0) {\n    \n  \n\n\nError Handling\n\n\n  \n    \n      Concept\n      Python\n      TypeScript\n    \n  \n  \n    \n      Try\n      try:\n      try {\n    \n    \n      Except/Catch\n      except ZeroDivisionError:\n      } catch (error) {\n    \n    \n      Finally\n      finally:\n      } finally {\n    \n  \n\n",
        "url": "/python-typescript-cheatsheet/",
        "date": "September 6, 2024"
      },
    
      {
        "title": "üèóÔ∏è Modern Data Science and AI Engineering with Deno 2.0",
        "content": "\n\nIntroduction\nThe landscape of Data Science and AI engineering is at a critical inflection point. While Python has dominated data science and machine learning, its fragmented ecosystem and deployment complexities increasingly impede production systems. I‚Äôve already touched on my solution to Python‚Äôs fragmentation. \nDeno 2.0 emerges as a compelling solution to these challenges, bringing together a number of technologies that cover most computing requirements across a very wide range of domains. Having said that, JavaScript (JS) and its superset TypeScript (TS) are far from perfect languages1, but this discussion is outside the scope of this blog post.\n\nThe key factors driving change are:\n\n  Python environment management complexity\n  Production security requirements\n  Deployment workflow friction\n  Need for type safety in large-scale AI applications\n\n\nThe Deno Advantage and Ecosystem\nCore Capabilities\nDeno 2.0 provides a comprehensive, zero-configuration solution with:\n\n  Native TS support\n  First-class security features\n  Cross-compilation through deno compile\n  Built-in development tools\n\n\nAs Ryan Dahl emphasized in a recent Syntax podcast episode: ‚ÄúDeno works really great as a single file. It‚Äôs really great for scripting, [‚Ä¶] you can just put some imports in and start working from a single file. And that is actually exactly what you want from notebooks‚Äù. This aligns with recent work by Alexis Gallagher on single-script Python development.\n\nAI and Data Processing Tools\nThe ecosystem provides direct parallels to Python‚Äôs essential tools:\n\nData Processing:\n\n  nodejs-polars for high-performance DataFrame operations\n  Observable Plot for modern visualisation\n\n\nMachine Learning:\n\n  LangChain.js and LlamaIndex.ts for LLM applications\n  Transformers.js for Hugging Face integration\n  TensorFlow.js and XGBoost-node for ML tasks\n\n\nInfrastructure:\n\n  Native Postgres and MongoDB support\n  Qdrant JS for vector storage\n  STDLib for extended functionality\n  NLP.js and compromise for NLP\n\n\nA Pragmatic Decision Framework\nModern AI systems need to balance rapid experimentation with production-ready stability. Through experimentation, I‚Äôve found that a TS-based approach using Deno provides an elegant solution to both needs.\n\nProduction-First Design\nFor a start, a zero-configuration Deno-based environment makes it easy to produce code spanning proof of concept (POC) to production. This gives the user native security, cross-compilation capabilities and simple single-binary distribution, eliminating many traditional deployment headaches. While Python remains popular for Data Science and AI research, Deno with simple TS2 has been able to handle most of my computational equally well, in a lightweight and productive way.\n\nPractical Implementation Guide\nTransitioning to this kind of all-in-one Deno-driven architecture can start by utilising tools like LangChain.js or LlamaIndex.ts for LLM applications. Data processing can be handled efficiently through nodejs-polars, while Observable Plots provides powerful visualisation.\nEmphasising simplicity, we can use REST/GraphQL to handle service communication, with shared data stores and container-based deployment maintaining clear service boundaries. This approach supports both monolithic and microservice architectures, based on project needs.\n\nDevelopment Best Practices\nIterative refinement development remains an equally productive approach. Strong typing helps with development and code robustness, while correctly used async/await patterns ensure system responsiveness. This approach enables rapid prototyping without sacrificing production readiness.\n\nConclusions\nLeveraging Deno with TS as a replacement for Python is a possible, viable and usually more lightweight alternative for developing more maintainable, secure and production-ready Data and AI systems. Deno‚Äôs zero-config setup, extensive tooling, security focus and stability address key pain points I have encountered in my Python development journey.\nThe Deno ecosystem has reached maturity, making it a viable and often superior alternative -in my experience- to traditional Python-based approaches for modern AI engineering workflows.\n\n\n\n  \n    \n      There are noteworthy -sadly not as widely used- languages such as Clojure and Racket, backed by computer science research, that pioneered concepts like iterative refinement (aka REPL-driven development) among others.¬†&#8617;\n    \n    \n      ‚Äúsimple‚Äù in this context refers to leveraging types but avoiding more involved TypeScript ideas.¬†&#8617;\n    \n  \n\n",
        "url": "/deno/",
        "date": "September 5, 2024"
      }
    
  ]
}
