<!DOCTYPE html>
<html lang="en"><head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>üéõÔ∏è A Practical Guide to Fine-tuning LLMs with InstructLab | Just-in-Time learning</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="üéõÔ∏è A Practical Guide to Fine-tuning LLMs with InstructLab" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Inquisitive. Learning. Sharing. Simplicity = Reliability" />
<meta property="og:description" content="Inquisitive. Learning. Sharing. Simplicity = Reliability" />
<link rel="canonical" href="http://0.0.0.0:4000/instructlab-and-rag/" />
<meta property="og:url" content="http://0.0.0.0:4000/instructlab-and-rag/" />
<meta property="og:site_name" content="Just-in-Time learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-12-19T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="üéõÔ∏è A Practical Guide to Fine-tuning LLMs with InstructLab" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-12-19T00:00:00+00:00","datePublished":"2024-12-19T00:00:00+00:00","description":"Inquisitive. Learning. Sharing. Simplicity = Reliability","headline":"üéõÔ∏è A Practical Guide to Fine-tuning LLMs with InstructLab","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/instructlab-and-rag/"},"url":"http://0.0.0.0:4000/instructlab-and-rag/"}</script>
<!-- End Jekyll SEO tag -->
<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://0.0.0.0:4000/feed.xml" title="Just-in-Time learning" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
delimiters: [
{left: "$$", right: "$$", display: true},
{left: "[%", right: "%]", display: true},
{left: "$", right: "$", display: false}
]}
);
        });
</script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
                if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
                }
                });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>











<link rel="stylesheet" href="/assets/css/style.css">
<script src="/assets/js/theme.js"></script>

</head>
<body><script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js"></script>
<script src="/assets/js/search.js"></script>

<header class="site-header">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/">Just-in-Time learning</a>
    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path
              d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,12.031,18,12.696,18,13.516L18,13.516z" />
          </svg>
        </span>
      </label>

      <div class="trigger">
        <div class="search-container">
          <input type="text" id="search-input" placeholder="Search..." aria-label="Search posts" class="search-input">
          <div id="search-results" class="search-results"></div>
        </div>
        <a class="page-link" href="/aihub">AI Hub</a>
        <a class="page-link" href="/about">About</a>
        <button class="theme-toggle" onclick="toggleTheme()" id="theme-toggle">
  üåô Dark
</button>

<script>
function toggleTheme() {
    const body = document.querySelector('body');
    const button = document.getElementById('theme-toggle');
    const isDark = body.classList.toggle('dark-theme');
    
    button.innerHTML = isDark ? '‚òÄÔ∏è Light' : 'üåô Dark';
    localStorage.setItem('darkTheme', isDark);
}

document.addEventListener('DOMContentLoaded', function() {
    if (localStorage.getItem('darkTheme') === 'true') {
        document.querySelector('body').classList.add('dark-theme');
        document.getElementById('theme-toggle').innerHTML = '‚òÄÔ∏è Light';
    }
});
</script>

      </div>
    </nav>
  </div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">
  <h1>üéõÔ∏è A Practical Guide to Fine-tuning LLMs with InstructLab</h1>

  <div class="post-meta">
    <span class="post-date">Dec 19, 2024</span>
    <span class="reading-time">





  7 minutes read

</span>
  </div>
  
  <!-- Debug info -->
  <!-- <div style="color: red;"> -->
  <!--   TOC enabled: true -->
  <!--   Layout: post -->
  <!-- </div> -->
  
  
    <nav class="toc">
  <div class="toc-header">
    <h2>Contents</h2>
    <button class="toc-toggle" aria-label="Toggle table of contents">
      <svg class="toc-icon" viewBox="0 0 24 24" width="24" height="24">
        <path class="chevron-down" d="M6 9l6 6 6-6"></path>
      </svg>
    </button>
  </div>
  <div class="toc-content">
    <ul>
      
      
        
      
        
          
          <li class="toc-level-2">
            <a href="#introduction">Introduction</a>
            
            
            
              <ul>
                
                  
                
                  
                    
                    <li class="toc-level-3">
                      <a href="#the-fine-tuning-challenge">The Fine-tuning Challenge</a>
                    </li>
                  
                
                  
                    
                    <li class="toc-level-3">
                      <a href="#real-world-challenges">Real-world Challenges</a>
                    </li>
                  
                
                  
                    
                    <li class="toc-level-3">
                      <a href="#the-role-of-instructlab">The Role of InstructLab</a>
                    </li>
                  
                
              </ul>
            
          </li>
        
      
        
          
          <li class="toc-level-2">
            <a href="#from-principles-to-practice">From Principles to Practice</a>
            
            
            
              <ul>
                
                  
                
                  
                    
                    <li class="toc-level-3">
                      <a href="#architectural-components">Architectural Components</a>
                    </li>
                  
                
                  
                    
                    <li class="toc-level-3">
                      <a href="#training-pipelines">Training Pipelines</a>
                    </li>
                  
                
                  
                    
                    <li class="toc-level-3">
                      <a href="#hardware-support-and-quantisation">Hardware Support and Quantisation</a>
                    </li>
                  
                
              </ul>
            
          </li>
        
      
        
          
          <li class="toc-level-2">
            <a href="#practical-workflow">Practical Workflow</a>
            
            
            
              <ul>
                
                  
                
                  
                    
                    <li class="toc-level-3">
                      <a href="#setup-and-installation">Setup and Installation</a>
                    </li>
                  
                
                  
                    
                    <li class="toc-level-3">
                      <a href="#core-workflow-steps">Core Workflow Steps</a>
                    </li>
                  
                
                  
                    
                    <li class="toc-level-3">
                      <a href="#model-deployment">Model Deployment</a>
                    </li>
                  
                
              </ul>
            
          </li>
        
      
        
          
          <li class="toc-level-2">
            <a href="#conclusion">Conclusion</a>
            
            
            
              <ul>
                
                  
                
                  
                    
                    <li class="toc-level-3">
                      <a href="#key-advantages">Key Advantages</a>
                    </li>
                  
                
                  
                    
                    <li class="toc-level-3">
                      <a href="#practical-impact">Practical Impact</a>
                    </li>
                  
                
                  
                    
                    <li class="toc-level-3">
                      <a href="#limitations-and-considerations">Limitations and Considerations</a>
                    </li>
                  
                
                  
                    
                    <li class="toc-level-3">
                      <a href="#rag-vs-fine-tuning">RAG vs Fine-tuning</a>
                    </li>
                  
                
              </ul>
            
          </li>
        
      
    </ul>
  </div>
</nav>

  
  
  <!--more-->

<h2 id="introduction">Introduction</h2>

<p>The explosion of Large Language Models (LLMs) has created a pressing need for domain-specific adaptations. While base models like GPT-4, Claude, and Llama demonstrate impressive general capabilities, organisations often need models that excel in specific domains or exhibit particular behavioural traits. This customisation typically requires fine-tuning, a process that has historically demanded significant expertise, computational resources, and sophisticated infrastructure.</p>

<h3 id="the-fine-tuning-challenge">The Fine-tuning Challenge</h3>

<p>Traditional LLM fine-tuning presents a complex web of interconnected challenges that organisations must navigate. At its core lies the need for sophisticated infrastructure, often requiring specialised hardware and carefully orchestrated software stacks. This infrastructure challenge is compounded by substantial computational costs, making experimentation and iteration expensive.<br />
The data challenge is equally significant. Fine-tuning demands large, high-quality datasets that are both rare and expensive to create. Even when such datasets exist, organisations face the risk of catastrophic forgetting, where models lose their general capabilities while acquiring new ones. Moreover, validating improvements remains a complex task, requiring careful benchmarking and evaluation frameworks.<br />
These challenges have historically restricted fine-tuning to well-resourced organisations, creating a significant barrier to entry for smaller teams and organisations seeking to adapt LLMs to their specific needs.</p>

<h3 id="real-world-challenges">Real-world Challenges</h3>

<p>The adaptation of LLMs to specific domains presents organisations with a multifaceted set of practical challenges. In healthcare, medical institutions grapple with the need for models that can accurately process and generate content using complex medical terminology while maintaining strict clinical protocols. This domain expertise challenge extends beyond mere vocabulary; it encompasses understanding of medical procedures, drug interactions, and diagnostic reasoning.<br />
The financial sector faces equally demanding requirements, particularly around compliance and regulation. Banks and financial institutions must ensure their models operate within specific regulatory frameworks, making decisions that are not only accurate but also auditable and explainable to regulatory bodies.<br />
Data quality emerges as a persistent challenge across sectors. Organisations typically struggle with historical datasets that exhibit inconsistent formatting, missing values, and inherent biases. The challenge extends to maintaining proper version control and data lineage tracking, crucial for both compliance and model improvement cycles.<br />
Regulatory constraints add another layer of complexity. Healthcare organisations must ensure strict HIPAA compliance in their model development and deployment processes. Similarly, any organisation handling European data must adhere to GDPR requirements, while specific industries often face additional certification needs. These regulatory requirements must be considered not just in the final deployment but throughout the entire fine-tuning process.</p>

<h3 id="the-role-of-instructlab">The Role of InstructLab</h3>

<p><a href="https://instructlab.ai/">InstructLab</a> emerges as a systematic solution to these challenges, offering a novel approach to LLM fine-tuning that combines:</p>
<ul>
  <li>Synthetic data generation for high-quality training examples</li>
  <li>Efficient <a href="https://arxiv.org/abs/2305.14314">QLoRA</a>-based training pipelines</li>
  <li>Comprehensive evaluation frameworks</li>
  <li>Hardware-adaptive processing</li>
</ul>

<p>The rest of this article will elaborate on <a href="https://instructlab.ai/">InstructLab</a>‚Äôs architecture, workflow, and practical considerations, demonstrating how it makes LLM fine-tuning accessible while maintaining rigorous quality standards. It will explore how organisations can leverage this tool to enhance their AI capabilities efficiently and systematically.</p>

<h2 id="from-principles-to-practice">From Principles to Practice</h2>

<p><a href="https://instructlab.ai/">InstructLab</a> is built around the LAB (Large-Scale Alignment for ChatBots) methodology, leveraging [QLoRA(https://arxiv.org/abs/2305.14314) (Quantized Low-Rank Adaptation) for efficient fine-tuning. The system requires Python 3.10/3.11 and approximately 500GB of disc space for full operation.</p>

<h3 id="architectural-components">Architectural Components</h3>

<p>The system operates through three primary components:</p>
<ul>
  <li><strong>Taxonomy Repository</strong>: A structured collection of knowledge and skills, organised in YAML files (max 2300 words per Q&amp;A pair)</li>
  <li><strong>Synthetic Data Generator</strong>: Uses a teacher model (default: Mixtral/Mistral instruct for full pipeline, Merlinite 7b for simple) to transform taxonomy entries into diverse training examples</li>
  <li><strong>Training Pipeline System</strong>: <a href="https://arxiv.org/abs/2305.14314">QLoRA</a>-based training options optimised for different hardware configurations</li>
</ul>

<h3 id="training-pipelines">Training Pipelines</h3>

<p><a href="https://instructlab.ai/">InstructLab</a> offers three specialised training pipelines:</p>

<ol>
  <li><strong>Simple Pipeline</strong>
    <ul>
      <li>Fast training (~1 hour)</li>
      <li>Uses SFT Trainer (Linux) or MLX (MacOS)</li>
      <li>Great for initial experiments and validation</li>
    </ul>
  </li>
  <li><strong>Full Pipeline</strong>
    <ul>
      <li>Custom <a href="https://arxiv.org/abs/2305.14314">QLoRA</a> training loop optimised for CPU/MPS</li>
      <li>Enhanced data processing functions</li>
      <li>Memory requirement: 32GB RAM</li>
      <li>Balanced performance and accessibility</li>
    </ul>
  </li>
  <li><strong>Accelerated Pipeline</strong>
    <ul>
      <li>GPU-accelerated distributed <a href="https://arxiv.org/abs/2305.14314">QLoRA</a> training</li>
      <li>Supports NVIDIA CUDA and AMD ROCm</li>
      <li>Requires 18GB+ GPU memory</li>
      <li>Ideal for production-grade fine-tuning</li>
    </ul>
  </li>
</ol>

<h3 id="hardware-support-and-quantisation">Hardware Support and Quantisation</h3>

<p><a href="https://instructlab.ai/">InstructLab</a> supports various hardware configurations with automatic quantisation:</p>
<ul>
  <li>Apple M-series chips: MLX optimisation, MPS acceleration</li>
  <li>NVIDIA GPUs: CUDA support, 4-bit quantisation available</li>
  <li>AMD GPUs: ROCm support, similar quantisation options</li>
  <li>Standard CPUs: Optimised quantisation for memory efficiency</li>
</ul>

<h2 id="practical-workflow">Practical Workflow</h2>

<p>With the architectural foundation established, <a href="https://instructlab.ai/">InstructLab</a> provides a systematic approach to implementing these components through a straightforward command-line interface. The following sections detail the practical steps to leverage this architecture effectively.</p>

<h3 id="setup-and-installation">Setup and Installation</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>instructlab
ilab config init
</code></pre></div></div>

<p>Key requirements:</p>
<ul>
  <li>Python 3.10 or 3.11 (&gt;=3.12 not supported<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>)</li>
  <li>500GB recommended disc space</li>
  <li>16GB RAM minimum, 32GB recommended</li>
</ul>

<h3 id="core-workflow-steps">Core Workflow Steps</h3>

<ol>
  <li><strong>Model Acquisition</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ilab model download
</code></pre></div>    </div>
    <ul>
      <li>Downloads pre-trained base models</li>
      <li>Supports GGUF (4-bit to 16-bit) and Safetensors formats</li>
      <li>Automatic quantisation with configurable parameters</li>
    </ul>
  </li>
  <li><strong>Synthetic Data Generation</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ilab model serve
ilab data generate <span class="nt">--pipeline</span> <span class="o">[</span>simple|full]
</code></pre></div>    </div>
    <p>Common issues and solutions:</p>
    <ul>
      <li>Server conflicts: Use different ports with <code class="language-plaintext highlighter-rouge">--port</code></li>
      <li>Memory errors: Reduce batch size or use <code class="language-plaintext highlighter-rouge">--pipeline simple</code></li>
      <li>Teacher model issues: Verify model checksum and try re-downloading</li>
    </ul>
  </li>
  <li><strong>Training</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ilab model train
</code></pre></div>    </div>
    <p>Hyperparameters (configurable in config.yaml):</p>
    <ul>
      <li>Max epochs: 10</li>
    </ul>
  </li>
  <li><strong>Evaluation</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ilab model evaluate
</code></pre></div>    </div>
    <p>Benchmarks and typical scores:</p>
    <ul>
      <li><a href="http://en.wikipedia.org/wiki/MMLU">MMLU</a>: Knowledge (0.0-1.0 scale)</li>
      <li>MMLUBranch: Delta improvements</li>
      <li>MTBench: Skills (0.0-10.0 scale)</li>
      <li>MTBenchBranch: Skill improvements</li>
    </ul>
  </li>
</ol>

<h3 id="model-deployment">Model Deployment</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ilab model serve <span class="nt">--model-path</span> &lt;new-model-path&gt;
ilab model chat <span class="nt">-m</span> &lt;new-model-path&gt; <span class="c"># Optionally, chat with the model</span>
</code></pre></div></div>
<p>Deployment considerations:</p>
<ul>
  <li>Verify quantisation level matches hardware capabilities</li>
  <li>Monitor memory usage during serving</li>
  <li>Consider temperature settings for inference (default: 1.0)</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p><a href="https://instructlab.ai/">InstructLab</a> represents a significant advancement in democratising LLM fine-tuning, bridging the gap between research capabilities and practical deployment. Through its innovative LAB methodology and <a href="https://arxiv.org/abs/2305.14314">QLoRA</a>-based implementation, it makes sophisticated model adaptation accessible to practitioners across different hardware configurations.</p>

<h3 id="key-advantages">Key Advantages</h3>

<ul>
  <li><strong>Accessibility</strong>: From laptops to data centres, <a href="https://instructlab.ai/">InstructLab</a> scales with available resources</li>
  <li><strong>Flexibility</strong>: Multiple training pipelines accommodate different needs and constraints</li>
  <li><strong>Systematic</strong>: Structured approach to knowledge and skill injection through taxonomy</li>
  <li><strong>Verifiable</strong>: Comprehensive evaluation suite ensures quality of fine-tuned models</li>
</ul>

<h3 id="practical-impact">Practical Impact</h3>

<p><a href="https://instructlab.ai/">InstructLab</a> enables organisations to:</p>
<ul>
  <li>Create domain-specialised models without massive compute resources</li>
  <li>Systematically inject new capabilities through structured knowledge representation</li>
  <li>Validate improvements through quantitative benchmarks</li>
  <li>Deploy fine-tuned models with minimal operational overhead</li>
</ul>

<h3 id="limitations-and-considerations">Limitations and Considerations</h3>

<ul>
  <li><strong>Model Constraints</strong>: Currently supports models up to 7B parameters effectively</li>
  <li><strong>Resource Timeline</strong>: Typical deployment cycle from setup to production:
    <ul>
      <li>Initial setup: a few hours</li>
      <li>Synthetic Data generation: 15 minutes to 1+ hours depending on computing resources</li>
      <li>Training: several hours on consumer hardware</li>
      <li>Evaluation and deployment: a few hours</li>
    </ul>
  </li>
  <li><strong>Maintenance Requirements</strong>:
    <ul>
      <li>Regular model evaluations against new benchmarks</li>
      <li>Periodic retraining with updated taxonomy</li>
      <li>System updates and dependency management</li>
      <li>Storage management for checkpoints and datasets</li>
    </ul>
  </li>
</ul>

<h3 id="rag-vs-fine-tuning">RAG vs Fine-tuning</h3>

<p>It‚Äôs important to recognise that fine-tuning isn‚Äôt always the optimal solution. For dynamic, frequently changing knowledge bases, Retrieval-Augmented Generation (RAG) often provides a more practical and maintainable solution. Fine-tuning through <a href="https://instructlab.ai/">InstructLab</a> is most valuable for:</p>
<ul>
  <li>Stable knowledge domains (e.g., natural sciences, engineering)</li>
  <li>Consistent skill enhancement needs</li>
  <li>Cases where inference latency is critical</li>
</ul>

<p>The system‚Äôs architecture strikes a careful balance between computational efficiency and training effectiveness, making it a practical tool for both experimentation and production use. While not eliminating the complexity of LLM fine-tuning entirely, <a href="https://instructlab.ai/">InstructLab</a> significantly reduces the technical barriers to entry in this crucial domain.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Python version compatibility remains a significant consideration in the ML ecosystem. While newer versions (‚â•3.12) offer improved performance, they often lack compatibility with essential ML frameworks. This constraint informs <a href="https://instructlab.ai/">InstructLab</a>‚Äôs current version requirements.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

</article>

<!-- Theme toggle -->
<script>
// Initialize theme from localStorage
if (localStorage.getItem('darkTheme') === 'true') {
    document.body.classList.add('dark-theme');
}
</script>

<!-- Collapsible TOC -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  const tocHeader = document.querySelector('.toc-header');
  const toc = document.querySelector('.toc');
  
  if (tocHeader && toc) {
    tocHeader.addEventListener('click', function() {
      toc.classList.toggle('collapsed');
      
      // Optional: Save state to localStorage
      localStorage.setItem('tocCollapsed', toc.classList.contains('collapsed'));
    });
    
    // Optional: Restore state from localStorage
    const isCollapsed = localStorage.getItem('tocCollapsed') === 'true';
    if (isCollapsed) {
      toc.classList.add('collapsed');
    }
  }
});
</script>

<div class="post-tags">
  Tags: 
  
    <a href="/">ai</a>, 
  
    <a href="/">llm</a>, 
  
    <a href="/">model-governance</a>, 
  
    <a href="/">production</a>, 
  
    <a href="/">quantisation</a>, 
  
    <a href="/">python</a>, 
  
    <a href="/">mlops</a>, 
  
    <a href="/">best-practices</a>, 
  
    <a href="/">data-science</a>
  
</div>


      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Just-in-Time learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Just-in-Time learning</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/ai-mindset"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">ai-mindset</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Inquisitive. Learning. Sharing. Simplicity = Reliability</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
