[
  {
    "title": "💡 TIL: Incremental AI Problem-Solving with Solveit",
    "date": "2025-08-26T00:00:00.000Z",
    "tags": [
      "til",
      "fast-ai",
      "answer-ai",
      "solveit",
      "ai",
      "best-practices",
      "llm",
      "performance",
      "productivity",
      "prompt-engineering"
    ],
    "url": "/posts/TIL-solveit.html",
    "content": "<p><strong>TL;DR:</strong> Answer.ai&#39;s Solveit approach mitigates LLM deterioration by breaking tasks into small steps, editing AI responses, and providing curated context, addressing three key issues: RLHF-trained overenthusiasm, autoregressive decline, and training data flaws.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>A student from fast.ai&#39;s &quot;<a href=\"http://solveit.fast.ai/\">Solve It With Code</a>&quot; course documented three LLM properties that cause deteriorating AI responses and corresponding mitigation techniques. The course, led by <a href=\"https://nitter.poast.org/jeremyphoward\">Jeremy Howard</a> and <a href=\"https://nitter.poast.org/johnowhitaker\">Johno Whitaker</a>, focuses on transforming problematic AI interactions into learning experiences through systematic problem-solving.</p>\n<p>The approach addresses what the author terms the &quot;deteriorating response pattern&quot; -where AI tools initially appear helpful but produce increasingly broken code through subsequent iterations. The common scenario: Request a weather app from ChatGPT, receive 100 lines of code that doesn&#39;t work, request fixes, encounter additional bugs. This occurs due to fundamental LLM properties, not implementation flaws.</p>\n<h2 id=\"the-three-properties--solutions\">The Three Properties &amp; Solutions</h2>\n<ol>\n<li><em>RLHF creates overly eager helpers</em></li>\n</ol>\n<p>Issue: Human raters prefer complete responses, so models provide overwhelming amounts of information at once\nSolution: Work in small steps, ask clarifying questions first\nBased on Pólya&#39;s problem-solving framework: understand → plan → implement → review</p>\n<ol start=\"2\">\n<li><em>Autoregression leads to deterioration</em></li>\n</ol>\n<p>Issue: Responses degrade over long conversations as models revert to mediocre training patterns\nSolution: Edit the LLM&#39;s responses to shape better patterns, pre-fill outputs, use examples\nThis involves rewriting AI responses to match preferred style, then using those as context for subsequent interactions</p>\n<ol start=\"3\">\n<li><em>Training data is flawed/outdated</em></li>\n</ol>\n<p>Issue: Hallucinations and outdated information from lossy compression of training data\nSolution: &quot;Jeremy RAG&quot; - manually curating relevant context rather than relying on automated retrieval systems\nTools like <a href=\"https://github.com/AnswerDotAI/contextkit\">contextkit</a> enable inclusion of specific documentation, followed by verification that the LLM correctly interprets the provided context</p>\n<h2 id=\"application-to-modern-ai-systems\">Application to Modern AI Systems</h2>\n<p>The methodology remains relevant for reasoning models like Claude Code or OpenAI&#39;s Deep Research. The primary challenge isn&#39;t that models cannot answer questions, but rather that users often don&#39;t know which questions to ask initially.</p>\n<p>Jeremy connected this to Eric Ries&#39; <a href=\"https://theleanstartup.com/principles\">Lean Startup methodology</a>: working in small steps enables adaptation of thinking and refinement of the actual question being posed (paraphrased from the original).</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The Solveit approach transforms problematic AI interactions into learning experiences through iterative, step-by-step problem-solving where each stage builds understanding. By breaking down complex tasks and maintaining control over the conversation flow, users can achieve more reliable results with AI assistants.</p>\n<p><em>Note: Solveit remains unreleased, but these principles apply to existing AI tools.</em></p>\n<p>Do you find that decomposing complex problems into smaller components reveals different requirements than initially anticipated?</p>\n"
  },
  {
    "title": "💡 TIL: Engineering Prompts Double as Human Checklists",
    "date": "2025-07-03T00:00:00.000Z",
    "tags": [
      "ai",
      "best-practices",
      "prompt-engineering",
      "system-prompts",
      "code-quality",
      "productivity",
      "til",
      "debugging"
    ],
    "url": "/posts/TIL-prompts-as-human-checklists.html",
    "content": "<p><strong>TL;DR:</strong> Well-crafted AI system prompts, like those in Microsoft&#39;s VSCode Copilot Chat extension, serve as excellent process documentation and step-by-step checklists that human developers can follow to improve their own workflows and debugging methodologies.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>I was exploring Microsoft&#39;s recently open-sourced <a href=\"https://github.com/microsoft/vscode-copilot-chat/\">VSCode Copilot Chat extension</a> codebase when I noticed something interesting: the prompts that power AI coding assistants make excellent checklists for human developers too.</p>\n<h2 id=\"engineering-prompts-as-process-documentation\">Engineering Prompts as Process Documentation</h2>\n<p>Take this <a href=\"https://github.com/microsoft/vscode-copilot-chat/blob/main/src/extension/prompts/node/agent/agentInstructions.tsx#L197\">agent instruction prompt</a> for example; it&#39;s essentially a 24-step debugging methodology distilled from countless hours of human engineering experience:</p>\n<ol>\n<li>Initialize Git and explore the repository structure</li>\n<li>Create a reproduction script to confirm the issue</li>\n<li>Execute the script to document the exact error</li>\n<li>Analyse the root cause</li>\n<li>Read relevant code blocks before making changes</li>\n<li>Develop comprehensive test cases</li>\n<li>Stage files in Git before editing</li>\n<li>Apply fixes iteratively...</li>\n</ol>\n<p>And so on. Each step represents a best practice that seasoned developers follow instinctively.</p>\n<p>The <a href=\"https://github.com/microsoft/vscode-copilot-chat/blob/40d039d8e08c2d17435a2e65846120c394d0727b/src/extension/xtab/common/promptCrafting.ts#L34\">system prompt template</a> is equally instructive. It emphasises context analysis, consistency, and understanding developer intent before suggesting changes.</p>\n<p>What&#39;s brilliant is that these prompts aren&#39;t just instructions for AI, they&#39;re codified human expertise. When we craft prompts for AI systems, we&#39;re essentially documenting our own thought processes and best practices. The better the prompt, the better the human process it represents.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Next time you&#39;re debugging a tricky issue or refactoring complex code, consider following the same systematic approach these AI prompts encourage. After all, good prompts are just good processes made explicit.</p>\n"
  },
  {
    "title": "🔨 REWORK",
    "date": "2025-03-28T00:00:00.000Z",
    "tags": [
      "37signals",
      "best-practices",
      "productivity",
      "efficiency",
      "company-culture",
      "remote-work",
      "minimal",
      "business-value"
    ],
    "url": "/posts/rework-the-art-of-working-smarter.html",
    "content": "<p><strong>TL;DR:</strong> Basecamp founders challenge conventional business wisdom in their book &quot;Rework,&quot; advocating for simplicity, constraints, sustainable work hours, and focused execution over endless planning, rapid growth, and workaholism - presenting practical principles for building profitable, sustainable businesses with minimal resources.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>The traditional approach to business often involves comprehensive planning, rapid growth, long hours, and complex processes. But is this truly the most effective way to succeed? <a href=\"https://world.hey.com/jason\">Jason Fried</a> and <a href=\"https://world.hey.com/david\">David Heinemeier Hansson</a>, founders of <a href=\"https://en.wikipedia.org/w/index.php?title=Basecamp_(company)&redirect=no\">Basecamp</a> (now <a href=\"https://en.wikipedia.org/wiki/37signals\">37signals</a>), challenge these conventional notions in their influential book &quot;<a href=\"https://basecamp.com/books/rework\">Rework</a>&quot;. Published in 2010, this manifesto presents an alternative philosophy for building successful businesses in the digital age -one that emphasises simplicity, efficiency, and balance. Drawing from their experience creating profitable web applications with a small team, Fried and Hansson offer practical insights for entrepreneurs and companies of all sizes. Their approach advocates working smarter rather than harder, focusing on what truly matters, and challenging business orthodoxy at every turn.</p>\n<h2 id=\"foundational-principles-of-rework\">Foundational Principles of &quot;Rework&quot;</h2>\n<h3 id=\"embrace-simplicity-and-constraints\">Embrace Simplicity and Constraints</h3>\n<p>Fried and Hansson consistently emphasise that constraints aren&#39;t limitations but advantages. With limited resources, you&#39;re forced to focus on what&#39;s essential:  </p>\n<ul>\n<li><strong>Build half a product, not a half-ar🤬ed product</strong>: It&#39;s better to do fewer things exceptionally well than to attempt everything poorly. Quality trumps quantity.</li>\n<li><strong>Embrace constraints</strong>: Limited time, budget, or people can spark creativity and force efficiency. They make you focus on doing more with less.</li>\n<li><strong>Underdo your competition</strong>: Instead of adding more features than competitors, focus on solving core problems elegantly. Simplicity is a competitive advantage.</li>\n</ul>\n<p>The authors point to specific examples, such as how Basecamp launched without billing functionality (adding it 30 days later) and how the Flip video camera succeeded by deliberately omitting features that competitors deemed essential.</p>\n<h3 id=\"challenge-traditional-business-thinking\">Challenge Traditional Business Thinking</h3>\n<p>&quot;Rework&quot; consistently questions business conventions that many take for granted:  </p>\n<ul>\n<li><strong>Planning is guessing</strong>: Detailed long-term business plans are often exercises in fiction. Instead, make decisions just in time with the most current information available.</li>\n<li><strong>Working long hours is counterproductive</strong>: &quot;Workaholism&quot; leads to burnout and mediocre output. Productivity isn&#39;t about hours worked but about focused, quality work.</li>\n<li><strong>Growth isn&#39;t always good</strong>: The authors argue against the obsession with expansion, suggesting companies find their &quot;right size&quot; and focus on sustainability rather than constant growth.</li>\n<li><strong>Skip the &quot;rock stars&quot;</strong>: Instead of obsessing over hiring &quot;ninjas&quot; or &quot;rock stars,&quot; create an environment where ordinary people can do extraordinary work.</li>\n</ul>\n<h3 id=\"focus-on-action-over-discussion\">Focus on Action Over Discussion</h3>\n<p>A central theme in &quot;Rework&quot; is the importance of creating rather than just talking about creating:  </p>\n<ul>\n<li><strong>Start making something</strong>: Ideas are worthless without execution. The world is filled with people who &quot;had that idea first&quot; but never acted on it.</li>\n<li><strong>Launch now</strong>: Perfection is unattainable; get your product out quickly and iterate based on real feedback rather than assumptions.</li>\n<li><strong>Meetings are toxic</strong>: They interrupt productivity, waste collective time, and often accomplish little. Minimise them ruthlessly.</li>\n</ul>\n<p>The authors illustrate this with their own experience building Basecamp, launching quickly with core functionality and improving based on actual customer feedback rather than theoretical market research.</p>\n<h3 id=\"build-an-audience-focused-business\">Build an Audience-Focused Business</h3>\n<p>Fried and Hansson outline a customer-centric approach to business development:  </p>\n<ul>\n<li><strong>Out-teach your competition</strong>: Share knowledge generously through blogs, articles, and tutorials. Teaching establishes authority and builds trust with potential customers.</li>\n<li><strong>Build an audience</strong>: Develop a following of people interested in what you have to say. When you launch products, you&#39;ll already have an engaged audience.</li>\n<li><strong>Emulate chefs</strong>: Like celebrity chefs who share their recipes freely, sharing your expertise doesn&#39;t diminish your business -it enhances it.</li>\n</ul>\n<p>Their company blog, Signal vs. Noise, exemplifies this approach, having built an audience of over 100,000 daily readers who became a natural customer base.</p>\n<h3 id=\"create-a-sustainable-work-culture\">Create a Sustainable Work Culture</h3>\n<p>The authors advocate for work environments that prioritise sustainability over burnout:  </p>\n<ul>\n<li><strong>Send people home at 5</strong>: Reasonable working hours increase per-hour productivity and lead to more creative solutions.</li>\n<li><strong>Avoid policies that treat people like children</strong>: Trust adults to manage their time and make good decisions.</li>\n<li><strong>Avoid unnecessary formality</strong>: Communicate in a human voice rather than corporate-speak. Sound like yourself, not like a faceless entity.</li>\n</ul>\n<h2 id=\"potential-limitations\">Potential Limitations</h2>\n<p>While &quot;Rework&quot; offers valuable counter-conventional wisdom, some of its approaches may not suit all business contexts. The authors&#39; philosophy works particularly well for software and service businesses with low overhead, but manufacturing or capital-intensive industries may require more traditional planning. Additionally, their &quot;do less&quot; approach might not always scale for businesses with complex regulatory requirements or those serving enterprise clients with extensive needs.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>&quot;Rework&quot; offers a refreshing alternative to conventional business wisdom, advocating for a more thoughtful, balanced, and human approach to work. The book&#39;s central message is that success doesn&#39;t require sixty-hour workweeks, venture capital, or extensive planning -it requires focus on what matters, elimination of what doesn&#39;t, and dedication to quality execution.<br>By challenging assumptions about growth, working hours, planning, and hiring, Fried and Hansson present a blueprint for building businesses that are not only profitable but also sustainable and enjoyable to run. Their philosophy can be distilled to a few key principles: embrace constraints, focus on quality over quantity, prioritise action over planning, and build businesses that respect both customers and employees.<br>Whether you&#39;re running a startup, managing a team, or simply looking to work more effectively, &quot;Rework&quot; provides valuable insights for doing more with less and building something that lasts. It&#39;s not about working more -it&#39;s about working smarter.</p>\n"
  },
  {
    "title": "🌐 Remote: Office Not Required",
    "date": "2025-03-26T00:00:00.000Z",
    "tags": [
      "37signals",
      "remote-work",
      "advantage",
      "company-culture",
      "productivity",
      "best-practices",
      "decision-making",
      "onboarding"
    ],
    "url": "/posts/remote-office-not-required.html",
    "content": "<p><strong>TL;DR:</strong> Basecamp founders present a comprehensive guide to remote work, arguing that distributed teams offer increased productivity, access to global talent, and better work-life balance whilst outlining practical strategies for effective communication, maintaining culture, and overcoming common objections to remote collaboration.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>In the rapidly evolving landscape of modern business, few changes have been as transformative as the shift toward remote work. In their insightful book &quot;<a href=\"https://basecamp.com/books#remote\">Remote: Office Not Required</a>&quot;, <a href=\"https://world.hey.com/jason\">Jason Fried</a> and <a href=\"https://world.hey.com/david\">David Heinemeier Hansson</a> of <a href=\"https://en.wikipedia.org/w/index.php?title=Basecamp_(company)&redirect=no\">Basecamp</a> (now <a href=\"https://en.wikipedia.org/wiki/37signals\">37signals</a>) present a compelling case for why working remotely isn&#39;t just a viable option - it&#39;s often superior to the traditional office-based model.<br>Written in 2013, well before the pandemic forced companies to adopt remote work en masse, the book now seems prophetic. Fried and Hansson argue that remote work is not a fleeting trend but an inevitable evolution in how businesses operate. The authors, who built their successful software company with team members scattered across the globe, offer practical advice for implementing and thriving in a remote work environment.<br>Let&#39;s explore the core principles of &quot;Remote&quot; that can help businesses and workers navigate this new reality.</p>\n<h2 id=\"why-remote-work-makes-sense\">Why Remote Work Makes Sense</h2>\n<h3 id=\"the-office-paradox\">The Office Paradox</h3>\n<p>One of the book&#39;s most compelling arguments is that traditional offices often hinder productivity rather than enhance it. Fried and Hansson point out that when people need to get serious work done, they rarely cite the office as their preferred location. Instead, they choose early mornings, late evenings, or weekends - times when interruptions are minimal.<br>Offices have become &quot;interruption factories&quot; where meaningful work is chopped into small, ineffective chunks. Meetings, impromptu desk visits, and constant noise create an environment where deep, focused work becomes nearly impossible. Remote work, by contrast, allows people to create their own distraction-free environments.</p>\n<h3 id=\"the-end-of-commuting\">The End of Commuting</h3>\n<p>Another significant advantage of remote work is eliminating the commute. Beyond the obvious time savings, research shows that long commutes correlate with increased obesity, stress, neck and back pain, and even higher divorce rates. The authors calculate that an average commute consumes 300-400 hours per year - time that could be redirected toward productive work or personal well-being.</p>\n<h3 id=\"access-to-global-talent\">Access to Global Talent</h3>\n<p>Perhaps most importantly, remote work dramatically expands the talent pool. Instead of limiting hiring to a specific geographical area, companies can recruit from anywhere in the world. This not only increases the chances of finding exceptional talent but also naturally leads to a more diverse workforce with varied perspectives.</p>\n<h2 id=\"making-remote-work-work\">Making Remote Work Work</h2>\n<h3 id=\"communication-the-key-to-success\">Communication: The Key to Success</h3>\n<p>Effective remote work hinges on communication. The book advocates for a blend of synchronous and asynchronous communication methods:</p>\n<ol>\n<li><strong>Overlap Time</strong>: Ensure team members have at least 4 hours of overlap in their workdays to allow for real-time collaboration when needed.</li>\n<li><strong>Screen Sharing</strong>: Use tools like WebEx, GoToMeeting, or Join.me to collaborate visually, making it feel more like sitting side-by-side.</li>\n<li><strong>Transparent Documentation</strong>: Make information accessible to everyone regardless of time zone, eliminating bottlenecks. Basecamp, their project management tool, was designed specifically with this in mind.</li>\n<li><strong>Virtual Water Cooler</strong>: Create spaces for casual conversation to maintain company culture and social connections. The authors used Campfire, their web-based chat service, for this purpose. While Campfire was discontinued as a standalone product, it was recently relaunched in 2024 as part of their ONCE line - allowing users to purchase and self-host the software on their own servers rather than subscribing to a SaaS model.</li>\n</ol>\n<h3 id=\"navigating-legal-and-financial-considerations\">Navigating Legal and Financial Considerations</h3>\n<p>The book doesn&#39;t shy away from the practical challenges of remote work. In the chapter &quot;Taxes, accounting, laws, oh my!&quot; the authors tackle the nuts and bolts of remote employment:  </p>\n<ul>\n<li><strong>Domestic remote work</strong> is relatively straightforward from a legal standpoint, with few complications beyond potential state tax implications if employees work across state lines.</li>\n<li><strong>International remote work</strong> presents more challenges. The authors outline two main approaches: establishing a local office (expensive but comprehensive) or hiring people as contractors (simpler but with limitations on benefits and employment protections).</li>\n<li><strong>For remote workers</strong>, they recommend setting up a personal company and billing as a contractor if working for an international company, though they acknowledge this isn&#39;t a perfect solution.</li>\n</ul>\n<p>The authors are refreshingly honest here, acknowledging that running with a less-than-perfect legal setup is common practice - though they recommend consulting professionals for complex situations.</p>\n<h3 id=\"overcoming-common-objections\">Overcoming Common Objections</h3>\n<p>Fried and Hansson systematically address the objections typically raised against remote work:</p>\n<ul>\n<li><strong>&quot;How do I know people are working?&quot;</strong> If you can&#39;t trust employees to work remotely, the issue is hiring, not location.</li>\n<li><strong>&quot;What about security?&quot;</strong> With proper protocols, remote work can be just as secure as office work.</li>\n<li><strong>&quot;We need face-to-face meetings.&quot;</strong> Most meetings can be conducted effectively online, and occasional in-person gatherings can satisfy the need for face time.</li>\n<li><strong>&quot;We need to maintain our culture.&quot;</strong> Culture stems from values and actions, not physical proximity.</li>\n</ul>\n<h3 id=\"avoiding-remote-work-pitfalls\">Avoiding Remote Work Pitfalls</h3>\n<p>The book doesn&#39;t gloss over remote work&#39;s challenges:</p>\n<ol>\n<li><strong>Isolation</strong>: Combat loneliness by encouraging employees to work from co-working spaces or cafés occasionally.</li>\n<li><strong>Overwork</strong>: Without clear boundaries, remote workers may struggle to disconnect. Managers should focus on results rather than hours worked and look out for signs of burnout.</li>\n<li><strong>Communication Barriers</strong>: When face-to-face interaction is limited, misunderstandings can occur. Clear, thoughtful communication becomes even more crucial.</li>\n</ol>\n<h2 id=\"building-and-managing-a-remote-team\">Building and Managing a Remote Team</h2>\n<h3 id=\"hiring-for-remote-work\">Hiring for Remote Work</h3>\n<p>The authors emphasise that great remote workers possess certain qualities:  </p>\n<ul>\n<li><strong>Self-motivation</strong>: They can stay productive without direct supervision.</li>\n<li><strong>Strong writing skills</strong>: Since much of remote communication is written, clear writing is essential.</li>\n<li><strong>Results-oriented mindset</strong>: They focus on output rather than hours at a desk.</li>\n</ul>\n<h3 id=\"creating-trust-and-accountability\">Creating Trust and Accountability</h3>\n<p>Rather than micromanaging, successful remote teams are built on trust. The book recommends:  </p>\n<ul>\n<li><strong>Focus on outputs</strong>: Judge work by what&#39;s accomplished, not when or how it&#39;s done.</li>\n<li><strong>Regular check-ins</strong>: Brief one-on-ones help maintain connection without becoming burdensome.</li>\n<li><strong>Eliminate roadblocks</strong>: Ensure remote workers have the authority and access they need to be effective.</li>\n</ul>\n<h3 id=\"the-remote-toolbox\">The Remote Toolbox</h3>\n<p>The authors provide a practical &quot;Remote Toolbox&quot; with specific recommendations:  </p>\n<ul>\n<li><strong>Basecamp</strong>: Their own project management tool for organising tasks, discussions, and files in one central location.</li>\n<li><strong>Video conferencing tools</strong>: Google Hangouts (now Google Meet) for group video calls with up to 10 people.</li>\n<li><strong>Screen sharing</strong>: WebEx, GoToMeeting, and Join.me for collaboration and demonstrations.</li>\n<li><strong>File sharing</strong>: Dropbox for keeping files synchronised across multiple devices and locations.</li>\n<li><strong>Collaborative documents</strong>: Google Docs for real-time collaboration on text documents and spreadsheets.</li>\n<li><strong>Co-working directories</strong>: Resources like Regus, LiquidSpace, Desktime, and the Coworking Wiki to find workspaces while travelling or to escape home office isolation.</li>\n</ul>\n<p>Many of these tools have evolved since the book&#39;s publication, but the core functions they serve remain essential to remote work.</p>\n<h3 id=\"the-importance-of-meetups\">The Importance of Meetups</h3>\n<p>Despite advocating for remote work, the authors stress the value of occasional in-person gatherings. At their company, they met at least twice yearly for 4-5 days[^1]. These meetups strengthen personal bonds, allow for intensive collaboration, and reinforce company culture.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>&quot;Remote: Office Not Required&quot; provides a comprehensive blueprint for implementing successful remote work practices. The authors convincingly argue that remote work offers numerous advantages: increased productivity, access to global talent, better work-life balance, and reduced overhead costs.<br>What makes this book particularly valuable is its grounding in real-world experience. Fried and Hansson have built their business on these principles and have navigated the challenges they describe.<br>As we continue to redefine what work means in the 21st century, the insights from &quot;Remote&quot; remain highly relevant. The authors envision a future where work is judged by results rather than location, where talent knows no geographical boundaries, and where both companies and employees enjoy greater freedom and flexibility.<br>For businesses looking to thrive in this new landscape, &quot;Remote&quot; offers not just philosophy but practical strategies for turning the challenges of distributed work into competitive advantages. The future of work is indeed remote - and this book provides an excellent road map for that journey.</p>\n<hr>\n<p>[^1]: Reasonable adjustments must be considered for those who are not able to travel far due to health, family or other reasons beyond their control. It is possible to build and maintain a strong culture that does not necessitate travelling, or at least not travelling often or far, if circumstances don&#39;t allow. </p>\n"
  },
  {
    "title": "😌 It Doesn't Have to Be Crazy at Work",
    "date": "2025-03-23T00:00:00.000Z",
    "tags": [
      "37signals",
      "advantage",
      "best-practices",
      "decision-making",
      "business-value",
      "slow-down",
      "onboarding",
      "remote-work",
      "productivity",
      "company-culture"
    ],
    "url": "/posts/it-doesnt-have-to-be-crazy-at-work-37-signals.html",
    "content": "<p><strong>TL;DR:</strong> Basecamp founders reject the &quot;crazy busy&quot; workplace culture, advocating instead for a &quot;calm company&quot; approach that emphasises reasonable 40-hour workweeks, focused attention, asynchronous communication, and flexible project scope - proving that sustainable work practices can yield successful businesses without sacrificing employee wellbeing.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>In today&#39;s hyperactive business environment, &quot;crazy busy&quot; has become a badge of honour. Endless workweeks, constant interruptions, and the expectation of instant responses have created workplaces where stress is the norm and burnout is inevitable. But does it have to be this way? <a href=\"https://world.hey.com/jason\">Jason Fried</a> and <a href=\"https://world.hey.com/david\">David Heinemeier Hansson</a>, the founders of <a href=\"https://en.wikipedia.org/w/index.php?title=Basecamp_(company)&redirect=no\">Basecamp</a> (renamed to <a href=\"https://en.wikipedia.org/wiki/37signals\">37signals</a> since 2014), argue emphatically that it doesn&#39;t. In their book &quot;<a href=\"https://basecamp.com/books/calm\">It Doesn&#39;t Have to Be Crazy at Work</a>&quot; they present a compelling case for a calmer, more sustainable approach to work -one where companies can still be successful without sacrificing the well-being of their employees.<br>The authors, who have built a profitable business with minimal stress and reasonable working hours, dismiss the idea that growth-at-all-costs and around-the-clock work schedules are necessary for success. Instead, they advocate for what they call a &quot;calm company&quot; -an organisation that values sustainable workloads, reasonable expectations, sufficient rest, and focused productivity. Let&#39;s dive into the key principles that can help transform a frantic workplace into a calm and productive environment.</p>\n<h2 id=\"the-calm-company-philosophy\">The Calm Company Philosophy</h2>\n<h3 id=\"rethinking-time-and-attention\">Rethinking Time and Attention</h3>\n<p>One of the core insights from the book is that modern workplaces have become &quot;interruption factories&quot; where meaningful work is nearly impossible. Offices chop the workday into tiny fragments -fifteen minutes here, ten minutes there- with meetings, calls, and constant distractions preventing sustained focus.<br>The authors argue that 8-hour workdays and 40-hour workweeks are plenty of time to accomplish great work, provided that time is actually protected. Instead of measuring commitment by hours spent at a desk, a calm company measures results and respects boundaries. Basecamp&#39;s philosophy is straightforward: &quot;Work 40 hours a week, then stop. No all-nighters, no weekends&quot;.<br>To protect time, the book advocates for asynchronous communication whenever possible. Not everything requires an immediate response. By promoting a culture of eventual response rather than instant reaction, companies give employees the space for deep, focused work. This might mean designating &quot;library rules&quot; in the office -quiet, focused concentration- and setting clear boundaries for when real-time collaboration is truly necessary.</p>\n<h3 id=\"eliminate-excessive-mms-meetings-and-managers\">Eliminate Excessive M&amp;Ms: Meetings and Managers</h3>\n<p>Meetings and micromanagement are two primary culprits behind workplace chaos. The authors are particularly critical of the modern meeting culture, noting that &quot;a one-hour meeting with ten people isn&#39;t a one-hour meeting -it&#39;s a ten-hour meeting&quot;. Before calling a meeting, they suggest asking whether it&#39;s truly worth pulling multiple people away from their focused work.<br>Similarly, the book challenges managers to stop &quot;managing the chairs&quot; (monitoring when people arrive and leave) and instead focus on managing the work itself. This means setting clear expectations, providing necessary resources, removing obstacles, and then trusting people to execute without constant supervision.<br>At Basecamp, they&#39;ve institutionalised practices like &quot;office hours&quot; for experts, where rather than being constantly available for interruption, they designate specific times when they&#39;re available for questions. They&#39;ve also moved away from real-time chat for important discussions, recognising that this medium often creates an unhealthy expectation of immediate response.</p>\n<h3 id=\"reasonable-expectations-and-focused-scope\">Reasonable Expectations and Focused Scope</h3>\n<p>Perhaps the most radical departure from conventional business thinking is the authors&#39; approach to goals and expectations. They proudly declare: &quot;We don&#39;t do goals at Basecamp&quot;. Instead of chasing arbitrary targets, they focus on doing excellent work consistently and sustainably.<br>The book introduces the concept of &quot;dreadlines&quot; versus deadlines. A dreadline appears when a deadline is paired with an ever-expanding scope. To combat this, Basecamp keeps deadlines fixed but makes scope flexible. Projects can only get smaller over time, not larger, ensuring teams can deliver quality work without burning out.<br>This means being deliberate about what not to do. As the authors put it: &quot;Having less to do isn&#39;t a problem, it&#39;s an advantage&quot;. They suggest developing the skill of &quot;narrowing as you go&quot; -starting projects with exploration, then gradually focusing in on what&#39;s truly important as you approach the deadline.<br>Basecamp also embraces the &quot;disagree and commit&quot; approach to decision-making. Rather than requiring consensus, which can lead to endless debate, someone makes the final call after everyone has been heard -and then the whole team commits to making it work, even if some initially disagreed.</p>\n<h3 id=\"building-a-healthy-remote-work-culture\">Building a Healthy Remote Work Culture</h3>\n<p>Remote work features prominently in Basecamp&#39;s approach to building a calm company. By removing the expectation that everyone must be in the same physical space, they&#39;ve created more flexibility while maintaining productivity.<br>However, they emphasise that remote work requires intentionality. Teams need sufficient overlap in working hours, clear communication practices, and strong writing skills. In fact, the authors consider good writing essential for remote teams, as it eliminates ambiguity and creates a clear record of decisions and rationales.<br>The authors also address the concern that remote work might lead to isolation or disconnection. They recommend regular in-person meetups[^1] and maintaining a strong company culture based on shared values and respect, not forced socialisation or perks designed to keep people at the office longer.</p>\n<h3 id=\"hiring-and-benefits-that-support-life-outside-work\">Hiring and Benefits That Support Life Outside Work</h3>\n<p>Basecamp&#39;s approach to hiring focuses on finding talented people who value calm productivity over chaotic hustle. Their compensation philosophy is refreshingly straightforward: equal pay for equal work, regardless of location, with no complex negotiation processes.<br>Their benefits are specifically designed to encourage life beyond work. Rather than offering free meals to keep employees in the office longer, they provide benefits that help people disconnect -like paid sabbaticals, summer hours (32-hour workweeks from May through August), and even covering the cost of employees&#39; vacations. This reinforces their belief that the best workers are well-rested ones with rich lives outside the office.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>&quot;It Doesn&#39;t Have to Be Crazy at Work&quot; presents a refreshing alternative to the burnout culture that pervades much of today&#39;s business world. The calm company model isn&#39;t about doing less or lowering standards -it&#39;s about working smarter, focusing on what truly matters, and creating sustainable conditions where people can do their best work without sacrificing their health and happiness.</p>\n<p>The key takeaways from the book include:</p>\n<ol>\n<li>Protect people&#39;s time and attention by eliminating unnecessary interruptions</li>\n<li>Stick to reasonable work hours (40 hours per week is plenty)</li>\n<li>Replace constant meetings with more thoughtful, asynchronous communication</li>\n<li>Focus on the quality of work rather than hours logged</li>\n<li>Keep deadlines fixed but be flexible about scope</li>\n<li>Build a culture of trust where remote work can thrive</li>\n<li>Be intentional about what you say no to</li>\n</ol>\n<p>As the authors suggest, &quot;calm is contagious&quot; -and so is crazy. By choosing calm, companies can create environments where employees thrive, creativity flourishes, and sustainable success becomes possible. The choice, as they say, is yours.</p>\n<hr>\n<p>[^1]: Reasonable adjustments must be considered for those who are not able to travel far due to health, family or other reasons beyond their control. It is possible to build and maintain a strong culture that does not necessitate travelling, or at least not travelling often or far, if circumstances don&#39;t allow. </p>\n"
  },
  {
    "title": "⏪ Making Data Transformations Reversible with fasttransform",
    "date": "2025-03-22T00:00:00.000Z",
    "tags": [
      "machine-learning",
      "data-processing",
      "fast-ai",
      "python",
      "data-science",
      "optimisation",
      "best-practices",
      "interpretability"
    ],
    "url": "/posts/fasttransform-for-reversible-data-transformations.html",
    "content": "<p><strong>TL;DR:</strong> Fast.ai&#39;s fasttransform library makes machine learning data pipelines reversible by pairing each transformation with its inverse, enabling visualisation of transformed data for debugging and utilising multiple dispatch to handle different data types intelligently - crucial for understanding model behaviour and identifying spurious correlations.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Machine learning practitioners face a common problem: after applying multiple transformations to prepare data for training, it becomes difficult to visualise what the model actually sees. This visualisation gap makes debugging challenging and often leads to missing critical insights about model behaviour.<br>For example, consider a model built to distinguish wolves from huskies that performs poorly on certain images. Without the ability to easily inspect how transformations affect the input data, one might miss that the model is actually detecting snow (common in wolf photos) rather than the animals themselves.<br>Fast.ai&#39;s solution to this problem is <a href=\"https://github.com/AnswerDotAI/fasttransform\">fasttransform</a>[^1], a library that ensures any transformation applied to data can be easily reversed. Let&#39;s explore how it works and why it matters.</p>\n<h2 id=\"reversible-pipelines-made-simple\">Reversible Pipelines Made Simple</h2>\n<h3 id=\"the-problem-with-one-way-transforms\">The Problem with One-Way Transforms</h3>\n<p>Traditional data transformation pipelines in libraries like PyTorch are one-way streets. Consider this simple example of normalising an image:</p>\n<pre><code class=\"language-python\">from torchvision import transforms as T\ntransforms_pt = T.Compose([\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize(*imagenet_stats)\n])\n\nimg = Image.open(&quot;husky.jpeg&quot;)\nimg_transformed = transforms_pt(img)\n</code></pre>\n<p>Attempting to visualise <code>img_transformed</code> results in a mess of pixel values outside the displayable range. To see what the model sees, one needs to manually write an inverse transform function:</p>\n<pre><code class=\"language-python\">def decode_pt(tensor, mean, std):\n    out = tensor.clone()\n    for t, m, s in zip(out, mean, std): t.mul_(s).add_(m)\n    out = out.mul(255).clamp(0, 255).byte()\n    return out\n</code></pre>\n<p>This is tedious and error-prone, especially as your transformation pipeline grows more complex.</p>\n<h3 id=\"an-elegant-solution\">An Elegant Solution</h3>\n<p>fasttransform takes a fundamentally different approach by pairing each transformation with its inverse. Here&#39;s the same pipeline using fasttransform:</p>\n<pre><code class=\"language-python\">from fastai.vision.all import *\n\ntransforms_ft = Pipeline([\n   PILImage.create,\n   Resize(256, method=&quot;squish&quot;),\n   Resize(224, method=&quot;crop&quot;),\n   ToTensor(),\n   IntToFloatTensor(),\n   Normalize.from_stats(*imagenet_stats)\n])\n\n# Transform our image\nfpath = Path(&quot;./huskies_vs_wolves/train/husky/husky_0.jpeg&quot;)\nimg_transformed = transforms_ft(fpath)\n# To reverse the transformations:\nimg_decoded = transforms_ft.decode(img_transformed)\n</code></pre>\n<p>The magic lies in how each transform defines both forward and reverse operations:</p>\n<pre><code class=\"language-python\">class Normalize(Transform):\n    def __init__(self, mean=None, std=None):\n        self.mean = mean\n        self.std = std\n        \n    def encodes(self, x): return (x-self.mean) / self.std  # forward transform\n    def decodes(self, x): return x*self.std + self.mean    # inverse transform\n</code></pre>\n<p>By defining both <code>encodes</code> and <code>decodes</code> methods, fasttransform automatically knows how to reverse your transformations. This is particularly valuable when working with fast.ai v2, where this kind of visualisation capability is built directly into core functions like <code>show_batch</code> and <code>show_results</code>.</p>\n<h3 id=\"multiple-dispatch-the-secret-sauce\">Multiple Dispatch: The Secret Sauce</h3>\n<p>Another powerful feature of fasttransform is how it handles different types of data. Using a concept called <a href=\"https://www.youtube.com/watch?v=kc9HwsxE1OY\">multiple dispatch</a>[^2], transformations can apply differently based on the type of data they receive.</p>\n<p>This becomes particularly valuable when dealing with images and their labels, allowing a single pipeline to handle both:</p>\n<pre><code class=\"language-python\"># Function that loads both image and its label\ndef load_img_and_label(fp): return PILImage.create(fp), parent_label(fp)\n\ntransforms_ft = Pipeline([\n   load_img_and_label,  # Loads both image and label as a tuple\n   Resize(256, method=&quot;squish&quot;),\n   Resize(224, method=&quot;crop&quot;),\n   ToTensor(),\n   IntToFloatTensor(),\n   Normalize.from_stats(*imagenet_stats)\n])\n</code></pre>\n<p>The pipeline intelligently applies each transform only to the appropriate data types, eliminating the need for separate transformation pipelines.</p>\n<h3 id=\"connections-to-julias-multiple-dispatch\">Connections to Julia&#39;s Multiple Dispatch</h3>\n<p>Interestingly, the concept of multiple dispatch that fasttransform leverages is a core feature of the Julia programming language. In Julia, which method of a function gets called depends on the types of all arguments, not just the first one (as in traditional object-oriented programming).<br>As explained in Julia&#39;s documentation: &quot;<em>Using all of a function&#39;s arguments to choose which method should be invoked, rather than just the first, is known as multiple dispatch. Multiple dispatch is particularly useful for mathematical code, where it makes little sense to artificially deem the operations to &#39;belong&#39; to one argument more than any of the others</em>&quot;.<br>The connection to Julia is particularly illuminating, as it demonstrates how concepts from one language can inspire powerful design patterns in another. Just as Julia&#39;s multiple dispatch enables elegant mathematical code, fasttransform&#39;s implementation of this concept allows for cleaner, more intuitive data pipelines in Python.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>fasttransform represents a significant step forward in making machine learning workflows more intuitive and debugging more accessible. By making transformations reversible through paired encode/decode methods and leveraging multiple dispatch to handle different data types intelligently, it solves two fundamental problems in data processing pipelines: the inability to easily reverse transformations to inspect data, and the need for separate transformation pipelines for different types of data.<br>The ability to easily visualise transformed data isn&#39;t just convenient -it&#39;s essential for understanding model behaviour and catching issues like the wolf/husky example, where models learn spurious correlations rather than intended features.<br>As machine learning systems grow more complex, tools like fasttransform that improve transparency and the ability to debug become increasingly valuable. Whether working with images, text, time series, or other data types, being able to see what a model sees provides critical insights that might otherwise be missed.<br>Returning to our wolf/husky example, the ability to easily visualise transformed data allows researchers to immediately identify that their model is learning to detect snow backgrounds rather than animal features -a crucial insight for building more robust models.<br>Those interested in trying fasttransform can install it with <code>pip install fasttransform</code> and check out the <a href=\"https://github.com/AnswerDotAI/fasttransform\">official fasttransform documentation</a> for more examples and detailed API references. The library offers these capabilities with minimal performance overhead, as the paired transformation approach adds negligible computational cost while providing significant benefits for debugging and understanding model behaviour.</p>\n<hr>\n<p>[^1]: Rens Dimmendaal, Hamel Husain, &amp; Jeremy Howard. &quot;<a href=\"https://www.fast.ai/posts/2025-02-20-fasttransform.html\">fasttransform: Reversible Pipelines Made Simple</a>&quot; fast.ai blog, February 20, 2025.\n[^2]: &quot;<a href=\"https://docs.julialang.org/en/v1/manual/methods/\">Methods · The Julia Language</a>&quot; Julia Documentation, docs.julialang.org.</p>\n"
  },
  {
    "title": "💡 TIL: A Reactive Python Notebook That Might Replace Jupyter",
    "date": "2025-03-22T00:00:00.000Z",
    "tags": [
      "til",
      "data-science",
      "python",
      "best-practices",
      "reproducibility",
      "literate-programming",
      "jupyter-alternative",
      "reactivity"
    ],
    "url": "/posts/git-friendly-literate-programming-with-marimo.html",
    "content": "<p><strong>TL;DR:</strong> Marimo offers a reactive Python notebook alternative to Jupyter that solves hidden state problems by storing notebooks as pure Python files with deterministic execution based on variable dependencies, making them git-friendly, reproducible, and deployable whilst providing modern features like Vim keybindings and interactive elements.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>As a long-time Vim/Neovim and IPython user, I&#39;m quite particular about my development environment. So when I say a notebook platform caught my attention enough to consider switching, that&#39;s significant. Recently, I stumbled upon <a href=\"https://marimo.io/\">Marimo</a>, and it might just be the Jupyter alternative I&#39;ve been searching for.</p>\n<h2 id=\"what-is-marimo\">What is Marimo?</h2>\n<p>Marimo is a reactive Python notebook environment that solves many long-standing issues with traditional notebooks. Unlike Jupyter, which stores notebooks as JSON with embedded code and outputs, Marimo notebooks are pure Python files that are:</p>\n<ul>\n<li><strong>Reactive</strong>: Run a cell, and Marimo automatically runs dependent cells or marks them as stale</li>\n<li><strong>Consistent</strong>: No hidden state problems that plague traditional notebooks</li>\n<li><strong>Executable</strong>: Can run as standard Python scripts from the command line</li>\n<li><strong>Git-friendly</strong>: Since they&#39;re just <code>.py</code> files, they work seamlessly with version control</li>\n<li><strong>Deployable</strong>: Easily share as interactive web apps or slides</li>\n</ul>\n<h2 id=\"why-this-matters-for-literate-programming\">Why This Matters for Literate Programming</h2>\n<p>Literate programming -the approach of writing code as a narrative explanation interleaved with executable components- is incredibly powerful for data science, ML, and AI work. It helps create self-documenting, reproducible research and applications.<br>The problem with Jupyter has always been that while it looks like literate programming, its execution model (arbitrary cell execution order) and hidden state make it fundamentally unreliable. Marimo solves this by ensuring deterministic execution based on variable dependencies rather than cell position.</p>\n<h2 id=\"key-features-that-won-me-over\">Key Features That Won Me Over</h2>\n<ol>\n<li><strong>Vim keybindings</strong>: As a Neovim user, this is non-negotiable</li>\n<li><strong>Modern editor features</strong>: GitHub Copilot integration, AI completion, and variable explorer</li>\n<li><strong>Reactive runtime</strong>: No more &quot;did I run all the cells in the right order?&quot; problems</li>\n<li><strong>Interactive elements</strong>: Sliders, tables, and plots that automatically update dependent cells</li>\n<li><strong>SQL integration</strong>: Write SQL against dataframes, databases, or other sources right in your notebook</li>\n<li><strong>Package management</strong>: Built-in support for dependency tracking and isolated environments</li>\n<li><strong>Pure Python storage</strong>: No more JSON files with embedded outputs making git diffs unreadable</li>\n</ol>\n<h2 id=\"comparisons-with-other-literate-programming-tools\">Comparisons with Other Literate Programming Tools</h2>\n<h3 id=\"plutojl-julia\">Pluto.jl (Julia)</h3>\n<p>Pluto.jl pioneered the reactive notebook concept that Marimo implements. Both share:</p>\n<ul>\n<li>Automatic reactivity based on variable dependencies</li>\n<li>Deterministic execution order</li>\n<li>Interactive UI elements</li>\n</ul>\n<p><strong>Differences</strong>:</p>\n<ul>\n<li>Pluto is Julia-specific; Marimo is Python-specific</li>\n<li>Marimo stores notebooks as standard <code>.py</code> files; Pluto uses a custom format</li>\n<li>Marimo has more built-in integrations with Python data science libraries</li>\n<li>Pluto has tighter integration with Julia&#39;s capabilities</li>\n</ul>\n<h3 id=\"livebook-elixir\">Livebook (Elixir)</h3>\n<p>Livebook brings reactive notebooks to the Elixir ecosystem, with:</p>\n<ul>\n<li>Smart cells for common tasks</li>\n<li>Built-in deployment capabilities</li>\n<li>Collaborative editing</li>\n</ul>\n<p><strong>Differences</strong>:</p>\n<ul>\n<li>Livebook embraces Elixir&#39;s concurrency model; Marimo follows Python&#39;s execution model</li>\n<li>Marimo&#39;s Python foundation makes it more accessible for data science work</li>\n<li>Livebook has more built-in tools for building distributed systems</li>\n</ul>\n<h2 id=\"pros-and-cons\">Pros and Cons</h2>\n<h3 id=\"pros\">Pros</h3>\n<ul>\n<li><strong>Reproducibility</strong>: Deterministic execution eliminates the &quot;run cells in wrong order&quot; problem</li>\n<li><strong>Git-friendly</strong>: Pure Python files make version control and collaboration much easier</li>\n<li><strong>No hidden state</strong>: Deleted cell variables are removed from memory</li>\n<li><strong>Deployability</strong>: From notebook to web app with minimal effort</li>\n<li><strong>Testability</strong>: Run standard test suites against your notebooks</li>\n<li><strong>Modern IDE features</strong>: Seems like they&#39;ve thought of everything</li>\n</ul>\n<h3 id=\"cons\">Cons</h3>\n<ul>\n<li><strong>Learning curve</strong>: The reactive model requires a shift in thinking if you&#39;re used to Jupyter</li>\n<li><strong>Ecosystem maturity</strong>: Newer than Jupyter, so fewer third-party extensions</li>\n<li><strong>Performance considerations</strong>: Automatic reactivity could cause issues with expensive computations (though there are options to mitigate this[^1])</li>\n<li><strong>Language limitation</strong>: Python-only, unlike Jupyter which supports multiple kernels</li>\n</ul>\n<h2 id=\"getting-started\">Getting Started</h2>\n<p>Installation is straightforward:</p>\n<pre><code class=\"language-bash\">uv pip install marimo\n# or with recommended extras\nuv pip install marimo[recommended]\n\n# Try the tutorial\nmarimo tutorial intro\n</code></pre>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>As someone deeply invested in both Vim/Neovim and the Python data ecosystem, Marimo strikes an impressive balance. It brings the benefits of reactive programming to Python notebooks while maintaining the flexibility and familiarity that Python users expect.<br>What truly sets Marimo apart is how it addresses the fundamental issues of reproducibility and hidden state that have plagued notebooks for years. By treating notebooks as actual programs with deterministic execution, it enables literate programming in a way that Jupyter always promised but never fully delivered.<br>Is it perfect? No. But it&#39;s the most compelling alternative I&#39;ve seen so far, and I&#39;m seriously considering making the switch for my daily work.</p>\n<hr>\n<p>[^1]: Marimo provides a &quot;lazy&quot; configuration option where cells that would be automatically re-executed are instead marked as &quot;stale&quot;, allowing users to manually control when expensive computations run. Users can also implement caching strategies using Marimo&#39;s built-in caching functionality, compartmentalise heavy computations into separate cells to control their execution flow, or use the @mo.cell decorator with runtime configurations to customise how specific cells behave when dependencies change.</p>\n"
  },
  {
    "title": "🏠 Why Companies and Individuals Are Moving Back from the Cloud",
    "date": "2025-03-20T00:00:00.000Z",
    "tags": [
      "cloud",
      "on-prem",
      "performance",
      "security",
      "mlops",
      "deployment",
      "best-practices",
      "data-science"
    ],
    "url": "/posts/cloud-repatriation-trends-implications.html",
    "content": "<p><strong>TL;DR:</strong> Cloud repatriation is gaining momentum with 86% of CIOs planning to move some workloads back on-premises, driven by unexpected costs, performance needs, security concerns, and desire for greater control - though most organisations are adopting hybrid approaches rather than abandoning cloud entirely, strategically placing workloads where they function most efficiently.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>The last decade has witnessed the meteoric rise of cloud computing, with organisations large and small migrating their data, applications, and infrastructure to public cloud environments. The promises were compelling: reduced capital expenditure, unlimited scalability, enhanced flexibility, and access to cutting-edge technologies without the overhead of managing physical infrastructure. However, a notable countertrend has emerged in recent years -cloud repatriation. This phenomenon, sometimes referred to as &quot;reverse cloud migration,&quot; involves moving workloads, applications, and data back from public cloud environments to on-premises data centres, private clouds, or hybrid setups (<a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International, 2023</a>). I&#39;ve previously explored this topic in my article [The On-Prem Comeback (aka Cloud Repatriation)]({{ site.baseurl }}{% link _posts/2024-11-14-cloud-repatriation.md %}), where I introduced the basic concepts and early examples of this trend.<br>This article explores the growing cloud repatriation movement, examining why organisations and individuals are reconsidering their cloud-first strategies, the key drivers behind these decisions, and how they&#39;re implementing these transitions to achieve more balanced and optimised IT infrastructures.</p>\n<h2 id=\"the-scale-of-the-cloud-repatriation-movement\">The Scale of the Cloud Repatriation Movement</h2>\n<p>The repatriation trend is not isolated but represents a significant shift in how organisations approach their IT infrastructure strategy. According to a 2021 survey by IDC cited by <a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International</a>, 80% of organisations reported repatriating workloads or data from public cloud environments. More recent data from the end of 2024 showed that 86% of CIOs planned to move some public cloud workloads back to private cloud or on-premises -the highest on record for the Barclays CIO Survey (<a href=\"https://www.puppet.com/blog/cloud-repatriation\">Puppet, 2025</a>).<br>A recent survey by Rackspace found that nearly seven in 10 companies (69%) have moved at least some applications off the cloud and back to on-premise systems or private clouds (<a href=\"https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/\">ZDNet, 2025</a>).<br>It&#39;s important to note that this doesn&#39;t represent a wholesale abandonment of cloud computing. Only about 8% of organisations are moving their entire workloads off the cloud, according to an October 2024 IDC survey (<a href=\"https://www.puppet.com/blog/cloud-repatriation\">Puppet, 2025</a>). Most are selectively repatriating specific workloads while maintaining others in the cloud, resulting in more nuanced, hybrid approaches to IT infrastructure.</p>\n<h2 id=\"key-drivers-of-cloud-repatriation\">Key Drivers of Cloud Repatriation</h2>\n<h3 id=\"cost-optimisation\">Cost Optimisation</h3>\n<p>While the cloud initially promised cost savings through reduced capital expenditure and operational flexibility, many organisations have experienced what industry experts call &quot;bill shock&quot; as their cloud usage scales. According to <a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International</a>, &quot;a Gartner study predicts that through 2024, 60% of infrastructure and operations leaders will encounter public cloud cost overruns that negatively impact their on-premises budgets&quot;.<br>This cost concern is particularly relevant for organisations with predictable, high-volume workloads. According to <a href=\"https://www.rsa.com/resources/blog/identity-governance-and-administration/cloud-repatriation-why-enterprise-it-is-returning-from-the-cloud/\">RSA</a>, the company 37Signals announced that its &quot;cloud exit&quot; would save more than $10 million over five years. Similarly, a 2022 report by Andreessen Horowitz found that repatriation of cloud workloads could reduce cloud bills by 50% or more for some companies (<a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International, 2023</a>).<br>David Linthicum, a leading consultant and former CTO with Deloitte, attributes much of this cost issue to technical debt: &quot;<em>They didn&#39;t refactor the applications to make them more efficient in running on the public cloud providers. So the public cloud providers, much like if we&#39;re pulling too much electricity off the grid, just hit them with huge bills to support the computational and storage needs of those under-optimized applications</em>&quot; (<a href=\"https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/\">ZDNet, 2025</a>).</p>\n<h3 id=\"performance-and-latency\">Performance and Latency</h3>\n<p>Performance requirements are driving many repatriation decisions, particularly for applications requiring ultra-low latency. According to <a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International</a>, &quot;a study by the IEEE found that for certain AI workloads, on-premises GPU clusters outperformed cloud-based solutions by up to 30% in terms of performance per dollar&quot;.<br>This performance concern is especially critical in fields like financial trading, scientific research, and manufacturing where latency can significantly impact outcomes. As <a href=\"https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully\">ComputerWeekly</a> notes, &quot;time-sensitive data includes information that users need to access as rapidly as possible -think financial trading feeds -or where the application is sensitive to latency&quot;.</p>\n<h3 id=\"security-and-compliance\">Security and Compliance</h3>\n<p>Security concerns and regulatory compliance requirements are powerful motivators for cloud repatriation. According to the Rackspace survey cited by <a href=\"https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/\">ZDNet</a>, data security and compliance concerns were the most common reason for repatriation, cited by 50% of respondents.<br>The implementation of stringent regulations like GDPR has compelled many organisations to keep certain data within specific geographic boundaries. As <a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International</a> highlights, &quot;<em>The Data Protection Commission reported a 59% increase in GDPR complaints in 2022, underscoring the importance of data sovereignty</em>&quot;.<br>Despite cloud providers&#39; significant security investments, many organisations prefer to maintain direct control over their most sensitive data. According to <a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International</a>, &quot;<em>a 2023 Thales Cloud Security Study found that 45% of businesses have experienced a cloud-based data breach or failed audit in the past 12 months, highlighting ongoing security concerns</em>&quot;.</p>\n<h3 id=\"control-and-vendor-lock-in\">Control and Vendor Lock-in</h3>\n<p>The desire for greater control over hardware and software configurations, along with concerns about vendor lock-in, are also driving repatriation efforts. On-premises infrastructure offers more customisation possibilities that may not be available in public cloud environments.<br>Richard Robbins, founder of TheTechnologyVault.com, observes that &quot;<em>enterprises don&#39;t like being dependent upon someone else&#39;s cloud infrastructure</em>&quot; (<a href=\"https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/\">ZDNet, 2025</a>). This concern is particularly acute among regulated industries such as financial institutions, which are &quot;<em>moving some or all of their web apps from the cloud back to on-prem or to hybrid setups</em>&quot; due to &quot;vulnerability and downsides to cloud hosting&quot; that make &quot;executives feel nervous about not having more control&quot;.</p>\n<h2 id=\"the-emergence-of-balanced-approaches\">The Emergence of Balanced Approaches</h2>\n<p>Rather than a binary choice between cloud and on-premises, organisations are increasingly adopting hybrid and multi-cloud approaches that offer the best of both worlds. This trend allows organisations to:</p>\n<ul>\n<li>Keep sensitive or high-performance workloads on-premises</li>\n<li>Leverage cloud services for scalability and innovation</li>\n<li>Maintain flexibility to adapt to changing business needs</li>\n</ul>\n<p>According to <a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International</a>, &quot;<em>The hybrid cloud market is expected to grow from $85.3 billion in 2022 to $262.4 billion by 2027, according to MarketsandMarkets research</em>&quot;. Similarly, &quot;<em>Flexera&#39;s 2023 State of the Cloud Report revealed that 71% of enterprises are pursuing a hybrid cloud strategy, combining public cloud, private cloud, and on-premises infrastructure</em>&quot;.</p>\n<h2 id=\"personal-cloud-repatriation\">Personal Cloud Repatriation</h2>\n<p>The repatriation trend isn&#39;t limited to enterprises. Individuals are also exploring self-hosting options for personal data.<br>For example, <a href=\"https://hachyderm.io/@Jeffrey04/114175854454606516\">a fediverse user</a> recently posted about developing a self-hosted photo album application when faced with cloud storage limitations: &quot;<em>Being an enthusiastic photographer, my partner captured moments of us together. However, the increasing stack of photos is accelerating the imminent explosion of my cloud storage</em>&quot; (<a href=\"https://kitfucoda.medium.com/a-love-story-in-code-building-my-self-hosted-photo-album-b56a4e89ebdd\">KitFu Coda, 2023</a>). This personal project highlights how individuals with technical skills can leverage idle hardware to create cost-effective alternatives to cloud storage services.<br>As <a href=\"https://kitfucoda.medium.com/a-love-story-in-code-building-my-self-hosted-photo-album-b56a4e89ebdd\">they note</a>, &quot;<em>Self-hosting your own data is becoming a trend these days, and it is really not hard to get started</em>&quot;. This trend parallels the enterprise movement, with individuals seeking greater control, cost savings, and privacy for their personal data.</p>\n<h2 id=\"planning-for-successful-repatriation\">Planning for Successful Repatriation</h2>\n<p>For organisations considering cloud repatriation, careful planning is essential. Key considerations include:</p>\n<ol>\n<li><strong>Workload Assessment</strong>: Not all workloads benefit equally from repatriation. <a href=\"https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully\">ComputerWeekly</a> advises that &quot;<em>broadly, repatriation might be the best option where data is sensitive, time sensitive or expensive to store in the cloud</em>&quot;.</li>\n<li><strong>Infrastructure Preparation</strong>: Organisations must ensure they have the physical capacity, networking, power, and cooling capabilities to support repatriated workloads. According to <a href=\"https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully\">ComputerWeekly</a>, &quot;<em>a large repatriation project might be a prompt to reorganise the datacentre, perhaps by moving to newer equipment that can pack more storage into a single rack or that consumes less power</em>&quot;.</li>\n<li><strong>Skills Assessment</strong>: <a href=\"https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully\">ComputerWeekly</a> notes the importance of having &quot;<em>enough staff to provision and manage a larger system</em>&quot; with the necessary &quot;<em>security and privacy skills needed to handle sensitive data</em>&quot; and &quot;<em>technical know-how to handle mission-critical, latency sensitive applications</em>&quot;.</li>\n<li><strong>Future-Proofing</strong>: <a href=\"https://www.rsa.com/resources/blog/identity-governance-and-administration/cloud-repatriation-why-enterprise-it-is-returning-from-the-cloud/\">RSA</a> emphasises the importance of maintaining flexibility: &quot;<em>Organizations should consider the long-term implications of repatriation for their overall IT strategy. This includes planning for future scalability, considering how repatriation fits into the broader digital transformation initiatives, and ensuring that the new infrastructure aligns with long-term business goals</em>&quot;.</li>\n</ol>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Cloud repatriation represents a maturing perspective on IT infrastructure strategy rather than a rejection of cloud computing. As organisations gain experience with cloud environments, they&#39;re becoming more strategic about which workloads belong where, based on factors like cost, performance, security, and control.<br>The future likely belongs to balanced, hybrid approaches that leverage the strengths of both cloud and on-premises infrastructure. As <a href=\"https://www.puppet.com/blog/cloud-repatriation\">Puppet</a> notes, &quot;<em>Cloud repatriation is not an endpoint, but rather a strategic tool in the ongoing evolution of enterprise IT. It empowers organizations to take control of their digital assets, enhance their security posture, and align their technology infrastructure with their business objectives</em>&quot;.<br>For both organisations and individuals, the key is making informed decisions about where and how to deploy IT resources based on specific needs rather than following blanket &quot;cloud-first&quot; or &quot;on-premises-first&quot; policies. This nuanced approach to infrastructure strategy will likely characterise the next phase of digital transformation as the industry moves beyond the initial hype cycles of cloud adoption.</p>\n"
  },
  {
    "title": "⚙️ Turning Data Science into Real-World Value with The Drivetrain Framework",
    "date": "2025-03-20T00:00:00.000Z",
    "tags": [
      "data-science",
      "decision-making",
      "machine-learning",
      "modelling-mindsets",
      "optimisation",
      "fast-ai",
      "advantage",
      "best-practices",
      "design-principles",
      "causal-inference",
      "business-value",
      "predictive-modelling",
      "integration",
      "deliberate-experimentation",
      "real-value"
    ],
    "url": "/posts/the-drivetrain-method.html",
    "content": "<p><strong>TL;DR:</strong> Jeremy Howard&#39;s Drivetrain Framework transforms data science from isolated predictions to value-creating systems through four steps: defining clear objectives, identifying controllable levers, collecting causal data through deliberate experimentation, and building integrated systems that combine predictive models with optimisation - bridging the gap between analytics and measurable business results.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Most data science initiatives fail to deliver meaningful impact. Why? Because they focus on prediction rather than action. Organisations spend millions building sophisticated prediction models that tell them what <em>might</em> happen, but provide no clear path to influencing outcomes.<br>This gap between prediction and value creation is what Jeremy Howard, data scientist and entrepreneur, addressed in his transformative &quot;<a href=\"https://www.youtube.com/watch?v=vYrWTDxoeGg\">Drivetrain Framework</a>&quot; back in 2012. Having successfully applied this approach to revolutionise insurance pricing, Howard outlines a systematic method for connecting data science to tangible business results.<br>The framework isn&#39;t about building more complex algorithms -it&#39;s about constructing systems that link predictions to decisions that drive value. If you&#39;re struggling to translate advanced analytics into bottom-line results or finding your data science investments yield interesting insights but limited action, this framework offers a practical solution to bridge that gap.</p>\n<h2 id=\"the-four-critical-components\">The Four Critical Components</h2>\n<p>The Drivetrain Framework consists of four interconnected steps that bridge the gap between data and value:</p>\n<h3 id=\"1-define-your-objective\">1. Define Your Objective</h3>\n<p>Begin with absolute clarity about what you&#39;re trying to achieve. In Howard&#39;s insurance example, the objective was straightforward: maximise profit from each customer based on price. For Google&#39;s search engine, it was finding the most relevant web page based on a query. For a marketing team, it might be maximising customer lifetime value.<br>Without a clear objective, data science becomes an academic exercise. With one, it becomes a targeted tool for value creation.</p>\n<h3 id=\"2-identify-your-levers\">2. Identify Your Levers</h3>\n<p>Next, determine what variables you can actually control. These are your &quot;levers&quot; -the actions you can take to influence outcomes:</p>\n<ul>\n<li>For Google, the key lever was the ordering of search results</li>\n<li>For insurers, it was the price offered to each customer</li>\n<li>For marketers, levers include product recommendations, discount offers, and communication timing</li>\n</ul>\n<p>The insight here is focusing not on what you can predict, but on what you can change.</p>\n<h3 id=\"3-collect-causal-data\">3. Collect Causal Data</h3>\n<p>Howard emphasises a crucial distinction: most organisations have plenty of observational data showing correlations, but lack causal data showing what happens when you pull different levers.<br>This requires intentional experimentation:  </p>\n<ul>\n<li>The insurance company randomly varied prices to understand true price-response relationships</li>\n<li>A marketing team might randomly test diverse recommendations rather than showing what customers already like</li>\n</ul>\n<p>The counterintuitive insight: You must sometimes sacrifice short-term optimisation to collect data that enables superior long-term results. Howard convinced insurance executives to randomise pricing for six months -initially accepting potentially lower profits -to build models that later significantly increased their profitability and transformed how the entire industry approached pricing.</p>\n<h3 id=\"4-build-an-integrated-system\">4. Build an Integrated System</h3>\n<p>The final step combines three elements to connect levers to objectives: </p>\n<ul>\n<li><strong>Modeller</strong>: Build predictive models for key relationships (e.g. how price affects purchase probability)</li>\n<li><strong>Simulator</strong>: Combine models to predict outcomes of actions (e.g. how price changes affect profit across customer segments)</li>\n<li><strong>Optimizer</strong>: Find the best lever settings to achieve objectives (e.g. optimal price for each customer)</li>\n</ul>\n<p>This integrated approach replaces the need for complex &quot;PageRank-like&quot; algorithms with systems that combine simpler models to optimise real-world outcomes.</p>\n<h2 id=\"application-revolutionising-marketing\">Application: Revolutionising Marketing</h2>\n<p>Howard suggests marketing analytics remains in the &quot;Dark Ages&quot; and ready for transformation through the Drivetrain approach:<br>Consider Amazon&#39;s recommendation system. Rather than simply suggesting more books by authors you&#39;ve already read, a Drivetrain-based system would:  </p>\n<ol>\n<li>Define the objective as maximising customer lifetime value</li>\n<li>Identify recommendation content as a key lever</li>\n<li>Collect causal data by testing diverse recommendations, including unexpected ones</li>\n<li>Build an integrated system that models what customers might enjoy but don&#39;t yet know about, and optimises for long-term value</li>\n</ol>\n<p>In Howard&#39;s experience, companies implementing this approach have seen substantial improvements in customer engagement and retention while achieving meaningful reductions in marketing costs.</p>\n<h2 id=\"drawing-from-engineering\">Drawing from Engineering</h2>\n<p>Howard notes that many solutions already exist in engineering disciplines, which data scientists would benefit from studying.<br>Aircraft designers have used integrated models and optimisation for decades, combining aerodynamic models, structural analysis, and optimisation techniques to create planes that safely fly millions of passengers daily.<br>Building construction similarly relies on systems that integrate architectural models, structural engineering, and materials science to optimize for safety, cost, and aesthetics.<br>The most advanced example might be Google&#39;s self-driving car, which integrates multiple predictive models (how the car responds to controls, what sensors detect) with optimisation to safely navigate real-world environments, significantly improving safety in testing environments.<br>These engineering successes demonstrate how combining relatively simple models into integrated systems can solve extraordinarily complex problems.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The Drivetrain Framework represents a fundamental shift in how we should approach data science:</p>\n<ol>\n<li>Move beyond building better predictive models in isolation</li>\n<li>Focus on connecting predictions to actions that drive real value</li>\n<li>Invest in collecting causal data through deliberate experimentation</li>\n<li>Integrate modelling, simulation, and optimisation into coherent systems</li>\n</ol>\n<p>By adopting this framework, organisations can bridge the gap between sophisticated analytics and meaningful results. The companies that will gain competitive advantage aren&#39;t those with marginally better algorithms, but those that build integrated systems connecting data to decisions that create value.</p>\n<h2 id=\"getting-started\">Getting Started</h2>\n<p>To begin implementing the Drivetrain approach:  </p>\n<ol>\n<li>Identify one high-value business objective with measurable outcomes</li>\n<li>Map the specific levers your team can control that influence this objective</li>\n<li>Design small-scale experiments to collect causal data about these relationships</li>\n<li>Start simple -build basic models for key relationships, then integrate them before attempting sophisticated optimisation</li>\n</ol>\n<p>The most important step is shifting your thinking from &quot;<em>what can we predict?</em>&quot; to &quot;<em>what actions can we take to create value?</em>&quot; -the essence of the Drivetrain Framework.</p>\n"
  },
  {
    "title": "📦 From Compilation to Containerisation and Back Again",
    "date": "2025-03-19T00:00:00.000Z",
    "tags": [
      "deno",
      "typescript",
      "deployment",
      "cross-platform",
      "evolution",
      "toolchain",
      "best-practices",
      "code-quality"
    ],
    "url": "/posts/compilation-going-back-full-circle.html",
    "content": "<p><strong>TL;DR:</strong> Programming languages have evolved from compiled executables to interpreted languages and containerisation, but Deno 2.0 brings deployment full circle by enabling TypeScript/JavaScript compilation into standalone binaries-offering simplified cross-platform deployment whilst maintaining ecosystem richness and enabling single-language development across entire application stacks.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Over the years, I&#39;ve experimented with numerous programming languages and deployment strategies. Python has been my domain&#39;s lingua franca -with its vast ecosystem for data science and AI applications. However, its deployment complexities have consistently been a pain point: managing dependencies, configuring containers, and setting up build pipelines.<br>This search for a better alternative has led me through statically compiled languages like Go and Rust; JIT-compiled languages like Julia; and hosted languages like Clojure and Scala. Yet most failed to provide a good balance between ecosystem richness and deployment simplicity. Recently, however, Deno 2.0 has emerged as a compelling solution -particularly with its ability to compile TypeScript (TS) / JavaScript (JS) to standalone executables.</p>\n<h2 id=\"the-circular-evolution-of-programming-languages\">The Circular Evolution of Programming Languages</h2>\n<p>Programming languages have undergone a fascinating evolution. In the beginning (the late 1950s and 1960s), languages like Fortran, COBOL, and C were ahead-of-time compiled -transformed directly into machine code executables that could run without additional dependencies.<br>As computing evolved, the pendulum swung toward higher-level languages -interpreted languages like Python and hosted environments like the JVM- prioritising readability and developer productivity over raw performance. These languages abstracted away machine-level concerns, allowing developers to focus on solving business problems.<br>Yet this shift introduced new challenges. Python applications often require managing complex dependency trees, virtual environments, and platform-specific configurations. The infamous &quot;<em>works on my machine</em>&quot; problem became so pervasive that containerisation emerged as a solution.<br>While effective, containerisation introduces its own complexities: orchestration, image management, and networking configurations. What began as a solution to simplify deployment has become a complex system requiring specialised knowledge.</p>\n<h2 id=\"deno-compilation-makes-a-comeback\">Deno: Compilation Makes a Comeback</h2>\n<p>Deno 2.0 represents a return to first principles. As highlighted in the <a href=\"https://youtube.com/watch?v=ZsDqTQs3_G0\">Run JavaScript Anywhere</a> video, its <code>compile</code> command enables developers to transform JS and TS programs into standalone binaries that run across major platforms -no runtime installation or dependencies required.</p>\n<pre><code class=\"language-typescript\">// sample.ts\nimport { open } from &quot;https://deno.land/x/open/index.ts&quot;;\n\n// Open a URL in the default browser\nawait open(&quot;https://example.com&quot;);\n</code></pre>\n<p>With a simple <code>deno compile sample.ts</code> command, this code becomes a standalone executable that works on any machine without requiring Deno to be installed.<br>This compilation process isn&#39;t traditional transpilation to machine code -it embeds your JS and TS code into a specialized Deno runtime binary (denort). Your script and dependencies are bundled as an EZIP file and injected into the runtime binary, creating a self-contained executable that can be code-signed for distribution.</p>\n<p>The key benefits include:</p>\n<ol>\n<li><strong>Cross-platform compatibility</strong> without runtime requirements</li>\n<li><strong>Simplified deployment</strong> with single-binary distribution</li>\n<li><strong>Bundled assets</strong> for complete portability</li>\n<li><strong>Improved startup times</strong> compared to interpreter-based approaches</li>\n</ol>\n<p>Deno 2.0 enhances these capabilities further with support for npm packages, web workers, cross-compilation, smaller binary sizes, and code signing with custom icons - making it viable for complete applications, not just scripts.</p>\n<h2 id=\"the-single-language-advantage\">The Single Language Advantage</h2>\n<p>Beyond deployment simplicity, using a single language across an entire project stack creates significant organisational benefits. I&#39;ve experienced first-hand how using different languages for front-end, back-end, and data science work can create silos within teams.<br><a href=\"https://dockyard.com/blog/2024/02/06/5-benefts-amplified-saw-switching-to-elixir\">Amplified&#39;s case study</a> demonstrates this point clearly. After switching from a React/JS front-end and Phoenix/Elixir back-end to an all-Elixir approach with LiveView, they reported:</p>\n<ol>\n<li><strong>Halved server costs</strong> through more efficient resource utilisation</li>\n<li><strong>Dramatically increased development speed</strong> by eliminating cross-language silos</li>\n<li><strong>Improved team cohesion</strong> with shared tooling and knowledge</li>\n<li><strong>Enhanced maintainability</strong> through code reuse</li>\n<li><strong>Reduced team size requirements</strong> from 12 developers to just 2</li>\n</ol>\n<p>TS with Deno provides a similar single language opportunity -allowing teams to build front-end interfaces, back-end services, and data processing workflows with the same toolchain. The JS/TS ecosystem is rapidly maturing for AI, ML, and data science applications, as I noted in my previous article on [Modern Data Science and AI Engineering with Deno 2.0]({{ site.baseurl }}{% link _posts/2024-09-05-deno.md %}).<br>One often overlooked benefit is the reduced cognitive load when developers don&#39;t need to context-switch between different language paradigms, package managers, testing frameworks, and debugging approaches.</p>\n<h2 id=\"practical-applications\">Practical Applications</h2>\n<p>Deno&#39;s compilation capabilities shine in several real-world scenarios:</p>\n<ol>\n<li><strong>CLI Tools</strong>: Creating self-contained executables that &quot;just work&quot; across platforms without complex installation instructions</li>\n<li><strong>Offline Environments</strong>: Deploying to systems without internet access, where package resolution at runtime isn&#39;t possible</li>\n<li><strong>Cross-Platform Applications</strong>: Building desktop applications that leverage web technologies without requiring a browser runtime</li>\n</ol>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>We&#39;ve come full circle in programming language evolution -from compiled languages like Fortran in the 1950s, to interpreted languages for improved developer experience, to containerisation for managing deployment complexities, and now back to compilation with Deno[^1].<br>Deno&#39;s approach represents a compelling blend - combining deployment simplicity with the ecosystem richness of modern TS/JS. For AI engineering, this addresses many pain points of Python deployment while maintaining access to growing ecosystem of data science tools.<br>While Elixir offers similar single language benefits, its distribution story remains a work in progress with projects like <a href=\"https://github.com/burrito-elixir/burrito\">Burrito</a> showing promise but not yet fully mature. Until then, Deno stands out as a viable alternative for simplified deployment without sacrificing ecosystem benefits.<br>The future of deployment may look surprisingly like its past, just with better languages and tools at our disposal -offering a path toward more cohesive, efficient software development that reduces complexity without sacrificing capability.</p>\n<hr>\n<p>[^1]: Go, Zig, Rust, C/C++ D, Nim, Common Lisp are some prominent examples of ahead-of-time compiled languages that -with the exception of Common Lisp- excel in systems programming. However, Deno allows a ubiquitous, higher-level language like JS and its superset TS to join the club of languages that can easily package code to a cross-platform single binary. </p>\n"
  },
  {
    "title": "🧠 RAG vs CAG: Understanding Knowledge Augmentation in LLMs",
    "date": "2025-03-18T00:00:00.000Z",
    "tags": [
      "rag",
      "llm",
      "ai",
      "machine-learning",
      "prompt-engineering",
      "nlp",
      "data-processing",
      "best-practices"
    ],
    "url": "/posts/rag-or-cag.html",
    "content": "<p><strong>TL;DR:</strong> Retrieval Augmented Generation (RAG) and Cache Augmented Generation (CAG) represent two distinct approaches to expanding LLM knowledge: RAG dynamically retrieves relevant documents for each query, offering scalability for large datasets, whilst CAG preloads all information into the model&#39;s context window, providing faster responses for smaller, static knowledge bases.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Large Language Models (LLMs) face a fundamental knowledge problem: they&#39;re limited to information present in their training data. This creates challenges when dealing with recent events that occurred after training or proprietary information specific to an organization.<br>To address these limitations, two primary augmentation techniques have emerged: Retrieval Augmented Generation (RAG) and Cache Augmented Generation (CAG). This article breaks down both approaches based on  <a href=\"https://www.youtube.com/channel/UCKWaEZ-_VweaEx1j62do_vQ\">IBM Technology</a>&#39;s comprehensive explanation from their <a href=\"https://youtube.com/watch?v=HdafI0t3sEY\">video on RAG vs CAG</a>, examining how they work, their capabilities, and when to use each one.</p>\n<h2 id=\"understanding-rag-and-cag\">Understanding RAG and CAG</h2>\n<h3 id=\"retrieval-augmented-generation-rag\">Retrieval Augmented Generation (RAG)</h3>\n<p>RAG operates through a two-phase system:</p>\n<ol>\n<li><strong>Offline Phase (Preparation)</strong><ul>\n<li>Documents are broken into manageable chunks.</li>\n<li>Vector embeddings are created for each chunk using an embedding model.</li>\n<li>These embeddings are stored in a vector database, creating a searchable knowledge index.</li>\n</ul>\n</li>\n<li><strong>Online Phase (Query &amp; Response)</strong><ul>\n<li>The user submits a query.</li>\n<li>The RAG retriever converts this query to a vector using the same embedding model.</li>\n<li>The system performs a similarity search in the vector database.</li>\n<li>It retrieves the most relevant document chunks (typically 3-5 passages).</li>\n<li>These chunks and the user&#39;s query are placed in the LLM&#39;s context window.</li>\n<li>The LLM generates an answer based on both the query and the retrieved context.</li>\n</ul>\n</li>\n</ol>\n<p>For example, if asked <em>&quot;What film won Best Picture this year?&quot;</em>, the system might retrieve information about <em>&quot;Anora&quot;</em> winning the award, even if this occurred after the model&#39;s original training.</p>\n<p>A key advantage of RAG is its modularity - components like the vector database, embedding model, or LLM can be swapped independently without rebuilding the entire system.</p>\n<h3 id=\"cache-augmented-generation-cag\">Cache Augmented Generation (CAG)</h3>\n<p>CAG takes a fundamentally different approach:</p>\n<ul>\n<li>Instead of retrieving knowledge on demand, CAG preloads all available information into the model&#39;s context window</li>\n<li>The entire knowledge corpus is formatted into one massive prompt that fits within the model&#39;s context limits</li>\n<li>The LLM processes this extensive input in a single forward pass</li>\n<li>The model&#39;s internal state is captured in what&#39;s called a &quot;KV cache&quot; (key-value cache)</li>\n<li>When a user query arrives, it&#39;s added to this pre-existing KV cache</li>\n<li>The model can access any relevant information from the cache without reprocessing the entire knowledge base</li>\n</ul>\n<p>The fundamental distinction: RAG fetches only what it predicts is needed, while CAG loads everything upfront and remembers it for later use.</p>\n<h2 id=\"comparing-capabilities\">Comparing Capabilities</h2>\n<h3 id=\"accuracy\">Accuracy</h3>\n<ul>\n<li><strong>RAG</strong>: Accuracy depends heavily on the retriever component. If the retriever fails to fetch relevant documents, the LLM won&#39;t have the facts needed to answer correctly.</li>\n<li><strong>CAG</strong>: Guarantees that all information is available (assuming it exists in the knowledge base), but places the burden on the LLM to extract the right information from a large context.</li>\n</ul>\n<h3 id=\"latency\">Latency</h3>\n<ul>\n<li><strong>RAG</strong>: Higher latency due to additional steps of embedding the query, searching the index, and processing retrieved text.</li>\n<li><strong>CAG</strong>: Lower latency once knowledge is cached, as answering queries requires only one forward pass without retrieval lookup time.</li>\n</ul>\n<h3 id=\"scalability\">Scalability</h3>\n<ul>\n<li><strong>RAG</strong>: Can scale to millions of documents as only a small portion is retrieved per query.</li>\n<li><strong>CAG</strong>: Limited by the model&#39;s context window size (typically ~32k-100k tokens), restricting it to a few hundred documents at most.</li>\n</ul>\n<h3 id=\"data-freshness\">Data Freshness</h3>\n<ul>\n<li><strong>RAG</strong>: Easy to update incrementally as you add new document embeddings or remove outdated ones.</li>\n<li><strong>CAG</strong>: Requires recomputation when data changes, making it less suitable for frequently updated information.</li>\n</ul>\n<h2 id=\"when-to-use-each-approach\">When to Use Each Approach</h2>\n<p>The video presents several scenarios to illustrate when each approach is more appropriate:</p>\n<ol>\n<li><strong>IT Help Desk Bot with Static Manual (200 pages, rarely updated)</strong><ul>\n<li><strong>Best Choice</strong>: CAG</li>\n<li><strong>Rationale</strong>: Knowledge base is small enough to fit in most LLM context windows, information is static, and caching enables faster query responses.</li>\n</ul>\n</li>\n<li><strong>Legal Research Assistant (Thousands of constantly updated documents)</strong><ul>\n<li><strong>Best Choice</strong>: RAG</li>\n<li><strong>Rationale</strong>: Knowledge base is massive and dynamic, precise citations are required, and incremental updates are essential.</li>\n</ul>\n</li>\n<li><strong>Clinical Decision Support System (Patient records, treatment guides, drug interactions)</strong><ul>\n<li><strong>Best Choice</strong>: Hybrid Approach</li>\n<li><strong>Rationale</strong>: Use RAG to retrieve relevant subsets from the massive knowledge base, then load that retrieved content into a long-context model using CAG for follow-up questions.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The choice between RAG and CAG ultimately depends on your specific use case. Consider RAG when dealing with large or frequently updated knowledge sources, when citations are necessary, or when resources for running long-context models are limited. CAG is preferable when working with a fixed knowledge set that fits within your model&#39;s context window, when low latency is crucial, or when you want to simplify deployment.<br>As LLM technology evolves with expanding context windows and improved retrieval mechanisms, we may see these approaches converge or new hybrid solutions emerge. For now, understanding the strengths and limitations of both RAG and CAG allows AI engineers to make informed decisions about knowledge augmentation strategies that best suit their specific applications.</p>\n"
  },
  {
    "title": "🤖 The State of AI Agents in 2025",
    "date": "2025-03-15T00:00:00.000Z",
    "tags": [
      "ai",
      "machine-learning",
      "llm",
      "best-practices",
      "evaluation",
      "prompt-engineering",
      "decision-making"
    ],
    "url": "/posts/navigating-ais-frontier-2025.html",
    "content": "<p><strong>TL;DR:</strong> Despite significant advancements creating a &quot;perfect storm&quot; for AI agents in 2025, truly autonomous systems still face five categories of cumulative errors that prevent reliable performance; overcoming these challenges requires focused strategies in data curation, robust evaluation frameworks, scaffolding systems, distinctive user experiences, and multimodal approaches.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>The AI landscape has evolved at a breathtaking pace over the past few years, with autonomous AI agents being positioned as the next revolutionary frontier. At the 2025 AI Engineer Summit, Grace Isford, a partner at Lux Capital, delivered an <a href=\"https://www.youtube.com/watch?v=HS5a8VIKsvA\">insightful keynote</a> on &quot;The State of the AI Frontier&quot; that challenged the prevailing narrative about AI agents. While many industry players proclaim that 2025 marks the &quot;perfect storm&quot; for AI agents, Isford&#39;s presentation offered a more nuanced view, highlighting both the tremendous progress and the significant challenges that remain. This article summarises the key insights from her keynote, examining the current state of AI agents and the strategies developers can employ to overcome persistent limitations.</p>\n<h2 id=\"the-perfect-storm-for-ai-agents\">The Perfect Storm for AI Agents</h2>\n<p>The speaker began by acknowledging the remarkable progress in AI over the past two and a half years. The industry has seen exponential advancements since the release of Stable Diffusion in August 2022, with the pace of innovation only accelerating. 2025 has already witnessed several landmark developments:</p>\n<ul>\n<li>The announcement of the $500 billion Stargate project collaboration between the U.S. government, OpenAI, SoftBank, and Oracle</li>\n<li>OpenAI&#39;s o3 model exceeding human performance in the Arc AGI challenge</li>\n<li>DeepSeq&#39;s R1 model launch causing market disruptions and reaching the top of the App Store</li>\n<li>France&#39;s new AI initiative announced at the France AI Summit, bringing Europe back into the global AI race</li>\n</ul>\n<p>These developments, alongside other factors, have created what many call the &quot;perfect storm&quot; for AI agents:</p>\n<ol>\n<li>Reasoning models (like OpenAI&#39;s o1 and o3, DeepSeq&#39;s R1, and Grok&#39;s latest offering) now outperform humans in various benchmarks</li>\n<li>Increased test-time compute (more resources allocated to inference rather than just training)</li>\n<li>Engineering and hardware optimisations driving efficiency</li>\n<li>Cheaper inference and hardware costs</li>\n<li>A narrowing gap between open-source and closed-source models</li>\n<li>Massive infrastructure investments from governments and corporations worldwide</li>\n</ol>\n<h2 id=\"the-reality-gap-why-ai-agents-arent-quite-working-yet\">The Reality Gap: Why AI Agents Aren&#39;t Quite Working Yet</h2>\n<p>Despite this promising landscape, Isford argued that truly autonomous AI agents aren&#39;t functioning as seamlessly as industry hype suggests. To illustrate this point, she shared a real-world example of trying to use OpenAI&#39;s operator to book a flight from New York to San Francisco with specific requirements. Despite seemingly straightforward criteria (departure time after 3 PM, avoiding rush hour, specific airlines, budget constraints, seat preferences), the agent failed to deliver a satisfactory result.</p>\n<p>The presenter identified five categories of cumulative errors that prevent AI agents from delivering consistent, reliable results:</p>\n<ol>\n<li><strong>Decision Errors</strong>: Choosing incorrect facts or overthinking/exaggerating scenarios</li>\n<li><strong>Implementation Errors</strong>: Encountering access issues or integration failures (like CAPTCHA challenges)</li>\n<li><strong>Heuristic Errors</strong>: Applying wrong criteria or missing critical contextual information</li>\n<li><strong>Taste Errors</strong>: Failing to account for personal preferences not explicitly stated</li>\n<li><strong>Perfection Paradox</strong>: User expectations heightened by AI&#39;s capabilities in some areas lead to frustration when agents perform at merely human speed or make basic errors</li>\n</ol>\n<p>These errors compound dramatically in complex multi-agent systems with multi-step tasks. Isford presented a compelling visual example showing how even agents with impressive 99% and 95% accuracy rates drop to 60% and 8% reliability respectively after just 50 consecutive steps.</p>\n<h2 id=\"five-strategies-for-building-better-ai-agents\">Five Strategies for Building Better AI Agents</h2>\n<p>The keynote then shifted to offering concrete strategies for mitigating these challenges and building more effective AI agents:</p>\n<h3 id=\"1-data-curation\">1. Data Curation</h3>\n<ul>\n<li>Recognise that data is increasingly diverse (text, images, video, audio, sensor data)</li>\n<li>Curate proprietary data, including data generated by the agent itself</li>\n<li>Design &quot;data flywheels&quot; that automatically improve agent performance through user interactions</li>\n<li>Recycle and adapt to user preferences in real-time</li>\n</ul>\n<h3 id=\"2-robust-evaluation-systems\">2. Robust Evaluation Systems</h3>\n<ul>\n<li>Move beyond evaluations for verifiable domains (math, science) to develop frameworks for subjective assessments</li>\n<li>Collect signals about human preferences</li>\n<li>Build personalised evaluation systems that reflect actual user needs</li>\n<li>Sometimes the best evaluation is direct human testing rather than relying solely on benchmarks</li>\n</ul>\n<h3 id=\"3-scaffolding-systems\">3. Scaffolding Systems</h3>\n<ul>\n<li>Implement safeguards to prevent cascading failures when errors occur</li>\n<li>Build complex compound systems that can work together harmoniously</li>\n<li>Incorporate human intervention at critical junctures</li>\n<li>Develop self-healing agents that can recognise their own mistakes and correct course</li>\n</ul>\n<h3 id=\"4-user-experience-as-a-competitive-moat\">4. User Experience as a Competitive Moat</h3>\n<ul>\n<li>Recognise that UX differentiation is crucial when most applications are using the same foundation models</li>\n<li>Deeply understand user workflows to create elegant human-machine collaboration</li>\n<li>Integrate seamlessly with existing systems to deliver tangible ROI</li>\n<li>Focus on industries with proprietary data sources and specialised workflows (robotics, manufacturing, life sciences)</li>\n</ul>\n<h3 id=\"5-multimodal-approaches\">5. Multimodal Approaches</h3>\n<ul>\n<li>Move beyond basic chatbot interfaces to create more human-like experiences</li>\n<li>Incorporate multiple sensory capabilities (vision, voice, and potentially touch or smell)</li>\n<li>Build personal memory systems that understand users on a deeper level</li>\n<li>Transform inconsistent but visionary products into experiences that exceed expectations through novel interfaces</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>While 2025 has created what appears to be a perfect storm for AI agents with advanced reasoning models, increased compute efficiency, and massive infrastructure investments, the reality is that autonomous AI agents still face significant challenges. The cumulative effect of small errors across decision-making, implementation, heuristics, and user preferences creates substantial reliability issues in complex agent systems.</p>\n<p>However, as this keynote emphasised, these challenges are not insurmountable. By focusing on meticulous data curation, developing sophisticated evaluation frameworks, implementing robust scaffolding systems, prioritising distinctive user experiences, and embracing multimodal approaches, developers can build AI agents that deliver on their transformative potential. The lightning strike of truly autonomous, reliable AI agents may not have happened yet, but with these strategies, the industry is moving steadily toward that breakthrough moment.</p>\n"
  },
  {
    "title": "🗄️ SQLite: The Minimalist Database for AI Engineering",
    "date": "2025-02-11T00:00:00.000Z",
    "tags": [
      "ai",
      "data-modeling",
      "data-processing",
      "data-science",
      "go",
      "minimal",
      "production",
      "python",
      "zero-config"
    ],
    "url": "/posts/sqlite-minimalist-choice-for-ai-engineering.html",
    "content": "<p><strong>TL;DR:</strong> SQLite offers a zero-configuration, pre-installed database solution ideal for AI engineering projects, supporting modern data structures including vectors, graphs, and JSON documents whilst providing single-file portability, ACID compliance, and broad language compatibility - making it an excellent minimalist choice when specialised database systems would be overkill.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>In today&#39;s AI engineering landscape, choosing the right database can feel overwhelming. While specialised solutions like <a href=\"https://qdrant.tech/\">Qdrant</a> (vectors), <a href=\"https://neo4j.com/\">Neo4j</a> (graphs), and <a href=\"https://www.mongodb.com/\">MongoDB</a> (documents) excel in their niches, there&#39;s a compelling case for <a href=\"https://www.sqlite.org/index.html\">SQLite</a> as a versatile, minimalist solution that comes pre-installed on most systems and supports multiple data structures effectively. Speaking of minimalism, <a href=\"https://github.com/tconbeer/harlequin\">Harlequin</a> (named after a <a href=\"https://en.wikipedia.org/wiki/Harlequin_duck\">sea 🦆</a>) makes data exploration very enjoyable. \nCredit for the SQLite idea goes to <a href=\"https://bsky.app/profile/simonwillison.net\">Simon Willison</a>, a prolific AI researcher among others, who has been posting <a href=\"https://simonwillison.net/tags/sqlite/\">blog articles</a> and <a href=\"https://til.simonwillison.net/sqlite\">TILs</a> (Today I Learned) about it since 2003! </p>\n<h2 id=\"the-power-of-pre-installation\">The Power of Pre-installation</h2>\n<p>SQLite&#39;s ubiquity is remarkable. It comes pre-installed on:</p>\n<ul>\n<li>macOS</li>\n<li>Most Linux distributions (including Ubuntu, as evidenced by its <a href=\"https://releases.ubuntu.com/24.10/ubuntu-24.10-desktop-amd64.manifest\">manifest</a>)</li>\n<li>Python&#39;s standard library</li>\n<li>Android devices</li>\n<li>iOS devices</li>\n</ul>\n<p>This universal availability means you can start developing immediately without additional setup or installation steps.</p>\n<h2 id=\"modern-data-structure-support\">Modern Data Structure Support</h2>\n<p>Despite its lightweight nature, SQLite handles modern data structures surprisingly well:</p>\n<ol>\n<li><strong>Vector Storage</strong>[^1]</li>\n</ol>\n<pre><code class=\"language-sql\">CREATE VIRTUAL TABLE vec_items USING vec0(embedding float[4])\n</code></pre>\n<pre><code class=\"language-sql\">-- vectors can be provided as JSON or in a compact binary format\nINSERT INTO vec_items(rowid, embedding)\n  VALUES\n    (1, &#39;[-0.200, 0.250, 0.341, -0.211, 0.645, 0.935, -0.316, -0.924]&#39;),\n    (2, &#39;[0.443, -0.501, 0.355, -0.771, 0.707, -0.708, -0.185, 0.362]&#39;),\n    (3, &#39;[0.716, -0.927, 0.134, 0.052, -0.669, 0.793, -0.634, -0.162]&#39;),\n    (4, &#39;[-0.710, 0.330, 0.656, 0.041, -0.990, 0.726, 0.385, -0.958]&#39;);\n</code></pre>\n<pre><code class=\"language-sql\">-- KNN-style query\nSELECT\n  rowid,\n  distance\nFROM vec_items\nWHERE embedding MATCH &#39;[0.890, 0.544, 0.825, 0.961, 0.358, 0.0196, 0.521, 0.175]&#39;\nORDER BY distance\nLIMIT 3\n</code></pre>\n<ol start=\"2\">\n<li><strong>Graph Relationships</strong>[^2]</li>\n</ol>\n<pre><code class=\"language-sql\">-- Create table `nodes`\nCREATE TABLE IF NOT EXISTS nodes (\n    id TEXT PRIMARY KEY,\n    properties TEXT\n)\n</code></pre>\n<pre><code class=\"language-sql\">-- Create table `edges`\nCREATE TABLE IF NOT EXISTS edges (\n    source TEXT,\n    target TEXT,\n    relationship TEXT,\n    weight REAL,\n    PRIMARY KEY (source, target, relationship),\n    FOREIGN KEY (source) REFERENCES nodes(id),\n    FOREIGN KEY (target) REFERENCES nodes(id)\n)\n</code></pre>\n<pre><code class=\"language-sql\">-- Create indices of the `edges` between `source` and `target`, for improved performance\nCREATE INDEX IF NOT EXISTS source_idx ON edges(source)\nCREATE INDEX IF NOT EXISTS target_idx ON edges(target)\n</code></pre>\n<pre><code class=\"language-sql\">-- Count the no. of incoming and outgoing edges per node, known as &#39;degree centrality&#39; \nSELECT id,\n       (SELECT COUNT(*) FROM edges WHERE source = nodes.id) +\n       (SELECT COUNT(*) FROM edges WHERE target = nodes.id) as degree\nFROM nodes\nORDER BY degree DESC\nLIMIT 10\n</code></pre>\n<ol start=\"3\">\n<li><strong>Document Storage</strong></li>\n</ol>\n<pre><code class=\"language-sql\">CREATE TABLE documents (\n    id INTEGER PRIMARY KEY,\n    content JSON,\n    metadata JSON\n);\n</code></pre>\n<h2 id=\"portability-and-simplicity\">Portability and Simplicity</h2>\n<p>One of SQLite&#39;s strongest features is its <a href=\"https://www.sqlite.org/onefile.html\">single-file</a> nature. Your entire database exists in one file that can be:</p>\n<ul>\n<li>Backed up with a simple copy operation</li>\n<li>Easily version controlled (for smaller databases)</li>\n<li>Moved between systems effortlessly</li>\n<li>Examined with standard SQLite tools</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>While specialised databases have their place, SQLite offers a compelling combination of features that make it ideal for many AI engineering projects:</p>\n<ul>\n<li>Zero configuration</li>\n<li>Pre-installed availability</li>\n<li>Support for multiple data structures</li>\n<li>Single-file portability</li>\n<li>Wide language support, especially in Python and Go</li>\n<li>ACID[^3] compliance</li>\n</ul>\n<p><strong>TL;DR</strong>: When you need a lightweight, self-contained database that can handle documents, vectors, and graphs without the complexity of a full database server, SQLite is often an excellent choice.</p>\n<hr>\n<p>[^1]: Example from <a href=\"https://alexgarcia.xyz/sqlite-vec/python.html\">sqlite-vec with Python</a>\n[^2]: Examples from <a href=\"https://dev.to/stephenc222/how-to-build-lightweight-graphrag-with-sqlite-53le\">How to Build Lightweight GraphRAG with SQLite</a>\n[^3]: Atomicity, Consistency, Isolation, Durability (<a href=\"https://en.wikipedia.org/wiki/ACID\">ACID</a>), per Wikipedia, &quot;<em>is a set of properties of database transactions intended to guarantee data validity despite errors, power failures, and other mishaps. In the context of databases, a sequence of database operations that satisfies the ACID properties (which can be perceived as a single logical operation on the data) is called a transaction. For example, a transfer of funds from one bank account to another, even involving multiple changes such as debiting one account and crediting another, is a single transaction.</em>&quot;</p>\n"
  },
  {
    "title": "💡 TIL: To Prepare for AI, Study History's Tech Cycles",
    "date": "2025-02-09T00:00:00.000Z",
    "tags": [
      "til",
      "ai",
      "fast-ai",
      "llm",
      "machine-learning",
      "best-practices",
      "decision-making",
      "evolution"
    ],
    "url": "/posts/TIL-prepare-for-ai.html",
    "content": "<p><strong>TL;DR:</strong> Fast.ai founder Jeremy Howard advocates studying historical technology cycles rather than attempting to predict AI&#39;s future, recommending a practical preparation strategy that combines domain expertise with AI capabilities through self-directed learning, side projects, and community engagement - emphasising that success will come from embracing uncertainty whilst pursuing counter-cyclical opportunities.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p><a href=\"https://jeremy.fast.ai/\">Jeremy Howard</a> isn&#39;t just another voice in the AI conversation. As the creator of <a href=\"https://towardsdatascience.com/understanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664\">ULMFiT</a> (the algorithm that modern LLMs like ChatGPT are based on), founding researcher at <a href=\"https://course.fast.ai/\">fast.ai</a>, and <a href=\"https://www.answer.ai/\">Answer.AI</a>, Howard brings a unique perspective shaped by decades at the forefront of AI development. Recently, when <a href=\"https://xcancel.com/chrisbarber/status/1888037803566747942\">asked about preparing for AI</a>, his response wasn&#39;t about futuristic predictions or doomsday scenarios. Instead, he offered something more valuable: practical wisdom drawn from historical patterns.</p>\n<h2 id=\"why-this-matters-now\">Why This Matters Now</h2>\n<p>We&#39;re at a critical juncture with AI, similar to where we were with the internet in 1990. Just as the internet transformed every aspect of our lives, AI is poised to do the same. The difference? We can learn from history this time. Howard&#39;s insights are particularly valuable because they come from someone who has not only observed but shaped these technological transitions.</p>\n<h2 id=\"key-insights-on-technology-evolution\">Key Insights on Technology Evolution</h2>\n<p>Howard emphasises a crucial pattern: technology doesn&#39;t just grow linearly. Each innovation follows a &quot;hockey stick&quot; growth curve before flattening into a sigmoid. </p>\n<center>\n    <figure>\n        <img src=\"https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/0a6eced3bce4c70b7ba715fe7873d1659ce2e9a9/images/hockey-stick-growth.png\" width=\"80%\" height=\"80%\" />\n    <figcaption>Hockey stick growth</figcaption>\n    </figure>\n</center>\n\n<p>More importantly, new &quot;hockey sticks&quot; emerge unexpectedly in different areas. This pattern repeats &quot;like clockwork&quot; making historical understanding more valuable than future predictions.</p>\n<h2 id=\"practical-preparation-strategy\">Practical Preparation Strategy</h2>\n<p>Rather than trying to predict AI&#39;s future, Howard advocates for:</p>\n<ul>\n<li>Embracing uncertainty while avoiding both dismissive fear and blind hype</li>\n<li>Taking a counter-cyclical approach: pursuing opportunities others overlook</li>\n<li>Investing months in mastering AI tools, accepting initial poor results as part of the learning process</li>\n<li>Combining AI capabilities with deep domain expertise</li>\n<li>Building practical knowledge through side projects and community engagement</li>\n</ul>\n<h2 id=\"the-education-perspective\">The Education Perspective</h2>\n<p>Howard challenges traditional educational paths, suggesting alternatives:</p>\n<ul>\n<li>Self-directed learning through resources like <a href=\"https://course.fast.ai/\">fast.ai</a></li>\n<li>Multiple side hustles to build practical experience</li>\n<li>Community building with like-minded innovators</li>\n<li>Using AI itself to learn technical skills</li>\n<li>Developing both technical and human skills as a generalist</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The key takeaway isn&#39;t about predicting AI&#39;s future -it&#39;s about preparing for it intelligently. Howard&#39;s message is: success in the AI era won&#39;t come from perfect predictions or traditional career paths. Instead, it will come from practical engagement, continuous learning, and the ability to combine domain expertise with AI capabilities. As he puts it, those who master this combination will have &quot;superpowers&quot; compared to those who don&#39;t adapt.<br>The most valuable insight? Even AI experts can&#39;t predict AI&#39;s future reliably. The best strategy is to engage deeply with the technology while maintaining a grounded, practical approach to learning and application. The future belongs to the tinkerers, the experimenters, and those willing to learn from both past and present.</p>\n"
  },
  {
    "title": "🚀 A Minimal, Pragmatic Approach to Production-Ready AI & ML with Go",
    "date": "2025-01-26T00:00:00.000Z",
    "tags": [
      "ai",
      "go",
      "llm",
      "minimal",
      "machine-learning",
      "toolchain",
      "zero-config",
      "code-quality",
      "cross-platform",
      "production"
    ],
    "url": "/posts/go-pragmatic-modern-development.html",
    "content": "<p><strong>TL;DR:</strong> Go offers a refreshingly minimal approach to AI and ML development with its concise 47-page specification, zero-configuration toolchain, and functional equivalents to key Python ML libraries - providing explicit error handling, enforced code consistency, and cross-platform capabilities whilst reducing cognitive overhead and team friction in production environments.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Modern software development often involves navigating complex toolchains, opinionated frameworks, and resource-heavy development environments. Many languages require extensive configuration, multiple runtime dependencies, and introduce significant cognitive overhead through their vast feature sets and multiple approaches to solving the same problem. Node.js, JVM languages, and even Python with its extensive ecosystem can lead to analysis paralysis, code inhomogeneity and team disagreements over tooling and style.<br>Go offers a refreshing alternative. With a language specification under 50 pages, a consolidated toolchain, and a &quot;batteries included&quot; approach, it provides a low-cognitive-overhead solution for developers seeking simplicity and productivity. Its zero-config philosophy, coupled with built-in formatting (<code>go fmt</code>), linting[^1] (<code>go vet</code>), and testing tools, promotes code uniformity and reduces team friction over stylistic choices. The sizeable Go community is centralised, using Slack in this case, which serves as a focal point for communication, support, networking, and staying informed about the latest developments.<br>While Go may lack a REPL as sophisticated as IPython or the Julia interactive environment, this limitation encourages proper Test-Driven Development practices rather than the post-implementation testing often seen in REPL-heavy environments. Tools like <a href=\"https://github.com/fatih/vim-go\">vim-go</a>&#39;s <code>:GoRun</code> and Go Playground provide sufficient interactive development capabilities for most use cases.<br>Below I&#39;m collecting some thoughts on attractive aspects of Go I&#39;ve discerned so far and how they compare with other languages I&#39;ve considered. The list of Go&#39;s features is far from complete, for example I&#39;ve not mentioned goroutines among others. </p>\n<h2 id=\"python-vs-go-libraries-comparison\">Python vs Go Libraries Comparison</h2>\n<table>\n<thead>\n<tr>\n<th>Domain</th>\n<th>Python Library</th>\n<th>Go Equivalent</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Numerical Computing</td>\n<td><a href=\"https://github.com/numpy/numpy\">NumPy</a></td>\n<td><a href=\"https://github.com/gonum/gonum\">gonum</a></td>\n</tr>\n<tr>\n<td>Data Processing</td>\n<td><a href=\"https://github.com/pandas-dev/pandas\">Pandas</a></td>\n<td><a href=\"https://github.com/go-gota/gota\">gota</a></td>\n</tr>\n<tr>\n<td>Visualisation</td>\n<td><a href=\"https://github.com/plotly/plotly.py\">Plotly</a></td>\n<td><a href=\"https://github.com/MetalBlueberry/go-plotly\">go-plotly</a></td>\n</tr>\n<tr>\n<td>Gradient Boosting</td>\n<td><a href=\"https://github.com/dmlc/xgboost\">XGBoost</a></td>\n<td><a href=\"https://github.com/Unity-Technologies/go-xgboost\">go-xgboost</a></td>\n</tr>\n<tr>\n<td>Machine Learning</td>\n<td><a href=\"https://github.com/scikit-learn/scikit-learn\">Scikit-Learn</a></td>\n<td><a href=\"https://github.com/sjwhitworth/golearn\">golearn</a></td>\n</tr>\n<tr>\n<td>Deep Learning</td>\n<td><a href=\"https://github.com/tensorflow/tensorflow\">TensorFlow</a><br><a href=\"https://github.com/pytorch/pytorch\">PyTorch</a></td>\n<td><a href=\"https://github.com/galeone/tfgo\">tfgo</a><br><a href=\"https://github.com/sugarme/gotch\">gotch</a></td>\n</tr>\n<tr>\n<td>LLM Development</td>\n<td><a href=\"https://github.com/langchain-ai/langchain\">LangChain</a></td>\n<td><a href=\"https://github.com/tmc/langchaingo\">langchaingo</a></td>\n</tr>\n<tr>\n<td>Vector Search</td>\n<td><a href=\"https://github.com/weaviate/weaviate-python-client\">Weaviate Client</a></td>\n<td><a href=\"https://github.com/weaviate/weaviate-python-client\">Weaviate Go Client</a></td>\n</tr>\n</tbody></table>\n<p><em>Update: <a href=\"https://github.com/Promacanthus/awesome-golang-ai\">Awesome Golang.ai</a> is a very nice curated list of AI-related Go libraries worth checking.</em></p>\n<h2 id=\"development-experience\">Development Experience</h2>\n<p>Go&#39;s tooling is exceptional. With <a href=\"https://github.com/fatih/vim-go\">vim-go</a> in <a href=\"https://neovim.io/\">Neovim</a>, you get immediate access to formatting, linting, and code navigation. Unlike JVM languages or JavaScript frameworks that may require more complex build configurations, Go projects maintain a simple, predictable structure thanks to <code>go mod</code>. The <code>go fmt</code> command -triggered on save by default- enforces consistent code style eliminating debates over formatting and best practices, while <code>go vet</code> catches common mistakes early.</p>\n<h2 id=\"error-handling-done-right\">Error Handling Done Right</h2>\n<p>Go&#39;s approach to error handling initially feels verbose:</p>\n<pre><code class=\"language-go\">result, err := someFunction()\nif err != nil {\n    return err\n}\n</code></pre>\n<p>But this explicitness pays dividends. By treating errors as values that must be handled, Go forces developers to think about failure cases upfront. The <code>defer</code> keyword complements this by ensuring clean-up code runs regardless of errors:</p>\n<pre><code class=\"language-go\">file, err := os.Open(&quot;data.txt&quot;)\nif err != nil {\n    return err\n}\ndefer file.Close()\n</code></pre>\n<h2 id=\"mlai-capabilities\">ML/AI Capabilities</h2>\n<p>While Go isn&#39;t the primary choice for ML/AI experimentation, its simplicity and performance make it excellent for production deployments. Its standard library and growing ecosystem provide solid foundations for numerical computing (<a href=\"https://github.com/gonum/gonum\">gonum</a>), data processing (<a href=\"https://github.com/go-gota/gota\">gota</a>), and ML/AI applications (<a href=\"https://github.com/gorgonia/gorgonia\">Gorgonia</a>, <a href=\"https://github.com/galeone/tfgo\">tfgo</a>, <a href=\"https://github.com/sugarme/gotch\">gotch</a>). The language&#39;s focus on simplicity and performance makes it particularly suitable for model serving and inference workloads.</p>\n<h2 id=\"language-design\">Language Design</h2>\n<p>Go&#39;s refreshingly concise specification (under 50 pages) contrasts sharply with other languages. Even the highly promising Zig, a younger language half of Go&#39;s age, has a 74-page specification despite being positioned as a simpler low-level language. </p>\n<figure>\n    <img src=\"https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/refs/heads/main/images/Zig%20language%20spec.png\" width=\"80%\" height=\"80%\"/>\n    <figcaption>Zig's language spec</figcaption>\n</figure>\n\n<p>Go&#39;s intentionally limited feature set and single way of solving problems promote maintainable, uniform code that&#39;s easier to reason about and review, as reflected in its compact language spec.</p>\n<figure>\n    <img src=\"https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/refs/heads/main/images/Go%20language%20spec.png\" width=\"80%\" height=\"80%\"/>\n    <figcaption>Go's language spec</figcaption>\n</figure>\n\n<p>For ML engineers and developers seeking a reliable, low-overhead language that excels at building robust, production-ready applications, Go offers a compelling choice. While it won&#39;t replace Python for rapid prototyping and research, its simplicity, performance, and consolidated toolchain make it an very compelling addition to any developer&#39;s toolkit.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>To my eyes, Go stands out as a pragmatic choice for modern development through its key strengths:</p>\n<ul>\n<li>Minimal cognitive overhead with a 47-page specification</li>\n<li>Zero-config toolchain including formatting, testing, and package management</li>\n<li>Centralised community, providing a single-source of truth </li>\n<li>Enforced error handling and clean resource management via <code>defer</code></li>\n<li>Growing ML/AI ecosystem comparable to Python&#39;s established libraries</li>\n<li>Cross-platform compilation and efficient garbage collection</li>\n<li>Single, clear way to solve problems, reducing team friction</li>\n<li>Lightweight development environment compared to JVM, .NET, BEAM or Node.js</li>\n</ul>\n<p>While Python remains dominant for ML/AI research, prototyping and -frequently- production, Go excels in production environments where code maintainability, performance, and team collaboration are crucial. Its intentionally limited feature set, combined with a comprehensive standard library and maturing ML ecosystem, makes it a very attractive choice for developers seeking simplicity without sacrificing capability.<br>The language&#39;s design philosophy strongly aligns with my needs as a Data professional looking to reduce tooling complexity and maintain consistent, reliable codebases. Go&#39;s lightweight yet rich toolchain allows writing safe, efficient AI and data-oriented code based on simplicity and reliability. This refreshing alternative in today&#39;s complex development landscape has strongly tempted me to start moving my practice to Go&#39;s more principled approach.</p>\n<hr>\n<p>[^1]: Vet is -in essence- a linter, since it helps improve code quality. Quoting Go&#39;s <a href=\"https://go.dev/src/cmd/vet/doc.go\">vet doc</a> <em>&quot;Vet examines Go source code and reports suspicious constructs, such as Printf calls whose arguments do not align with the format string. Vet uses heuristics that do not guarantee all reports are genuine problems, but it can find errors not caught by the compilers.&quot;</em></p>\n"
  },
  {
    "title": "🔧 A 5-Minute Guide to Engineering Machine Learning Systems",
    "date": "2025-01-21T00:00:00.000Z",
    "tags": [
      "machine-learning",
      "best-practices",
      "mlops",
      "monitoring",
      "production",
      "quality-assurance",
      "data-science",
      "decision-making"
    ],
    "url": "/posts/ml-best-practices.html",
    "content": "<p><strong>TL;DR:</strong> This concise guide distils Google&#39;s 43 machine learning best practices into essential principles across four phases: starting with simple heuristics before ML, building robust data pipelines, prioritising feature engineering over complex algorithms, and gradually introducing complexity only after monitoring systems are established - emphasising engineering excellence over ML expertise.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>This is a concise reference guide distilling Martin Zinkevich&#39;s <a href=\"https://developers.google.com/machine-learning/guides/rules-of-ml\">influential Google article on machine learning best practices</a>. While the original spans 43 detailed rules, this 10-minute summary captures the essential principles for building production ML systems. Whether you&#39;re starting a new project or reviewing an existing one, this summary can be used as a practical checklist for engineering-focused machine learning.</p>\n<h2 id=\"core-philosophy\">Core Philosophy</h2>\n<blockquote>\n<p>Do machine learning like the great engineer you are, not like the great machine learning expert you aren&#39;t.</p>\n</blockquote>\n<p>Most ML gains come from great features, not algorithms. The basic approach should be:</p>\n<ol>\n<li>Ensure solid end-to-end pipeline</li>\n<li>Start with reasonable objective</li>\n<li>Add common-sense features simply</li>\n<li>Maintain pipeline integrity</li>\n</ol>\n<h2 id=\"phase-i-before-machine-learning-rules-1-3\">Phase I: Before Machine Learning (Rules #1-3)</h2>\n<ol>\n<li><p><strong>Don&#39;t be afraid to launch without ML</strong></p>\n<ul>\n<li>Simple heuristics get you 50% of the way</li>\n<li>Launch with heuristics when data is insufficient</li>\n<li>Example: Use install rate for app ranking</li>\n</ul>\n</li>\n<li><p><strong>First, design and implement metrics</strong></p>\n<ul>\n<li>Track everything possible in current system</li>\n<li>Get early permission from users</li>\n<li>Design systems with metric instrumentation</li>\n<li>Implement experiment framework</li>\n</ul>\n</li>\n<li><p><strong>Choose ML over complex heuristics</strong></p>\n<ul>\n<li>Simple heuristics for launching</li>\n<li>Complex heuristics become unmaintainable</li>\n<li>ML models are easier to maintain long-term</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"phase-ii-first-pipeline-rules-4-11\">Phase II: First Pipeline (Rules #4-11)</h2>\n<ol>\n<li><p><strong>Keep first model simple, get infrastructure right</strong></p>\n<ul>\n<li>Focus on data pipeline integrity</li>\n<li>Define clear evaluation metrics</li>\n<li>Plan model integration carefully</li>\n</ul>\n</li>\n<li><p><strong>Pipeline Health is Critical</strong></p>\n<ul>\n<li>Test infrastructure independently</li>\n<li>Monitor freshness requirements</li>\n<li>Watch for silent failures</li>\n<li>Give feature columns owners</li>\n<li>Document feature expectations</li>\n</ul>\n</li>\n<li><p><strong>Starting Your ML System</strong></p>\n<ul>\n<li>Test getting data into algorithm</li>\n<li>Test getting models out correctly</li>\n<li>Monitor data statistics continuously</li>\n<li>Build alerting system</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"your-first-objective-rules-12-15\">Your First Objective (Rules #12-15)</h2>\n<ol>\n<li><p><strong>Choose Objectives Wisely</strong></p>\n<ul>\n<li>Don&#39;t overthink initial objective choice</li>\n<li>Start with simple, observable metrics</li>\n<li>Use directly observed user behaviours</li>\n<li>Example: clicks, downloads, shares</li>\n</ul>\n</li>\n<li><p><strong>Model Selection Guidelines</strong></p>\n<ul>\n<li>Start with interpretable models</li>\n<li>Separate spam filtering from quality ranking</li>\n<li>Use simple linear models initially</li>\n<li>Make debugging easier</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"phase-iii-feature-engineering-rules-16-22\">Phase III: Feature Engineering (Rules #16-22)</h2>\n<ol>\n<li><p><strong>Plan to launch and iterate</strong></p>\n<ul>\n<li>Expect regular model updates</li>\n<li>Design for feature flexibility</li>\n<li>Keep infrastructure clean</li>\n</ul>\n</li>\n<li><p><strong>Feature Engineering Principles</strong></p>\n<ul>\n<li>Start with directly observed features</li>\n<li>Use cross-product features wisely</li>\n<li>Clean up unused features</li>\n<li>Scale feature complexity with data</li>\n</ul>\n</li>\n<li><p><strong>Feature Coverage and Quality</strong></p>\n<ul>\n<li>Features that generalise across contexts</li>\n<li>Monitor feature coverage</li>\n<li>Document feature ownership</li>\n<li>Regular feature clean-up</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"human-analysis-rules-23-28\">Human Analysis (Rules #23-28)</h2>\n<ol>\n<li><p><strong>Testing and Validation</strong></p>\n<ul>\n<li>Use crowdsourcing or live experiments</li>\n<li>Measure model deltas explicitly</li>\n<li>Look for error patterns</li>\n<li>Consider long-term effects</li>\n</ul>\n</li>\n<li><p><strong>Common Pitfalls</strong></p>\n<ul>\n<li>Engineers aren&#39;t typical users</li>\n<li>Beware of confirmation bias</li>\n<li>Quantify undesirable behaviours</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"training-serving-skew-rules-29-37\">Training-Serving Skew (Rules #29-37)</h2>\n<ol>\n<li><p><strong>Prevent Skew</strong></p>\n<ul>\n<li>Save serving-time features</li>\n<li>Weight sampled data properly</li>\n<li>Reuse code between training/serving</li>\n<li>Test on future data</li>\n</ul>\n</li>\n<li><p><strong>Monitor Everything</strong></p>\n<ul>\n<li>Track performance metrics</li>\n<li>Watch data distributions</li>\n<li>Monitor feature coverage</li>\n<li>Check prediction bias</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"phase-iv-optimisation-and-complex-models-rules-38-43\">Phase IV: Optimisation and Complex Models (Rules #38-43)</h2>\n<ol>\n<li><p><strong>When to Add Complexity</strong></p>\n<ul>\n<li>After simple approaches plateau</li>\n<li>When objectives are well-aligned</li>\n<li>If maintenance cost justifies gains</li>\n</ul>\n</li>\n<li><p><strong>Advanced Techniques</strong></p>\n<ul>\n<li>Keep ensembles simple</li>\n<li>Look for new information sources</li>\n<li>Balance complexity vs. benefits</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"final-recommendations\">Final Recommendations</h2>\n<ol>\n<li><p><strong>Launch Decisions</strong></p>\n<ul>\n<li>Consider multiple metrics</li>\n<li>Use proxies for long-term goals</li>\n<li>Balance simple vs. complex</li>\n</ul>\n</li>\n<li><p><strong>System Evolution</strong></p>\n<ul>\n<li>Start simple, add complexity gradually</li>\n<li>Monitor consistently</li>\n<li>Keep infrastructure clean</li>\n<li>Document everything</li>\n</ul>\n</li>\n</ol>\n"
  },
  {
    "title": "🤖 Understanding AI Agents: Tools, Planning, and Evaluation",
    "date": "2025-01-14T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "prompt-engineering",
      "system-prompts",
      "evaluation",
      "best-practices",
      "toolchain",
      "machine-learning"
    ],
    "url": "/posts/agents-chip-huyen.html",
    "content": "<p><strong>TL;DR:</strong> Chip Huyen&#39;s analysis of AI agents explores how they combine foundation models with specialised tools (knowledge augmentation, capability extension, and write actions), planning mechanisms (ReAct, Reflexion), and evaluation frameworks to accomplish complex tasks whilst highlighting challenges in tool selection, planning efficiency, and error management.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>This article summarises Chip Huyen&#39;s comprehensive blog post &quot;<a href=\"https://huyenchip.com//2025/01/07/agents.html\">Agents</a>&quot; adapted from her upcoming book AI Engineering (2025). The original piece provides an in-depth examination of intelligent agents, which represent a fundamental concept in AI, defined by Russell and Norvig in their seminal 1995 book <a href=\"https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach\">Artificial Intelligence: A Modern Approach</a> as anything that can perceive its environment through sensors and act upon it through actuators. Huyen explores how the unprecedented capabilities of foundational models have transformed theoretical possibilities into practical applications, enabling agents to operate in diverse environments -from digital workspaces for coding to physical settings for robotics. These agents can now assist with tasks ranging from website creation to complex negotiations.</p>\n<h2 id=\"understanding-agents-and-their-tools\">Understanding Agents and Their Tools</h2>\n<p>An agent&#39;s effectiveness is determined by two key factors: its environment and its tool inventory. The environment defines the scope of possible actions, while tools enable the agent to perceive and act within this environment. Modern agents leverage three distinct categories of tools.<br>Knowledge augmentation tools, including text retrievers and web browsing capabilities, prevent model staleness by enabling access to current information. However, web browsing tools require careful API selection to protect against unreliable or harmful content. Capability extension tools address inherent model limitations -for instance, providing calculators for precise arithmetic or code interpreters for programming tasks. These interpreters demand robust security measures to prevent code injection attacks.<br>Write actions represent the most powerful and potentially risky category, enabling agents to modify databases or send emails. These tools are distinguished from read-only actions by their ability to affect the environment directly. The <a href=\"https://arxiv.org/abs/2304.09842\">Chameleon</a> system demonstrated the power of tool augmentation, achieving an 11.37% improvement on ScienceQA (a science question answering task) and 17% on TabMWP (a tabular math problem-solving task) through strategic tool combination.</p>\n<center>\n    <figure>\n           <a href=\"https://huyenchip.com//2025/01/07/agents.html\"><img src=\"https://huyenchip.com/assets/pics/agents/8-tool-transition.png\" width=\"80%\" height=\"80%\"/></a>\n        <figcaption>A tool transition tree by Chameleon</figcaption>\n    </figure>\n</center>\n\n\n<h2 id=\"planning-and-execution-strategies\">Planning and Execution Strategies</h2>\n<p>Effective planning requires balancing granularity and flexibility. While <a href=\"https://arxiv.org/abs/2302.04761\">Toolformer</a> managed with 5 tools and <a href=\"https://arxiv.org/abs/2304.09842\">Chameleon</a> with 13, <a href=\"https://arxiv.org/abs/2305.15334\">Gorilla</a> attempted to handle 1,645 APIs, illustrating the complexity of tool selection. Plans can be expressed either in natural language or specific function calls, each approach offering different advantages in maintainability and precision.<br>Foundational Model planners require minimal training but need careful prompting, while Reinforcement Learning planners demand extensive training for robustness. Modern planning systems support multiple control flows: sequential, parallel, conditional, and iterative patterns. The <a href=\"https://arxiv.org/abs/2210.03629\">ReAct</a> framework successfully combines reasoning with action, </p>\n<center>\n    <figure>\n        <a href=\"https://huyenchip.com//2025/01/07/agents.html\"><img src=\"https://huyenchip.com/assets/pics/agents/5-ReAct.png\" width=\"80%\" height=\"80%\"/></a>\n        <figcaption>ReAct agent</figcaption>\n    </figure>\n</center>\n\n\n<p>while <a href=\"https://arxiv.org/abs/2303.11366\">Reflexion</a> separates evaluation and self-reflection for improved performance.</p>\n<center>\n    <figure>\n        <a href=\"https://huyenchip.com//2025/01/07/agents.html\"><img src=\"https://huyenchip.com/assets/pics/agents/6-reflexion.png\" width=\"80%\" height=\"80%\"/></a>\n        <figcaption>Reflexion agent</figcaption>\n    </figure>\n</center>\n\n<h2 id=\"reflection-and-error-management\">Reflection and Error Management</h2>\n<p>Continuous reflection and error correction form the backbone of reliable agent systems. The process begins with query validation, continues through plan assessment, and extends to execution monitoring. Chameleon&#39;s tool transition analysis shows how tools are commonly used together, while Voyager&#39;s skill manager builds on this by tracking and reusing successful tool combinations.</p>\n<h2 id=\"evaluation-framework\">Evaluation Framework</h2>\n<p>Agent evaluation requires a comprehensive approach to failure mode analysis. Planning failures might involve invalid tools or incorrect parameters, while tool-specific failures demand targeted analysis. Efficiency metrics must consider not just step count and costs, but also completion time constraints. When comparing AI and human agents, it&#39;s essential to recognise their different operational patterns -what&#39;s efficient for one may be inefficient for the other. Working with domain experts helps identify missing tools and validate performance metrics.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Huyen&#39;s analysis demonstrates that successful AI agents emerge from the careful orchestration of three key elements: strategic tool selection, sophisticated planning mechanisms, and robust evaluation frameworks. While tools dramatically enhance agent capabilities -as evidenced by Chameleon&#39;s significant performance improvements- their effectiveness depends on thoughtful curation, balancing between Toolformer&#39;s minimal approach and Gorilla&#39;s extensive API integration. The integration of planning frameworks like ReAct and Reflexion shows how combining reasoning with action and incorporating systematic reflection can enhance agent performance. However, as an emerging field without established theoretical frameworks, significant challenges remain in tool selection, planning efficiency, and error management. Future developments will focus on agent framework evaluation and memory systems for handling information beyond context limits, while maintaining the delicate balance between capability and control that Huyen emphasises throughout her analysis.</p>\n"
  },
  {
    "title": "💡 TIL: A Simple Yet Effective Ensemble Technique called Model Soup 🍲",
    "date": "2025-01-10T00:00:00.000Z",
    "tags": [
      "neural-network",
      "machine-learning",
      "performance",
      "mlops",
      "production",
      "evaluation"
    ],
    "url": "/posts/TIL-model-soups.html",
    "content": "<p><strong>TL;DR:</strong> Model soups provide a computationally efficient ensemble technique by averaging the weights of similarly trained neural networks, outperforming both individual models and traditional prediction-averaging ensembles while maintaining single-model inference speed.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>While most ensemble methods in machine learning combine model predictions, thanks to <a href=\"https://bsky.app/profile/chrisalbon.com/post/3lfbbixka7c25\">Chris Albon</a> I recently learned about an alternative approach called &quot;<em>model soups</em>&quot; that works directly with model parameters. Instead of aggregating outputs, model soups blend the actual weights and biases of neural networks, showing promising results in computer vision and language tasks.</p>\n<center>\n   <a href=\"https://bsky.app/profile/chrisalbon.com/post/3lfbbixka7c25\"><img src=\"https://cdn.bsky.app/img/feed_fullsize/plain/did:plc:umpsiyampiq3bpgce7kigydz/bafkreihvr4b4gid7v6y7karhiusawtqfdbhoen2bt6q55pmugyioj3q3gq@jpeg\" width=\"80%\" height=\"80%\"/></a>\n</center>\n\n<h2 id=\"main-concept\">Main Concept</h2>\n<p>Model soups are created by averaging the parameters (weights and biases) of multiple independently trained neural networks that share the same architecture and training setup. For example, if we have three models with weights 2.32, 4.21, and 1.23 for a particular parameter, the &quot;souped&quot; model would use (2.32 + 4.21 + 1.23) / 3 = 2.587 for that parameter. This process is repeated across all parameters in the network. However, not all parameter combinations lead to improvements -models typically need similar training datasets, optimisation methods, and hyperparameters (like learning rate and batch size) to blend effectively. When done right, parameter-averaged models can outperform both individual networks and traditional prediction-averaging ensembles, while maintaining the inference speed of a single model.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Model soups challenge our intuitions about neural networks by showing that directly averaging weights can produce better results than averaging predictions. While the technique requires careful consideration of training conditions, it provides a computationally efficient way to combine multiple models into a single network, making it particularly valuable for resource-constrained production environments where running multiple models in parallel isn&#39;t feasible.</p>\n"
  },
  {
    "title": "🔍 Understanding LLM Interpretability",
    "date": "2025-01-09T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "machine-learning",
      "neural-network",
      "model-governance",
      "interpretability"
    ],
    "url": "/posts/interpreting-llms.html",
    "content": "<p><strong>TL;DR:</strong> LLMs present unique interpretability challenges due to neurons exhibiting polysemanticity - responding to multiple unrelated concepts through superposition - which sparse autoencoders help address by mapping neuron combinations to specific concepts, enhancing our ability to understand, control, and improve these increasingly influential AI systems.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Large Language Models (LLMs) have become increasingly sophisticated, yet understanding their inner workings remains a critical challenge for AI safety and development. This blog post summarises concepts and research presented in <a href=\"https://www.youtube.com/watch?v=UGO_Ehywuxc\">Welch Labs&#39; video on mechanistic interpretability</a>, examining how LLMs process information and recent advances in making their decision-making processes more transparent.  </p>\n<h2 id=\"how-llms-think\">How LLMs Think</h2>\n<p>LLMs process text through a sophisticated pipeline:</p>\n<ol>\n<li>Text is converted into tokens and mapped to vectors</li>\n<li>These vectors flow through multiple layers via &quot;<em>residual streams</em>&quot;</li>\n<li>Each layer transforms the information through attention mechanisms</li>\n<li>Final outputs emerge from probability distributions across possible tokens</li>\n</ol>\n<p>This process, while mathematically precise, creates a black box of neural connections that resist simple interpretation.</p>\n<h2 id=\"the-challenge-of-model-transparency\">The Challenge of Model Transparency</h2>\n<p><a href=\"https://ai.google.dev/gemma\">Google Gemma</a> models&#39; analysis of the sentence &quot;<em>the reliability of Wikipedia is very</em>&quot; demonstrates this complexity. The model assigns varying probabilities to different completions:</p>\n<ul>\n<li>&quot;<em>important</em>&quot; (20.21%)</li>\n<li>&quot;<em>high</em>&quot; (11.16%)</li>\n<li>&quot;<em>questionable</em>&quot; (9.48%)</li>\n</ul>\n<p>These probabilities emerge from intricate interactions between neurons, leading to a phenomenon called <em>superposition</em>[^1].</p>\n<h2 id=\"superposition-and-its-solution\">Superposition and Its Solution</h2>\n<p>Unlike vision models where neurons correspond to specific concepts, LLMs exhibit <a href=\"https://arxiv.org/abs/2210.01892\">polysemanticity</a> -individual neurons respond to multiple, unrelated concepts. This occurs because LLMs encode more concepts than available neurons by using specific neuron combinations.</p>\n<p>This complexity necessitated the development of [sparse autoencoders]({{ site.baseurl }}{% link _posts/2025-01-09-sparse-autoencoders.md %}), which:</p>\n<ol>\n<li>Map complex neuron combinations to specific concepts</li>\n<li>Extract interpretable features from LLMs</li>\n<li>Enable direct manipulation of model behaviour</li>\n</ol>\n<h2 id=\"practical-implications\">Practical Implications</h2>\n<p>Understanding LLM internals has crucial implications:</p>\n<ul>\n<li><strong>AI Safety</strong>: Better control over model behaviours and outputs</li>\n<li><strong>Development</strong>: More targeted improvements in model capabilities</li>\n<li><strong>Deployment</strong>: Enhanced ability to predict and prevent unwanted behaviours</li>\n<li><strong>Trust</strong>: Greater transparency in AI decision-making processes</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>While tools like sparse autoencoders have provided unprecedented insights into model behaviour, they&#39;ve also revealed the vast complexity of LLM internal mechanisms -the &quot;dark matter&quot; of AI. As these models become more integral to society, advancing our ability to interpret and control them becomes increasingly critical for responsible AI development.<br>This improved understanding represents not just academic progress, but a crucial step toward safer, more reliable AI systems.</p>\n<hr>\n<p>[^1]: superposition in the context of neural networks is the ability of a single neuron to represent multiple features simultaneously.  <a href=\"https://hdl.handle.net/1721.1/157073\">https://hdl.handle.net/1721.1/157073</a></p>\n"
  },
  {
    "title": "📐 Sparse Autoencoders: A Technical Overview",
    "date": "2025-01-09T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "neural-network",
      "machine-learning",
      "data-science",
      "linear-algebra",
      "statistics",
      "evaluation",
      "interpretability",
      "modelling-mindsets",
      "design-principles",
      "best-practices",
      "data-processing"
    ],
    "url": "/posts/sparse-autoencoders.html",
    "content": "<p><strong>TL;DR:</strong> Sparse autoencoders are neural networks that learn efficient data representations by reconstructing their input while enforcing neuron inactivity constraints, combining reconstruction error, weight decay, and KL-divergence sparsity penalties to automatically extract interpretable features without manual engineering.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Supervised learning has achieved remarkable successes in areas ranging from computer vision to genomics. However, as Andrew Ng points out in his <a href=\"https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf\">CS294A lecture notes</a>, it faces a fundamental limitation: the need for manually engineered features. While researchers have spent years crafting specialised features for vision, audio, and text processing, this approach neither scales nor generalises well.\nSparse autoencoders offer an elegant solution to this challenge by automatically learning features from unlabelled data. These neural networks are distinguished by two key characteristics:  </p>\n<ol>\n<li>They attempt to reconstruct their input, forcing them to capture essential data patterns  </li>\n<li>They employ a sparsity constraint that mimics biological neural systems, where neurons fire infrequently and selectively</li>\n</ol>\n<p>While simple implementations may not outperform hand-engineered features in specific domains like computer vision, their strength lies in their generality and biological plausibility. The sparse coding principle has proven effective across diverse domains including audio, text, and visual processing.<br>The mathematical framework combines reconstruction error, regularisation, and sparsity penalties to learn efficient, interpretable representations. This approach not only advances machine learning capabilities but also provides insights into how biological neural networks might learn and process information.\nThis overview examines the mathematical foundations, practical implementation, and emergent properties of sparse autoencoders, following the framework presented in Stanford&#39;s CS294A course notes.</p>\n<h2 id=\"sparse-autoencoders\">Sparse Autoencoders</h2>\n<p>An autoencoder is a neural network that learns to reconstruct its input. In a sparse autoencoder, we add a critical biological constraint: neurons should be &quot;inactive&quot; most of the time, mimicking how biological neurons exhibit low average firing rates.<br>The basic architecture is:  </p>\n<pre><code>Input (x) -&gt; Hidden Layer (sparse activation) -&gt; Output (x̂)\n</code></pre>\n<p>Where:</p>\n<ul>\n<li>Input and output dimensions are equal $(x, \\hat{x} \\in \\R^n)$</li>\n<li>Hidden layer learns a sparse representation</li>\n<li>Network uses sigmoid activation: $f(z) = \\frac{1}{1+e^{-z}}$</li>\n</ul>\n<h2 id=\"mathematical-framework\">Mathematical Framework</h2>\n<ol>\n<li><p><strong>Base Cost Function</strong> (single training example):  </p>\n<p> $$ \n J(W,b; x,y) = \\frac{1}{2}||h_{W,b}(x) - y||^2 \n $$  </p>\n<p> For a single training example:<br> - Measures reconstruction error between network output $h_{W,b}(x)$ and target $y$<br> - For autoencoders: $y = x$ (we reconstruct the input)<br> - $\\frac{1}{2}$ factor simplifies gradient computations<br> - Squared L2 norm penalises larger reconstruction errors quadratically  </p>\n</li>\n<li><p><strong>Full Cost Function with Weight Decay</strong>:  </p>\n<p> The cost function $J(W,b)$ combines the average reconstruction error<br> $\\frac{1}{m}\\sum_{i=1}^m \\frac{1}{2}||h_{W,b}(x^{(i)}) - x^{(i)}||^2$</p>\n<p> with the weight decay regularisation, to prevent overfitting by penalising large weights:<br> $\\frac{\\lambda}{2}\\sum_{l=1}^{n_l-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(W_{ji}^{(l)})^2$  </p>\n<p> $$ \n J(W,b) = \\left[\\frac{1}{m}\\sum_{i=1}^m \\frac{1}{2}||h_{W,b}(x^{(i)}) - y^{(i)}||^2\\right] + \\frac{\\lambda}{2}\\sum_{l=1}^{n_l-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(W_{ji}^{(l)})^2 \n $$  </p>\n<p> Key points:  </p>\n<ul>\n<li>For autoencoders, output $y^{(i)}$ equals input $x^{(i)}$  </li>\n<li>Weight decay applies only to weights $W$, not biases $b$  </li>\n<li>$\\lambda$ balances reconstruction accuracy vs. weight magnitude  </li>\n<li>The $\\frac{1}{2}$ factor simplifies derivative calculations in backpropagation  </li>\n<li>This regularisation is distinct from the sparsity constraint (KL divergence term)</li>\n</ul>\n</li>\n<li><p><strong>Sparsity Measurement</strong>:  </p>\n<p> The average activation $\\hat{\\rho}_j$ measures how frequently hidden unit $j$ fires across the training set:  </p>\n<p> $$ \n \\hat{\\rho}<em>j = \\frac{1}{m}\\sum</em>{i=1}^m[a_j^{(2)}(x^{(i)})] \n $$  </p>\n<p> Key points:  </p>\n<ul>\n<li>$a_j^{(2)}(x^{(i)})$ is hidden unit $j$&#39;s activation for input $x^{(i)}$  </li>\n<li>With sigmoid activation:  <ul>\n<li>Values near 1 mean &quot;active&quot; or &quot;firing&quot;  </li>\n<li>Values near 0 mean &quot;inactive&quot;</li>\n</ul>\n</li>\n<li>We constrain $\\hat{\\rho}_j \\approx \\rho$ where $\\rho$ is small (typically 0.05)  </li>\n<li>This enforces selective firing: each neuron responds strongly to specific input patterns</li>\n</ul>\n</li>\n<li><p><strong>Sparsity Penalty</strong> (using <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\">KL divergence</a>):  </p>\n<p> The sparsity penalty uses KL divergence to enforce $$\\hat{\\rho}_j \\approx \\rho$$:  </p>\n<p> $$ \n \\sum_{j=1}^{s_2}\\rho\\log\\frac{\\rho}{\\hat{\\rho}_j} + (1-\\rho)\\log\\frac{1-\\rho}{1-\\hat{\\rho}_j} \n $$  </p>\n<p> Properties of this penalty:  </p>\n<ul>\n<li>Minimised (zero) when $\\hat{\\rho}_j = \\rho$  </li>\n<li>Monotonically increases as $\\hat{\\rho}_j$ deviates from $\\rho$  </li>\n<li>Becomes infinite as $\\hat{\\rho}_j$ approaches 0 or 1</li>\n</ul>\n</li>\n<li><p><strong>Final Cost Function</strong>:  </p>\n<p> $$ \n J_{sparse}(W,b) = J(W,b) + \\beta\\sum_{j=1}^{s_2}KL(\\rho||\\hat{\\rho}_j) \n $$  </p>\n<p> Components:  </p>\n<ul>\n<li>$J(W,b)$: Standard autoencoder cost (reconstruction error + weight decay)  </li>\n<li>Sparsity term: KL divergence penalty summed over $s_2$ hidden units</li>\n</ul>\n<p> $\\beta$ controls:  </p>\n<ul>\n<li>Balance between accurate reconstruction and sparse representation  </li>\n<li>Strength of sparsity enforcement  </li>\n<li>Higher $\\beta$ → stronger sparsity constraint</li>\n</ul>\n<p> This formulation naturally penalises both over- and under-activation of hidden units relative to target sparsity $\\rho$.</p>\n</li>\n</ol>\n<h2 id=\"training-process\">Training Process</h2>\n<p>The key modification to standard backpropagation occurs in the hidden layer:  </p>\n<p>$$ \n\\delta_i^{(2)} = \\left(\\sum_{j=1}^{s_3}W_{ji}^{(3)}\\delta_j^{(3)}\\right)f&#39;(s_i^{(2)}) + \\beta\\left(-\\frac{\\rho}{\\hat{\\rho}_i} + \\frac{1-\\rho}{1-\\hat{\\rho}_i}\\right) \n$$  </p>\n<p>Where:</p>\n<ul>\n<li>First term: Standard backpropagation gradient through the network</li>\n<li>Second term: Gradient of KL-divergence sparsity penalty</li>\n<li>$s_i^{(2)}$ is weighted input sum to hidden unit $i$</li>\n<li>$\\hat{\\rho}_i$ must be pre-computed using full training set</li>\n</ul>\n<p>This modification ensures gradient descent optimises both reconstruction accuracy and sparsity.</p>\n<h2 id=\"practical-guidelines\">Practical Guidelines</h2>\n<ul>\n<li>$\\rho$ ≈ 0.05 (5% target activation rate)</li>\n<li>$\\beta$ controls sparsity penalty strength</li>\n<li>Initialise weights randomly near zero</li>\n<li>Must compute forward pass on all examples first to calculate $\\hat{\\rho}$</li>\n</ul>\n<h2 id=\"results\">Results</h2>\n<p>When trained on images, the network naturally learns edge detectors at different orientations, similar to what is found in the visual cortex. This emergence of biologically plausible features validates the sparsity approach.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Sparse autoencoders represent a mathematically principled approach to unsupervised feature learning, combining biological inspiration with rigorous optimisation techniques. Their key innovation lies in the sparsity constraint, implemented through KL divergence, which forces hidden units to develop specialised, interpretable features.</p>\n<p>The mathematical framework achieves this through three key components:</p>\n<ol>\n<li>A reconstruction cost that ensures faithful data representation</li>\n<li>A weight decay term that prevents overfitting</li>\n<li>A sparsity penalty that enforces selective neural activation</li>\n</ol>\n<p>This formulation has proven successful in practice, typically leading to:</p>\n<ul>\n<li>Edge and feature detectors emerging naturally from visual data</li>\n<li>Interpretable representations comparable to biological neural coding</li>\n<li>Robust feature learning even with <a href=\"https://en.wikipedia.org/wiki/Overcompleteness\">overcomplete</a> hidden layers</li>\n</ul>\n<p>The practical value of sparse autoencoders extends beyond their theoretical elegance -they provide a foundation for understanding how neural networks can learn meaningful data representations without supervision. Their success in learning biologically plausible features validates both their design principles and their potential for advanced machine learning applications. Their main limitation lies in hyperparameter sensitivity, particularly to the sparsity target ρ and weight β, requiring careful tuning for optimal performance.</p>\n"
  },
  {
    "title": "💡 TIL: How Different Societies View and Value Choice",
    "date": "2025-01-08T00:00:00.000Z",
    "tags": [
      "til",
      "decision-making",
      "best-practices",
      "evaluation",
      "statistics",
      "design-principles",
      "modelling-mindsets"
    ],
    "url": "/posts/TIL-the-art-of-choice.html",
    "content": "<p><strong>TL;DR:</strong> Sheena Iyengar&#39;s cross-cultural research reveals that how we perceive and respond to choice varies dramatically between societies - with evidence showing that sometimes having fewer choices or allowing others to choose for us can lead to better outcomes, challenging the widely-held Western belief that more individual choice is always beneficial.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Today I revisited a talk on <a href=\"https://www.youtube.com/watch?v=lDq9-QxvsNU\">the art of choosing</a> by Sheena Iyengar. A humourous and informative presentation, it reminded me that our assumptions about choice –as studied by Prof. Iyengar through research spanning American, European and Asian populations– reveals fascinating cultural differences in how we perceive and respond to choice. Her research reveals some eye-opening insights that I&#39;ll briefly summarise below.</p>\n<h2 id=\"perceiving-choice\">Perceiving Choice</h2>\n<p>First, while Americans believe individual choice is sacred (think &quot;have it your way&quot;), research shows this isn&#39;t universal. When studying children solving puzzles, Asian-American children actually performed better when their mothers chose for them, while Anglo-American children did better choosing for themselves. This reveals how deeply cultural context shapes not just our preferences, but the actual effectiveness of our choices.</p>\n<p>Second, remember how overwhelming it feels staring at 50 different breakfast cereals? Turns out, people from post-communist countries often saw seven different sodas as just one choice: &quot;soda or no soda.&quot; This isn&#39;t because they&#39;re less sophisticated, it&#39;s because the ability to spot tiny differences between products is a learned skill -not a natural one.</p>\n<p>Most striking was the research on medical decisions. When comparing American and French parents making end-of-life decisions for infants, American parents had more negative emotions and guilt despite insisting on having the choice, while French parents, whose doctors made the decisions, coped better. This challenges the core American belief that having choice is always better.</p>\n<p>Concluding with a personal story, Prof. Iyengar -who is blind- shared how she once brought two &quot;clearly different&quot; shades of pink nail polish to her lab. When she removed the labels, half the participants couldn&#39;t tell them apart. Those who could, chose differently when the labels were present versus absent, showing how marketing narratives shape what we think we&#39;re choosing.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The TL;DR is: Through cross-cultural research, Prof. Iyengar shows that how we understand and value choice varies dramatically across cultures. Sometimes, having fewer choices or letting others choose for us might actually lead to better outcomes.<br>As a technologist, inundated with a very wide choice of tools that often offer similar results, I have made the conscious decision to reduce my tooling footprint to the minimum viable toolstack possible. I&#39;m happy to let more knowledgeable professionals choose, with <em>adequate justification</em>, tools for my line of work but I do disagree with the zealotry that&#39;s occasionally observed in tech and complemented by big egos. </p>\n"
  },
  {
    "title": "💡 TIL: The Matrix Equation That Makes Linear Regression Work",
    "date": "2025-01-08T00:00:00.000Z",
    "tags": [
      "data-science",
      "machine-learning",
      "statistics",
      "ai",
      "linear-algebra",
      "til",
      "modelling-mindsets",
      "data-modeling"
    ],
    "url": "/posts/TIL-lin-alg-applied-to-stats.html",
    "content": "<p><strong>TL;DR:</strong> Linear regression can be elegantly solved using the matrix equation β = (X^TX)^(-1)X^Ty, which mathematically guarantees minimum squared error by accounting for feature correlations - though real-world applications often favour gradient descent due to the direct solution&#39;s computational complexity, numerical instability with correlated features, and memory constraints.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>This morning <a href=\"https://xcancel.com/andrew_n_carr/status/1876855682529480844\">an interesting interview question</a> motivated me to remind myself how it&#39;s possible to solve linear regression through matrix algebra. Below is what I learned:  </p>\n<h2 id=\"the-theory-an-elegant-mathematical-solution\">The Theory: An Elegant Mathematical Solution</h2>\n<p>Linear regression finds the best-fit line through data points by finding optimal coefficients ($\\beta$) that minimise squared errors. The equation $\\beta = (X^TX)^{-1}X^Ty$ elegantly solves this optimisation problem using matrix algebra.</p>\n<p>The solution involves these key components:</p>\n<ol>\n<li>$X$ is our feature matrix (n samples × p features)</li>\n<li>$y$ is our target values (n × 1)</li>\n<li>$X^T$ is the transpose of X</li>\n<li>$\\beta$ is our solution vector (p × 1) of coefficients</li>\n</ol>\n<p>Here&#39;s how this elegant solution works:  </p>\n<ol>\n<li><p>$X^TX$ creates a $(p \\times p)$ matrix of feature products:  </p>\n<ul>\n<li>Each element $(i,j)$ contains the dot product between features $i$ and $j$</li>\n<li>When features are centred, these products are proportional to covariances[^1]</li>\n<li>When features are also standardised, it yields correlations scaled by $n$</li>\n</ul>\n</li>\n<li><p>$(X^TX)^{-1}$ computes the inverse of this matrix:  </p>\n<ul>\n<li>Compensates for feature correlations in coefficient calculations[^2]</li>\n<li>Required for solving the normal equations $X^TX\\beta = X^Ty$</li>\n<li>Exists only when no feature is a linear combination of others</li>\n</ul>\n</li>\n<li><p>$X^Ty$ creates a $(p \\times 1)$ vector of feature-target products:  </p>\n<ul>\n<li>Each element $i$ contains the dot product of feature $i$ with target $y$</li>\n<li>Represents raw feature-target relationships before adjustment</li>\n<li>When centred, proportional to feature-target covariances[^3]</li>\n</ul>\n</li>\n<li><p>Final multiplication $(X^TX)^{-1}X^Ty$:  </p>\n<ul>\n<li>Solves the normal equations $X^TX\\beta = X^Ty$</li>\n<li>Accounts for inter-feature correlations in determining coefficients</li>\n<li>Mathematically guarantees minimum squared error</li>\n</ul>\n</li>\n</ol>\n<p>For more information, check Hastie, Tibshirani &amp; Friedman&#39;s &quot;<a href=\"https://archive.org/details/elementsofstatis0000hast\">Elements of Statistical Learning</a>&quot; seminal book. </p>\n<h2 id=\"the-real-world-catch\">The Real-World Catch</h2>\n<p>While mathematically elegant, this direct solution has practical limitations in real-world applications:  </p>\n<ol>\n<li><em>Computational Complexity</em>: Computing $(X^TX)^{-1}$ requires $\\Omicron(n^3)$ operations, becoming prohibitively expensive for large feature sets. This is why gradient descent, with its $\\Omicron(n^2)$  per-iteration complexity, often proves more practical.  </li>\n<li><em>Numerical Instability</em>: When features are highly correlated (like monthly and annual income), $X^TX$ becomes nearly singular[^3]. Even small rounding errors in the computation of its inverse can lead to large errors in $\\beta$. In extreme cases, when features are perfectly correlated, the inverse doesn&#39;t exist at all. Gradient descent avoids this matrix inversion entirely.    </li>\n<li><em>Memory Constraints</em>: Large datasets require holding the entire $X^TX$ matrix in memory, while gradient descent can work with mini-batches, making it more memory-efficient.</li>\n</ol>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>While this equation brilliantly demonstrates the power of linear algebra in statistics, real-world machine learning often favours gradient descent&#39;s iterative approach. Think of it as choosing between a perfect GPS route through heavy traffic (direct solution) versus taking smaller, adaptable steps through clear side streets (gradient descent). Both reach the same destination, but the practical path often wins in real-world conditions.</p>\n<hr>\n<p>[^1]: When features are centred (mean = 0), each product becomes $n$ times the covariance. This means $X^TX$ captures how features vary together, which is crucial because correlated features can lead to unstable coefficients if not accounted for. The relationship between $X^TX$ and covariance comes from the definition of sample covariance: $cov(X_i, X_j) = \\frac{1}{n-1}\\sum_{k=1}^n (x_{ki} - \\bar{x_i})(x_{kj} - \\bar{x_j})$. When data is centred, this simplifies to $\\frac{1}{n-1}(X^TX)_{ij}$.  $\\frac{X^TX}{n-1}$ returns the sample covariance matrix. This matters because a) when features are uncentred, $(X^TX)$ gives the sum of products, b) when centred $\\frac{X^TX}{n-1}$ gives covariances, c) when also standardised (std = 1), $\\frac{X^TX}{n-1}$ gives correlations.\n[^2]: Adjusts coefficient estimates to account for shared information between features. For example, if height and weight are correlated, we need to determine each variable&#39;s unique contribution to the prediction, not their overlapping effect.\n[^3]: When centred, each element becomes $n$ times the covariance between a feature and the target. This reveals how each feature individually relates to $y$ before accounting for other features&#39; effects, providing a starting point for determining final coefficients.\n[^3]: A matrix is singular (or non-invertible) when its determinant is zero. In practical terms, this means one or more columns can be expressed as linear combinations of other columns.</p>\n"
  },
  {
    "title": "💊 Lessons for Modern Drug Development from the Golden Age of Antibiotics",
    "date": "2025-01-07T00:00:00.000Z",
    "tags": [
      "iterative-refinement",
      "evolution",
      "data-science",
      "evaluation",
      "decision-making",
      "best-practices",
      "modelling-mindsets",
      "production"
    ],
    "url": "/posts/golden-age-of-antibiotics.html",
    "content": "<p><strong>TL;DR:</strong> Despite our vastly superior modern technology, antibiotic development has dramatically declined since the post-WWII &quot;Golden Age&quot; (1940s-1960s) that produced most antibiotic classes we still use today-highlighting how scientific capability alone cannot drive progress without three key elements working together: economic incentives that correct market failures, institutional coordination, and systematic application of technological tools.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>In a thought-provoking analysis[^1], Our World in Data reveals a striking paradox in medical progress: the most productive period in antibiotic development occurred in the two decades following World War II, with scientific capabilities far more limited than today. This &quot;Golden Age of Antibiotics&quot; (1940s-1960s) produced nearly two-thirds of the antibiotic drug classes we still rely on[^2].<br>Even more surprisingly, since 1970 -despite exponential advances in computing power and biotechnology- only eight new classes of antibiotics have been approved[^2]. This indicates a stark decline that threatens the foundation of modern medicine. Traditional screening methods now rediscover existing compounds most of the time rather than finding new ones[^3].<br>Modern tools like genome sequencing and systematic screening methods offer unprecedented capabilities. We&#39;ve only identified a small fraction of bacterial species, many of which could harbour new antibiotic compounds[^2]. Yet despite these capabilities, development has stagnated due to fundamental market failures and fragmented research efforts.<br>This article examines this paradoxical inverse relationship between technological capability and antibiotic development: How did the Golden Age achieve such remarkable success with limited tools? Why has progress slowed as our capabilities have grown? Most importantly, what combinations of economic incentives and modern technology could spark a new era of antibiotic discovery?</p>\n<h2 id=\"when-urgency-met-innovation\">When Urgency Met Innovation</h2>\n<p>The Golden Age of Antibiotics stands as medicine&#39;s most productive period in antimicrobial discovery, yielding over 20 new antibiotic classes -more than double what we&#39;ve developed in the 50 years since[^2]. Three pivotal breakthroughs, coupled with unprecedented coordination, drove this remarkable success.<br>The foundation was laid by Paul Ehrlich&#39;s systematic approach to drug discovery. By methodically testing hundreds of compounds, he discovered <a href=\"https://en.wikipedia.org/wiki/Arsphenamine\">salvarsan</a> in 1910 -the first synthetic antibiotic that effectively treated syphilis[^2]. A second milestone emerged when Alexander Fleming discovered penicillin in 1928. However, the real innovation came through coordinated wartime effort. With infections being the second-most common cause of hospital admissions in the US Army, the U.S. Office of Scientific Research and Development (OSRD) launched a global search for more productive penicillin strains, ultimately finding a high-yielding strain on a cantaloupe[^4].<br>The third breakthrough came from Selman Waksman&#39;s insight into soil bacteria. His discovery that soil-dwelling <a href=\"https://en.wikipedia.org/wiki/Actinomycetales\">actinomycetes</a> bacteria naturally produce antibiotics led to <a href=\"https://en.wikipedia.org/wiki/Streptomycin\">streptomycin</a>&#39;s development and opened an entirely new avenue for antibiotic discovery[^5].<br>What transformed these breakthroughs into a &quot;golden age&quot; was unprecedented coordination. The U.S. War Production Board orchestrated collaboration between government, academia, and industry -removing patent restrictions, sharing data, and streamlining clinical trials[^6]. The results were remarkable: some antibiotics, like <a href=\"https://en.wikipedia.org/wiki/Tetracycline_antibiotics\">tetracyclines</a> and <a href=\"https://en.wikipedia.org/wiki/Macrolide\">macrolides</a>, went from discovery to clinical use within the same year.</p>\n<h2 id=\"scientific-progress-and-market-failure\">Scientific Progress and Market Failure</h2>\n<p>The contrast between the Golden Age and our current era reflects a fundamental misalignment between public health needs and market incentives[^2]. The market structure fundamentally disfavours antibiotics in two ways:  </p>\n<ol>\n<li>Revenue Structure: While chronic disease medications can generate billions in annual revenue over decades, new antibiotics typically generate only tens of millions annually[^7], by comparison. This revenue gap has driven many large pharmaceutical companies away from antibiotic development[^8].  </li>\n<li>Conservation Requirements: New antibiotics must be reserved for severe drug-resistant infections, reaching less than 1% of hospitalised patients[^7]. This necessary conservation practice severely limits market potential.</li>\n</ol>\n<p>Meanwhile, our technological capabilities offer three particularly promising approaches:  </p>\n<ol>\n<li>Genome mining: a breakthrough technique that identifies hidden antibiotic genes in microbes that remain dormant under standard laboratory conditions. This computational approach has already yielded promising candidates like humimycins[^9].  </li>\n<li>Advanced bacterial exploration: research into extreme environments like deep oceans and deserts, where previously &quot;unculturable&quot; bacteria might harbour entirely new antibiotic classes[^3].  </li>\n<li>Smart combination strategies: exploiting the observation that bacterial resistance to one antibiotic can increase vulnerability to others, opening new therapeutic possibilities[^10].</li>\n</ol>\n<p>Yet these powerful tools remain underutilised due to insufficient investment and coordination. The challenge isn&#39;t scientific capability -it&#39;s the failure to create systems that effectively deploy these technologies within sustainable economic frameworks.</p>\n<h2 id=\"integrating-economics-and-technology\">Integrating Economics and Technology</h2>\n<p>Drawing from evidence in antibiotic development research, several promising approaches could help overcome current market failures while leveraging modern technological capabilities[^7].</p>\n<h3 id=\"economic-solutions-to-market-failures\">Economic Solutions to Market Failures</h3>\n<ol>\n<li>Subscription Models: The UK has pioneered a system where healthcare systems pay annual fees for antibiotic access rather than per-volume pricing. This addresses both the revenue challenge and conservation requirements by providing stable income while supporting appropriate antibiotic use[^7].  </li>\n<li>Advance Market Commitments: These provide guaranteed payments to companies that successfully develop new antibiotics, similar to successful vaccine development programs. This directly addresses the revenue uncertainty that has driven companies away from antibiotic development[^11].  </li>\n<li>Collaborative Funding Initiatives: Organisations like CARB-X and GARDP help smaller companies navigate costly clinical trials, distributing development risks that large pharmaceutical companies are unwilling to bear[^7].</li>\n</ol>\n<h3 id=\"leveraging-modern-technology\">Leveraging Modern Technology</h3>\n<p>To maximise the impact of these economic incentives, three technological approaches show particular promise:  </p>\n<ol>\n<li>Systematic Genome Mining: Using computational power to identify promising antibiotic-producing genes in bacterial genomes, revealing compounds that traditional screening would miss[^9].  </li>\n<li>Environmental Exploration: Research into extreme environments could unlock entirely new antibiotic classes, enabled by modern sequencing technologies[^3].  </li>\n<li>Smart Combination Strategies: Systematic exploration of how resistance to one antibiotic can increase vulnerability to others, offering new therapeutic possibilities[^10].</li>\n</ol>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The story of antibiotic development demonstrates that scientific capability alone cannot drive progress. The Golden Age succeeded through a powerful combination of systematic approaches, unprecedented collaboration, and removal of institutional barriers -even with limited technological tools[^2].<br>Today&#39;s challenge is fundamentally different. We possess sophisticated tools -from genome mining to advanced screening methods- yet development has stalled. This paradox reveals that progress requires three key elements working in concert: economic incentives, institutional coordination, and technological application[^7].<br>The evidence-based solutions presented in the original Our World in Data article[^1] offer a path forward. Market reforms like subscription models and advance market commitments could help correct the fundamental economic misalignment in antibiotic development[^8]. Meanwhile, systematic application of computational tools, genomic analysis, and bacterial exploration could help unlock new classes of antibiotics that traditional methods miss[^9].<br>The urgency is clear. Antimicrobial resistance threatens to undermine many advances in modern medicine[^12]. However, by combining proven coordination approaches from the Golden Age with modern capabilities and sustainable economic frameworks, we can revitalise antibiotic development for the challenges ahead.</p>\n<hr>\n<p>[^1]: Our World in Data (2024). &quot;What was the Golden Age of Antibiotics, and how can we spark a new one?&quot; <a href=\"https://ourworldindata.org/golden-age-antibiotics\">https://ourworldindata.org/golden-age-antibiotics</a>\n[^2]: Hutchings, M. I., Truman, A. W., &amp; Wilkinson, B. (2019). Antibiotics: Past, present and future. Current Opinion in Microbiology, 51, 72–80. <a href=\"https://doi.org/10.1016/j.mib.2019.10.008\">https://doi.org/10.1016/j.mib.2019.10.008</a>\n[^3]: Kolter, R., &amp; Van Wezel, G. P. (2016). Goodbye to brute force in antibiotic discovery? Nature Microbiology, 1(2), 15020. <a href=\"https://doi.org/10.1038/nmicrobiol.2015.20\">https://doi.org/10.1038/nmicrobiol.2015.20</a>\n[^4]: Gaynes, R. (2017). The Discovery of Penicillin -New Insights After More Than 75 Years of Clinical Use. Emerging Infectious Diseases, 23(5), 849–853. <a href=\"https://doi.org/10.3201/eid2305.161556\">https://doi.org/10.3201/eid2305.161556</a>\n[^5]: Waksman, S. A., &amp; Schatz, A. (1945). Streptomycin–Origin, Nature, and Properties. Journal of the American Pharmaceutical Association, 34(11), 273–291. <a href=\"https://doi.org/10.1002/jps.3030341102\">https://doi.org/10.1002/jps.3030341102</a>\n[^6]: Sampat, B. N. (2023). Second World War and the Direction of Medical Innovation. SSRN Electronic Journal. <a href=\"https://doi.org/10.2139/ssrn.4422261\">https://doi.org/10.2139/ssrn.4422261</a>\n[^7]: Årdal, C., et al. (2020). Antibiotic development -economic, regulatory and societal challenges. Nature Reviews Microbiology, 18(5), 267-274. <a href=\"https://doi.org/10.1038/s41579-019-0293-3\">https://doi.org/10.1038/s41579-019-0293-3</a>\n[^8]: Renwick, M. J., Brogan, D. M., &amp; Mossialos, E. (2016). A systematic review and critical assessment of incentive strategies for discovery and development of novel antibiotics. The Journal of Antibiotics, 69(2), 73-88. <a href=\"https://doi.org/10.1038/ja.2015.98\">https://doi.org/10.1038/ja.2015.98</a>\n[^9]: Chu, J., et al. (2016). Discovery of MRSA active antibiotics using primary sequence from the human microbiome. Nature Chemical Biology, 12(12), 1004-1006. <a href=\"https://doi.org/10.1038/nchembio.2207\">https://doi.org/10.1038/nchembio.2207</a>\n[^10]: Baym, M., Stone, L. K., &amp; Kishony, R. (2016). Multidrug evolutionary strategies to reverse antibiotic resistance. Science, 351(6268), aad3292. <a href=\"https://doi.org/10.1126/science.aad3292\">https://doi.org/10.1126/science.aad3292</a>\n[^11]: Kremer, M., Levin, J., &amp; Snyder, C. M. (2020). Advance Market Commitments: Insights from Theory and Experience. AEA Papers and Proceedings, 110, 269-273. <a href=\"https://www.aeaweb.org/articles?id=10.1257/pandp.20201017\">https://www.aeaweb.org/articles?id=10.1257/pandp.20201017</a>\n[^12]: World Health Organization (2024). Antimicrobial Resistance Fact Sheet. <a href=\"https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance\">https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance</a></p>\n"
  },
  {
    "title": "💡 TIL: Test-Driven Development Is Key to Better LLM System Prompts",
    "date": "2025-01-02T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "til",
      "prompt-engineering",
      "testing",
      "best-practices",
      "evaluation",
      "machine-learning",
      "system-prompts"
    ],
    "url": "/posts/TIL-tdd-good-system-prompts.html",
    "content": "<p><strong>TL;DR:</strong> Anthropic&#39;s approach to system prompt development parallels test-driven development-first creating test cases where default model behaviour fails, then developing prompts that pass these tests, followed by iterative refinement-highlighting how robust automated evaluation is not merely a quality check but the foundation for building reliable LLM applications.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>2024 has made clear that writing good automated evaluations for LLM-powered systems is the most critical skill for building useful applications. This insight parallels Anthropic&#39;s internal approach to system prompt development. As usual, Simon Willison&#39;s <a href=\"https://simonwillison.net/2024/Dec/31/llms-in-2024/#evals-really-matter\">recent insightful 2024 LLM overview</a> was a treasure trove. One item I picked up on was evaluating system prompts using a test-driven approach. </p>\n<h2 id=\"the-evaluation-first-approach\">The Evaluation-First Approach</h2>\n<p><a href=\"https://askell.io/\">Amanda Askell</a>, leading fine-tuning at Anthropic, <a href=\"https://xcancel.com/amandaaskell/status/1866207266761760812\">outlines a test-driven process</a> for system prompts:</p>\n<ol>\n<li>Create a test set of messages where the model&#39;s default behaviour fails to meet requirements</li>\n<li>Develop a system prompt that passes these tests</li>\n<li>Identify cases where the system prompt is misapplied and refine it</li>\n<li>Expand the test set and repeat</li>\n</ol>\n<p>This methodology&#39;s importance extends beyond prompt engineering. Companies with strong evaluation suites can adopt new models faster and build more reliable features than competitors. As <a href=\"https://xcancel.com/cramforce/status/1860436022347075667\">Vercel&#39;s experience demonstrates</a>, moving from complex prompt protection to robust testing enables rapid iteration and development.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>While everyone acknowledges evals&#39; importance, implementing them effectively remains challenging. The key insight is clear: robust automated evaluation isn&#39;t just a quality check, it&#39;s the foundation for building reliable LLM-powered systems.</p>\n"
  },
  {
    "title": "📝 From Vim to VSCode to Neovim",
    "date": "2024-12-24T00:00:00.000Z",
    "tags": [
      "minimal",
      "cross-platform",
      "toolchain",
      "best-practices",
      "design-principles",
      "python",
      "deno",
      "zero-config"
    ],
    "url": "/posts/vscode-to-neovim.html",
    "content": "<p><strong>TL;DR:</strong> For me, Neovim strikes the perfect balance between Vim&#39;s simplicity and VSCode&#39;s features. After wrestling with VSCode&#39;s keyboard input failures on Fedora and its resource demands, I found that Neovim&#39;s single configuration file, robust plugins, and cross-platform reliability make it ideal for my Python, Deno, and Clojure development needs. Sometimes stepping back to move forward is exactly what we need!</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Vim&#39;s portable <code>.vimrc</code> embodies software minimalism at its best. One file, one minute to setup, resulting in a complete development environment. This simplicity served me well until Azure development motivated the use of VSCode.<br>While VSCode worked reasonably well on macOS, Fedora revealed its constraints: keyboard input failures, heavy resource usage, and <a href=\"https://stackoverflow.com/questions/35368889/how-can-i-export-settings\">complex environment portability</a> compared to Vim&#39;s <code>vim +PlugInstall</code>. These limitations drove my search for tools that could maintain simplicity while meeting my development requirements with simplicity and portability in mind.</p>\n<h2 id=\"vim---vscode---neovim\">Vim -&gt; VSCode -&gt; Neovim</h2>\n<p>Azure development initially pulled me into VSCode&#39;s ecosystem. While stable on macOS, Fedora revealed deal-breakers: random keyboard input failures that only responded to command palette (Ctrl+Shift+P). No amount of configuration resets or reinstalls resolved these issues.</p>\n<p>This instability, coupled with VSCode&#39;s resource footprint, led me to Neovim. The timing aligned with my exploration of Clojure, where Neovim&#39;s Conjure plugin offered a compelling Lisp development experience that rivaled Emacs.</p>\n<p>My requirements were specific:</p>\n<ul>\n<li>A lightweight Python IDE</li>\n<li>A lightweight Deno IDE</li>\n<li>A lightweight Clojure IDE</li>\n</ul>\n<p>Through [Dialogue Engineering]({{ site.baseurl }}{% link _posts/2024-11-15-dialogue-engineering.md %}), I crafted a complete IDE using a <a href=\"https://github.com/ai-mindset/init.vim\">single configuration file</a>. Neovim&#39;s mixed ecosystem of package managers and dual Vimscript/Lua support presents a learning curve, but the resulting environment is fast, stable, and precisely tailored to my needs. One minor drawback is the complexity of adding colour to Conjure&#39;s output, especially when compared to the rich REPL experiences offered by <a href=\"https://ipython.org/\">IPython</a>, <a href=\"https://deno.com/\">Deno</a>, and Clojure with <a href=\"https://github.com/bhauman/rebel-readline\">rebel-readline</a>.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The journey from Vim to VSCode and finally to Neovim reflects a common pattern in software development: sometimes we need to step backward to move forward. While VSCode offered modern IDE features, its stability and resource issues on Linux highlighted the enduring value of minimal, portable tools.<br>Neovim strikes an elegant balance: it preserves Vim&#39;s philosophy of simplicity and portability while providing modern IDE capabilities. Despite minor challenges with REPL colourisation, its single configuration file approach and robust plugin ecosystem make it a powerful choice for polyglot development. For developers who value both minimal tooling and modern features, Neovim proves that we don&#39;t always have to choose between the two.</p>\n"
  },
  {
    "title": "💡 TIL: Exploring OpenAI's API with Swagger",
    "date": "2024-12-23T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "openai",
      "openapi",
      "spec"
    ],
    "url": "/posts/TIL-openai-openapi.html",
    "content": "<p><strong>TL;DR:</strong> You can easily explore OpenAI&#39;s complete API documentation by loading their GitHub-hosted OpenAPI YAML file directly into Swagger&#39;s web interface. This approach lets you interactively examine all endpoints, request/response schemas, and test functionality—a valuable reference for anyone building services that need to maintain compatibility with OpenAI&#39;s API structure.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>OpenAI maintains a comprehensive <a href=\"https://github.com/openai/openai-openapi/\">OpenAPI specification</a> that documents their entire API surface. While browsing through their GitHub repository, <a href=\"https://simonwillison.net/\">Simon Willison</a>[^1] discovered you can easily explore this spec using Swagger&#39;s web interface.</p>\n<h2 id=\"the-discovery\">The Discovery</h2>\n<p>Willison recently highlighted a neat trick: you can browse OpenAI&#39;s full API documentation by loading their <a href=\"https://github.com/openai/openai-openapi/blob/master/openapi.yaml\">OpenAPI YAML file</a> directly into <a href=\"https://petstore.swagger.io/?url=https://raw.githubusercontent.com/openai/openai-openapi/refs/heads/master/openapi.yaml#/\">Swagger&#39;s web UI</a>.</p>\n<h2 id=\"why-this-matters\">Why This Matters</h2>\n<p>This approach offers several advantages:</p>\n<ul>\n<li>Interactive exploration of all API endpoints</li>\n<li>Complete request/response schemas</li>\n<li>Built-in testing capability</li>\n<li>Detailed parameter documentation</li>\n</ul>\n<p>For developers working with AI APIs, this provides a valuable reference point - especially when building services that need to maintain compatibility with OpenAI&#39;s API structure.</p>\n<h2 id=\"try-it-yourself\">Try It Yourself</h2>\n<p>Visit the <a href=\"https://petstore.swagger.io/\">Swagger UI</a> and paste this URL:<br><code>https://raw.githubusercontent.com/openai/openai-openapi/refs/heads/master/openapi.yaml</code></p>\n<hr>\n<p>[^1]: Co-founder of <a href=\"https://blog.natbat.net/post/61658401806/lanyrd-from-idea-to-exit\">Lanyrd</a>, co-creator of <a href=\"https://simonwillison.net/2005/Jul/17/django/\">Django</a> and <a href=\"https://datasette.io/\">Datasette</a> and a prolific independent AI researcher</p>\n"
  },
  {
    "title": "🎛️ A Practical Guide to Fine-tuning LLMs with InstructLab",
    "date": "2024-12-19T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "model-governance",
      "production",
      "quantisation",
      "python",
      "mlops",
      "best-practices",
      "data-science"
    ],
    "url": "/posts/instructlab-and-rag.html",
    "content": "<p><strong>TL;DR:</strong> InstructLab democratises LLM fine-tuning through its structured LAB methodology, offering three hardware-adaptive QLoRA-based training pipelines (Simple, Full, and Accelerated) that enable organisations to create domain-specific models without massive computing resources whilst maintaining comprehensive evaluation frameworks.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>The explosion of Large Language Models (LLMs) has created a pressing need for domain-specific adaptations. While base models like GPT-4, Claude, and Llama demonstrate impressive general capabilities, organisations often need models that excel in specific domains or exhibit particular behavioural traits. This customisation typically requires fine-tuning, a process that has historically demanded significant expertise, computational resources, and sophisticated infrastructure.</p>\n<h3 id=\"the-fine-tuning-challenge\">The Fine-tuning Challenge</h3>\n<p>Traditional LLM fine-tuning presents a complex web of interconnected challenges that organisations must navigate. At its core lies the need for sophisticated infrastructure, often requiring specialised hardware and carefully orchestrated software stacks. This infrastructure challenge is compounded by substantial computational costs, making experimentation and iteration expensive.<br>The data challenge is equally significant. Fine-tuning demands large, high-quality datasets that are both rare and expensive to create. Even when such datasets exist, organisations face the risk of catastrophic forgetting, where models lose their general capabilities while acquiring new ones. Moreover, validating improvements remains a complex task, requiring careful benchmarking and evaluation frameworks.<br>These challenges have historically restricted fine-tuning to well-resourced organisations, creating a significant barrier to entry for smaller teams and organisations seeking to adapt LLMs to their specific needs.</p>\n<h3 id=\"real-world-challenges\">Real-world Challenges</h3>\n<p>The adaptation of LLMs to specific domains presents organisations with a multifaceted set of practical challenges. In healthcare, medical institutions grapple with the need for models that can accurately process and generate content using complex medical terminology while maintaining strict clinical protocols. This domain expertise challenge extends beyond mere vocabulary; it encompasses understanding of medical procedures, drug interactions, and diagnostic reasoning.<br>The financial sector faces equally demanding requirements, particularly around compliance and regulation. Banks and financial institutions must ensure their models operate within specific regulatory frameworks, making decisions that are not only accurate but also auditable and explainable to regulatory bodies.<br>Data quality emerges as a persistent challenge across sectors. Organisations typically struggle with historical datasets that exhibit inconsistent formatting, missing values, and inherent biases. The challenge extends to maintaining proper version control and data lineage tracking, crucial for both compliance and model improvement cycles.<br>Regulatory constraints add another layer of complexity. Healthcare organisations must ensure strict HIPAA compliance in their model development and deployment processes. Similarly, any organisation handling European data must adhere to GDPR requirements, while specific industries often face additional certification needs. These regulatory requirements must be considered not just in the final deployment but throughout the entire fine-tuning process.  </p>\n<h3 id=\"the-role-of-instructlab\">The Role of InstructLab</h3>\n<p><a href=\"https://instructlab.ai/\">InstructLab</a> emerges as a systematic solution to these challenges, offering a novel approach to LLM fine-tuning that combines:</p>\n<ul>\n<li>Synthetic data generation for high-quality training examples</li>\n<li>Efficient <a href=\"https://arxiv.org/abs/2305.14314\">QLoRA</a>-based training pipelines</li>\n<li>Comprehensive evaluation frameworks</li>\n<li>Hardware-adaptive processing</li>\n</ul>\n<p>The rest of this article will elaborate on <a href=\"https://instructlab.ai/\">InstructLab</a>&#39;s architecture, workflow, and practical considerations, demonstrating how it makes LLM fine-tuning accessible while maintaining rigorous quality standards. It will explore how organisations can leverage this tool to enhance their AI capabilities efficiently and systematically.</p>\n<h2 id=\"from-principles-to-practice\">From Principles to Practice</h2>\n<p><a href=\"https://instructlab.ai/\">InstructLab</a> is built around the LAB (Large-Scale Alignment for ChatBots) methodology, leveraging [QLoRA(<a href=\"https://arxiv.org/abs/2305.14314\">https://arxiv.org/abs/2305.14314</a>) (Quantized Low-Rank Adaptation) for efficient fine-tuning. The system requires Python 3.10/3.11 and approximately 500GB of disc space for full operation.</p>\n<h3 id=\"architectural-components\">Architectural Components</h3>\n<p>The system operates through three primary components:  </p>\n<ul>\n<li><strong>Taxonomy Repository</strong>: A structured collection of knowledge and skills, organised in YAML files (max 2300 words per Q&amp;A pair)</li>\n<li><strong>Synthetic Data Generator</strong>: Uses a teacher model (default: Mixtral/Mistral instruct for full pipeline, Merlinite 7b for simple) to transform taxonomy entries into diverse training examples</li>\n<li><strong>Training Pipeline System</strong>: <a href=\"https://arxiv.org/abs/2305.14314\">QLoRA</a>-based training options optimised for different hardware configurations</li>\n</ul>\n<h3 id=\"training-pipelines\">Training Pipelines</h3>\n<p><a href=\"https://instructlab.ai/\">InstructLab</a> offers three specialised training pipelines:</p>\n<ol>\n<li><p><strong>Simple Pipeline</strong></p>\n<ul>\n<li>Fast training (~1 hour)</li>\n<li>Uses SFT Trainer (Linux) or MLX (MacOS)</li>\n<li>Great for initial experiments and validation</li>\n</ul>\n</li>\n<li><p><strong>Full Pipeline</strong></p>\n<ul>\n<li>Custom <a href=\"https://arxiv.org/abs/2305.14314\">QLoRA</a> training loop optimised for CPU/MPS</li>\n<li>Enhanced data processing functions</li>\n<li>Memory requirement: 32GB RAM</li>\n<li>Balanced performance and accessibility</li>\n</ul>\n</li>\n<li><p><strong>Accelerated Pipeline</strong></p>\n<ul>\n<li>GPU-accelerated distributed <a href=\"https://arxiv.org/abs/2305.14314\">QLoRA</a> training</li>\n<li>Supports NVIDIA CUDA and AMD ROCm</li>\n<li>Requires 18GB+ GPU memory</li>\n<li>Ideal for production-grade fine-tuning</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"hardware-support-and-quantisation\">Hardware Support and Quantisation</h3>\n<p><a href=\"https://instructlab.ai/\">InstructLab</a> supports various hardware configurations with automatic quantisation:</p>\n<ul>\n<li>Apple M-series chips: MLX optimisation, MPS acceleration</li>\n<li>NVIDIA GPUs: CUDA support, 4-bit quantisation available</li>\n<li>AMD GPUs: ROCm support, similar quantisation options</li>\n<li>Standard CPUs: Optimised quantisation for memory efficiency</li>\n</ul>\n<h2 id=\"practical-workflow\">Practical Workflow</h2>\n<p>With the architectural foundation established, <a href=\"https://instructlab.ai/\">InstructLab</a> provides a systematic approach to implementing these components through a straightforward command-line interface. The following sections detail the practical steps to leverage this architecture effectively.</p>\n<h3 id=\"setup-and-installation\">Setup and Installation</h3>\n<pre><code class=\"language-bash\">pip install instructlab\nilab config init\n</code></pre>\n<p>Key requirements:</p>\n<ul>\n<li>Python 3.10 or 3.11 (&gt;=3.12 not supported[^1])</li>\n<li>500GB recommended disc space</li>\n<li>16GB RAM minimum, 32GB recommended</li>\n</ul>\n<h3 id=\"core-workflow-steps\">Core Workflow Steps</h3>\n<ol>\n<li><p><strong>Model Acquisition</strong></p>\n<pre><code class=\"language-bash\">ilab model download\n</code></pre>\n<ul>\n<li>Downloads pre-trained base models</li>\n<li>Supports GGUF (4-bit to 16-bit) and Safetensors formats</li>\n<li>Automatic quantisation with configurable parameters</li>\n</ul>\n</li>\n<li><p><strong>Synthetic Data Generation</strong></p>\n<pre><code class=\"language-bash\">ilab model serve\nilab data generate --pipeline [simple|full]\n</code></pre>\n<p>Common issues and solutions:</p>\n<ul>\n<li>Server conflicts: Use different ports with <code>--port</code></li>\n<li>Memory errors: Reduce batch size or use <code>--pipeline simple</code></li>\n<li>Teacher model issues: Verify model checksum and try re-downloading</li>\n</ul>\n</li>\n<li><p><strong>Training</strong></p>\n<pre><code class=\"language-bash\">ilab model train\n</code></pre>\n<p>Hyperparameters (configurable in config.yaml):</p>\n<ul>\n<li>Max epochs: 10</li>\n</ul>\n</li>\n<li><p><strong>Evaluation</strong></p>\n<pre><code class=\"language-bash\">ilab model evaluate\n</code></pre>\n<p>Benchmarks and typical scores:</p>\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/MMLU\">MMLU</a>: Knowledge (0.0-1.0 scale)</li>\n<li>MMLUBranch: Delta improvements</li>\n<li>MTBench: Skills (0.0-10.0 scale)</li>\n<li>MTBenchBranch: Skill improvements</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"model-deployment\">Model Deployment</h3>\n<pre><code class=\"language-bash\">ilab model serve --model-path &lt;new-model-path&gt;\nilab model chat -m &lt;new-model-path&gt; # Optionally, chat with the model\n</code></pre>\n<p>Deployment considerations:</p>\n<ul>\n<li>Verify quantisation level matches hardware capabilities</li>\n<li>Monitor memory usage during serving</li>\n<li>Consider temperature settings for inference (default: 1.0)</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p><a href=\"https://instructlab.ai/\">InstructLab</a> represents a significant advancement in democratising LLM fine-tuning, bridging the gap between research capabilities and practical deployment. Through its innovative LAB methodology and <a href=\"https://arxiv.org/abs/2305.14314\">QLoRA</a>-based implementation, it makes sophisticated model adaptation accessible to practitioners across different hardware configurations.</p>\n<h3 id=\"key-advantages\">Key Advantages</h3>\n<ul>\n<li><strong>Accessibility</strong>: From laptops to data centres, <a href=\"https://instructlab.ai/\">InstructLab</a> scales with available resources</li>\n<li><strong>Flexibility</strong>: Multiple training pipelines accommodate different needs and constraints</li>\n<li><strong>Systematic</strong>: Structured approach to knowledge and skill injection through taxonomy</li>\n<li><strong>Verifiable</strong>: Comprehensive evaluation suite ensures quality of fine-tuned models</li>\n</ul>\n<h3 id=\"practical-impact\">Practical Impact</h3>\n<p><a href=\"https://instructlab.ai/\">InstructLab</a> enables organisations to:</p>\n<ul>\n<li>Create domain-specialised models without massive compute resources</li>\n<li>Systematically inject new capabilities through structured knowledge representation</li>\n<li>Validate improvements through quantitative benchmarks</li>\n<li>Deploy fine-tuned models with minimal operational overhead</li>\n</ul>\n<h3 id=\"limitations-and-considerations\">Limitations and Considerations</h3>\n<ul>\n<li><p><strong>Model Constraints</strong>: Currently supports models up to 7B parameters effectively</p>\n</li>\n<li><p><strong>Resource Timeline</strong>: Typical deployment cycle from setup to production:</p>\n<ul>\n<li>Initial setup: a few hours</li>\n<li>Synthetic Data generation: 15 minutes to 1+ hours depending on computing resources </li>\n<li>Training: several hours on consumer hardware</li>\n<li>Evaluation and deployment: a few hours</li>\n</ul>\n</li>\n<li><p><strong>Maintenance Requirements</strong>:</p>\n<ul>\n<li>Regular model evaluations against new benchmarks</li>\n<li>Periodic retraining with updated taxonomy</li>\n<li>System updates and dependency management</li>\n<li>Storage management for checkpoints and datasets</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"rag-vs-fine-tuning\">RAG vs Fine-tuning</h3>\n<p>It&#39;s important to recognise that fine-tuning isn&#39;t always the optimal solution. For dynamic, frequently changing knowledge bases, Retrieval-Augmented Generation (RAG) often provides a more practical and maintainable solution. Fine-tuning through <a href=\"https://instructlab.ai/\">InstructLab</a> is most valuable for:</p>\n<ul>\n<li>Stable knowledge domains (e.g., natural sciences, engineering)</li>\n<li>Consistent skill enhancement needs</li>\n<li>Cases where inference latency is critical</li>\n</ul>\n<p>The system&#39;s architecture strikes a careful balance between computational efficiency and training effectiveness, making it a practical tool for both experimentation and production use. While not eliminating the complexity of LLM fine-tuning entirely, <a href=\"https://instructlab.ai/\">InstructLab</a> significantly reduces the technical barriers to entry in this crucial domain.</p>\n<hr>\n<p>[^1]: Python version compatibility remains a significant consideration in the ML ecosystem. While newer versions (≥3.12) offer improved performance, they often lack compatibility with essential ML frameworks. This constraint informs <a href=\"https://instructlab.ai/\">InstructLab</a>&#39;s current version requirements.</p>\n"
  },
  {
    "title": "💡 TIL: Understanding GGUF Model Quantisation",
    "date": "2024-12-07T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "energy-reduction",
      "performance",
      "quantisation"
    ],
    "url": "/posts/TIL-llm-quantisation.html",
    "content": "<p><strong>TL;DR:</strong> GGUF quantisation converts LLM weights from 16-bit to lower precision formats (2-bit to 6-bit) to run large models on consumer hardware. Each format offers different tradeoffs between size, speed, and quality, with Q4_K_S (4-bit) representing the sweet spot for most users—providing 3.7x size reduction while maintaining good quality. Mixed precision strategies (_S/_M/_L variants) further optimize performance by targeting attention and feed-forward layers with higher precision bits.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>When experimenting with larger language models (12B, 30B, 70B etc.), choosing the right quantisation format becomes crucial for striking a good balance i.e. running them on consumer hardware while maintaining reasonably good performance. I wrote this guide after spending time looking up different GGUF quantisation types to optimise model selection for my machine&#39;s constraints. This guide explains quantisation methods and their practical tradeoffs to help the reader select the optimal format for their setup.<br>The quantisation formats discussed here are implemented in popular frameworks like <a href=\"https://github.com/ggerganov/llama.cpp\">llama.cpp</a>. Q4_K_S is typically the default format due to its good balance of size, speed, and quality, while Q2_K and Q3_K variants are offered for more constrained systems.</p>\n<h2 id=\"what-is-quantisation\">What is Quantisation?</h2>\n<p>Quantisation converts model weights from 16-bit floating point (F16) to lower precision formats using fixed-size blocks. Each block contains multiple weights that share scaling parameters.<br>Perplexity is the key metric used to measure model quality after quantisation. It indicates how well the model predicts text, the lower the perplexity the better the predictions. For example, a change from 5.91 to 6.78 perplexity represents a noticeable but often acceptable drop in prediction quality. A model with perplexity 6.78 is slightly less certain about its predictions than one with perplexity 5.91.</p>\n<h2 id=\"basic-quantisation-types-and-k-quantisation\">Basic Quantisation Types and K-Quantisation</h2>\n<p>K-quantisation is a way to make AI models smaller using two methods to store weights (the model&#39;s numbers):</p>\n<ol>\n<li>Type-0 (simpler): reconstructs weight as <code>weight = scale × quant</code></li>\n<li>Type-1 (more precise): reconstructs weight as <code>weight = scale × quant + minimum</code></li>\n</ol>\n<p>The &quot;block minimum&quot; <code>minimum</code> is the smallest value found in a group of weights. By tracking this minimum, we can represent the other values more precisely relative to it, rather than having to represent their full absolute values.</p>\n<p>Each format groups weights into &quot;super-blocks&quot; to save space. Specifically:</p>\n<p>Q2_K (2-bit):</p>\n<ul>\n<li>Uses Type-1 formula</li>\n<li>Organises weights in groups of 256 (16 blocks × 16 weights)</li>\n<li>Uses 4 bits to store both scales and minimums</li>\n<li>Takes exactly 2.5625 bits per weight</li>\n<li>Result: Shrinks a 13GB model to 2.67GB, but quality drops (perplexity increases from 5.91 to 6.78)</li>\n</ul>\n<p>Q3_K (3-bit):</p>\n<ul>\n<li>Uses Type-0 formula (simpler one)</li>\n<li>Same organisation: 16 blocks × 16 weights</li>\n<li>Uses 6 bits to store scales</li>\n<li>Takes exactly 3.4375 bits per weight</li>\n<li>Better quality than Q2_K but bigger file size</li>\n</ul>\n<p>Q4_K (4-bit):</p>\n<ul>\n<li>Uses Type-1 formula</li>\n<li>Different organisation: 8 blocks × 32 weights = 256 total</li>\n<li>Uses 6 bits for both scales and minimums</li>\n<li>Takes exactly 4.5 bits per weight</li>\n<li>Much better quality, file size around 3.56GB</li>\n</ul>\n<p>Q5_K (5-bit):</p>\n<ul>\n<li>Uses Type-1 formula</li>\n<li>Same organisation as Q4_K</li>\n<li>Also uses 6 bits for scales and minimums</li>\n<li>Takes exactly 5.5 bits per weight</li>\n<li>Quality getting very close to original</li>\n</ul>\n<p>Q6_K (6-bit):</p>\n<ul>\n<li>Uses Type-0 formula</li>\n<li>Back to 16 blocks × 16 weights</li>\n<li>Uses 8 bits for scales</li>\n<li>Takes exactly 6.5625 bits per weight</li>\n<li>Almost perfect quality, file size 5.15GB</li>\n</ul>\n<p>The main tradeoff: Fewer bits means smaller files but lower quality. More bits means better quality but larger files. This lets users choose what works best for their needs.<br>When compressing numbers in Type-1 quantisation, each block keeps track of its smallest value (the minimum). When reconstructing the weights, this minimum is added back after multiplication. This helps preserve the range of values more accurately than just using scaling alone.</p>\n<p>A simple way to think of this concept is:</p>\n<ul>\n<li>Type-0 just stretches/shrinks values using a scale</li>\n<li>Type-1 first shifts all numbers by subtracting the minimum (making them smaller), then scales them for storage, and when reconstructing adds the minimum back</li>\n</ul>\n<p>This is why Type-1 generally gives better quality results but needs more storage space. It has to keep track of both the scale and minimum for each block.</p>\n<h2 id=\"mixed-precision-strategies\">Mixed Precision Strategies</h2>\n<p>K-quantisations use different precision levels for different model components. From <a href=\"https://github.com/ggerganov/llama.cpp\">llama.cpp</a> documentation, there are three variants:</p>\n<ul>\n<li><p>S (Small): Uses single quantisation throughout\nExample using Q3_K_S:  </p>\n<blockquote>\n<p>All model tensors → Q3_K (3-bit)<br>Result: 2.75GB size, 6.46 perplexity (7B model)  </p>\n</blockquote>\n</li>\n<li><p>M (Medium): Strategic mixed precision\nExample using Q3_K_M:  </p>\n<blockquote>\n<p>attention.wv[^1], attention.wo[^2], feed_forward.w2[^3] → Q4_K (4-bit)<br>All other tensors → Q3_K (3-bit)<br>Result: 3.06GB size, 6.15 perplexity (7B model)  </p>\n</blockquote>\n</li>\n<li><p>L (Large): Higher precision mix\nExample using Q3_K_L:  </p>\n<blockquote>\n<p>attention.wv[^1], attention.wo[^2], feed_forward.w2[^3] → Q5_K (5-bit)<br>All other tensors → Q3_K (3-bit)<br>Result: 3.35GB size, 6.09 perplexity (7B model)</p>\n</blockquote>\n</li>\n</ul>\n<p>These strategies target attention and feed-forward layers with higher precision because they directly impact text processing quality, as demonstrated by the perplexity improvements in benchmarks: Q3_K_S (6.46) → Q3_K_M (6.15) → Q3_K_L (6.09).<br>The improvement in perplexity scores demonstrates why mixed precision strategies are effective, though they require more storage space.</p>\n<h2 id=\"performance-comparison-7b-model\">Performance Comparison (7B model)</h2>\n<pre><code>Format | Size(GB) | Reduction | BPW  | Perplexity | RTX4080  | M2Max   \nF16    | 13.0     | 1.0x      | 16.0 | 5.91       | 60.0ms   | 116ms\nQ2_K   | 2.67     | 4.9x      | 2.56 | 6.78       | 15.5ms   | 56ms\nQ3_K_S | 2.75     | 4.7x      | 3.44 | 6.46       | 18.6ms   | 81ms\nQ4_K_S | 3.56     | 3.7x      | 4.50 | 6.02       | 15.5ms   | 50ms\nQ6_K   | 5.15     | 2.5x      | 6.56 | 5.91       | 18.3ms   | 75ms\n</code></pre>\n<p>*BPW = Bits Per Weight, Speed in milliseconds per token</p>\n<p>Practical Recommendations:</p>\n<ul>\n<li>Balanced Performance: Q4_K_S</li>\n<li>Maximum Compression: Q2_K</li>\n<li>Best Quality: Q6_K (matches F16)</li>\n<li>Limited RAM: Q2_K or Q3_K</li>\n<li>GPU Inference: Q4_K (optimal speed/quality)</li>\n</ul>\n<p>All data are from recent <a href=\"https://github.com/ggerganov/llama.cpp/pull/1684\">llama.cpp</a> performance benchmarks and <a href=\"https://github.com/ggerganov/ggml\">GGML</a> implementation details.</p>\n<h2 id=\"memory-requirements-for-inference\">Memory Requirements for Inference</h2>\n<p>When running quantised models, more RAM is required than the model size alone for inference overhead. Memory requirements depend on several factors:</p>\n<ul>\n<li>Model architecture and size</li>\n<li>Batch size for inference</li>\n<li>Number of layers loaded at once</li>\n<li>Operating system and framework overhead</li>\n</ul>\n<p>For 7B models (verified from benchmarks):</p>\n<pre><code>Format | Model Size | Note\nF16    | 13.0GB    | Base format\nQ4_K_S | 3.56GB    | Common choice\nQ3_K_S | 2.75GB    | Minimum size\nQ6_K   | 5.15GB    | Highest quality\n</code></pre>\n<p>For larger models scale the memory requirements proportionally and ensure additional overhead memory is available for inference. Test with smaller models first to gauge the system&#39;s capabilities.<br>Actual RAM/VRAM requirements will be higher than the model size. Consider monitoring memory usage during inference to determine exact requirements for a specific setup.<br>Here is an example memory usage scenario for a Q4_K_S 7B model:</p>\n<ul>\n<li>Model size: 3.56GB</li>\n<li>Inference overhead: ~2GB for standard settings</li>\n<li>Operating system buffer: ~1GB recommended</li>\n<li>Total recommended free memory: ~7GB</li>\n</ul>\n<p>This explains why a model that&#39;s &quot;3.56GB&quot; might need 6-7GB of free RAM/VRAM to run smoothly. The exact overhead varies based on your settings and system.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Modern quantisation techniques offer multiple ways to run large language models on consumer hardware. Here&#39;s what we need to remember:</p>\n<ul>\n<li>K-quantisation provides the best balance through super-blocks and mixed precision strategies</li>\n<li>Q4_K_S (4-bit) represents the current sweet spot for most users, offering:<ul>\n<li>3.7x size reduction</li>\n<li>Good perplexity (6.02)</li>\n<li>Excellent inference speed on both GPU and CPU</li>\n</ul>\n</li>\n<li>For more constrained setups, Q2_K/Q3_K variants can run larger models with acceptable quality loss</li>\n<li>Higher bits (Q5_K, Q6_K) approach F16 quality but require more memory</li>\n<li>The _S/_M/_L variants let the user fine-tune the quality-size tradeoff by adjusting precision where it matters most</li>\n</ul>\n<p>Before downloading a quantised model, check the system&#39;s available RAM and choose a format that leaves enough memory for comfortable operation. For most users with modern GPUs, Q4_K variants will provide the best experience.</p>\n<hr>\n<p>[^1]: In <a href=\"https://github.com/ggerganov/llama.cpp/tree/master/examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp\">llama.cpp</a>, <code>attention.wv</code> refers to a tensor that holds the weights for the value vectors in the self-attention mechanism of the model. This tensor is crucial for determining how much focus the model places on different parts of the input when generating responses. \n[^2]: <code>attention.wo</code> refers to the weight matrix used in the output layer of the attention mechanism within a transformer model. It plays a crucial role in transforming the attention output into the final representation that is used for generating predictions.\n[^3]: <code>feed_forward.w1</code> projects input to a higher-dimensional space, enabling the capture of complex features. <code>feed_forward.w2</code> projects transformed input back to the original dimension with a non-linear activation function, whereas <code>feed_forward.w3</code> applies an additional transformation to enhance the learning of complex patterns. These matrices collectively enable the feed-forward network to transform and learn from the input effectively, contributing to the overall performance of the transformer model. </p>\n"
  },
  {
    "title": "💡 TIL: LLM Evaluation using Critique Shadowing",
    "date": "2024-12-05T00:00:00.000Z",
    "tags": [
      "til",
      "llm",
      "ai",
      "machine-learning",
      "mlops",
      "best-practices",
      "production",
      "model-governance",
      "evaluation",
      "observability",
      "monitoring",
      "quality-assurance",
      "iterative-refinement"
    ],
    "url": "/posts/TIL-llm-eval-critique-shadowing.html",
    "content": "<p><strong>TL;DR:</strong> Critique Shadowing offers an expert-centered approach to LLM evaluation by starting with binary pass/fail judgments and detailed critiques before building automated systems. This iterative methodology—reminiscent of 1970s knowledge engineering—prioritizes domain expertise over complex metrics, revealing valuable insights about products and users while developing reliable evaluation systems that capture nuanced quality standards.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>As LLMs increasingly drive critical business decisions, ensuring their reliability becomes paramount. Many teams struggle with complex metrics and scoring systems that lead to confusion rather than clarity. <a href=\"https://hamel.dev/\">Hamel Husain</a>&#39;s Critique Shadowing methodology[^1] offers a systematic path from drowning in metrics to developing reliable evaluation systems.</p>\n<h2 id=\"the-critique-shadowing-method\">The Critique Shadowing Method</h2>\n<p>The key insight behind Critique Shadowing is deceptively simple: start with binary (pass/fail) expert judgements and detailed critiques before building automated evaluation systems. This approach solves two critical challenges: capturing domain expertise and scaling evaluation processes.</p>\n<p>This expert-centric approach echoes <a href=\"https://en.wikipedia.org/wiki/Knowledge_engineering\">knowledge engineering</a> practices from the 1970-80s, when AI researchers first recognised the necessity of systematically capturing domain expertise. Just as <a href=\"https://en.wikipedia.org/wiki/Mycin\">MYCIN</a>&#39;s creators worked closely with medical doctors to encode diagnostic knowledge, Critique Shadowing similarly structures the process of extracting expert judgement for LLM evaluation. While the technology has evolved from rule-based systems to large language models, the fundamental challenge of effectively capturing and operationalising expert knowledge remains central.</p>\n<h3 id=\"implementation-process\">Implementation Process</h3>\n<p>The methodology follows a structured, iterative process:</p>\n<center>\n    <img src=\"https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/refs/heads/master/images/Critique%20Framework%20Hamel%20Husain.png\" width=\"80%\" height=\"80%\"/>\n</center>\n\n<ol>\n<li>Identify a principal domain expert as the arbiter of quality</li>\n<li>Create a diverse dataset covering different scenarios and user types</li>\n<li>Expert conducts binary pass/fail judgements with detailed critiques</li>\n<li>Address discovered issues and verify fixes</li>\n<li>Develop LLM-based judges using expert critiques as few-shot examples</li>\n<li>Analyse error patterns and root causes</li>\n<li>Create specialised judges for persistent issues</li>\n</ol>\n<p>The process is continuous, repeating periodically or when material changes occur. For simpler applications or when manual review is feasible, teams can adapt or streamline these steps while maintaining the core principle of systematic data examination.</p>\n<h2 id=\"beyond-automation\">Beyond Automation</h2>\n<p>Husain&#39;s most striking observation is that the process of developing evaluation systems often provides more value than the resulting automated judges. The systematic collection of expert feedback reveals product insights, user needs, and failure modes that might otherwise remain hidden. This understanding drives improvements in the core system, not just its evaluation.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The Critique Shadowing methodology succeeds by prioritising expert knowledge and systematic data collection over premature automation. For teams building LLM applications, this approach offers a clear path to reliable evaluation systems while simultaneously deepening their understanding of their product and users.<br>LLM evaluation is an active area of interest and research both in academia and industry. Here is a short list of resources to look into: </p>\n<ul>\n<li><a href=\"https://www.ibm.com/think/topics/llm-evaluation\">IBM LLM Evaluation</a></li>\n<li><a href=\"https://docs.mistral.ai/guides/evaluation/\">Mistral AI - Evaluation</a></li>\n<li><a href=\"https://github.com/mistralai/mistral-evals\">Mistral Evals</a></li>\n<li><a href=\"https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool\">Anthropic - Using the Evaluation Tool</a></li>\n<li><a href=\"https://dev.to/guybuildingai/-top-5-open-source-llm-evaluation-frameworks-in-2024-98m\">Top 5 Open-Source LLM Evaluation Frameworks in 2024</a></li>\n</ul>\n<hr>\n<p>[^1]: Husain, H. (2024). &quot;Creating a LLM-as-a-Judge That Drives Business Results&quot; <a href=\"https://hamel.dev/blog/posts/llm-judge/\">https://hamel.dev/blog/posts/llm-judge/</a></p>\n"
  },
  {
    "title": "✍ A Path to Maintainable AI Systems using Norman's Design Principles",
    "date": "2024-12-03T00:00:00.000Z",
    "tags": [
      "ai",
      "data-science",
      "design-principles",
      "code-quality",
      "mlops",
      "monitoring",
      "observability",
      "production",
      "model-governance",
      "minimal"
    ],
    "url": "/posts/design-principles-ds-ai.html",
    "content": "<p><strong>TL;DR:</strong> Don Norman&#39;s timeless design principles - visibility, feedback, constraints, mappings, and error prevention - apply powerfully to AI systems, where abstract interfaces and complex workflows often become overwhelming. By implementing these principles with a carefully selected, minimal toolset, we can create maintainable, observable AI systems that reduce complexity while providing comprehensive functionality - just as Norman observed in physical objects, good design in AI leads to fewer errors and greater user satisfaction.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Don Norman&#39;s principles of good design, outlined in <a href=\"https://archive.org/details/thedesignofeverydaythingsbydonnorman\">The Design of Everyday Things</a>, are particularly relevant to Data Science and AI Engineering, where systems often suffer from unnecessary complexity. This article presents a minimalist approach to implementing these principles using a carefully selected set of tools that maximise impact while reducing operational overhead. Norman&#39;s insights about visibility, feedback, constraints, and mappings translate powerfully to AI system design, where abstract interfaces and complex workflows can easily become overwhelming. Just as Norman observed that poorly designed physical objects lead to user frustration and errors, poorly architected AI systems can result in maintenance nightmares, hidden failure modes, and costly debugging cycles. By applying his principles - making system states visible, providing clear feedback, implementing appropriate constraints, and creating natural mappings between components, we can build AI systems that are not only more intuitive to use but also easier to maintain, debug, and evolve over time.</p>\n<h2 id=\"design-principles-implementation\">Design Principles Implementation</h2>\n<h3 id=\"1-visibility\">1. Visibility</h3>\n<p>Implement comprehensive system observability using <a href=\"https://mlflow.org/\">MLflow</a> as your central platform:</p>\n<ul>\n<li>Track experiments, parameters, and metrics</li>\n<li>Version models and artefacts</li>\n<li>Log production predictions and outcomes</li>\n<li>Monitor model performance metrics</li>\n</ul>\n<p>For system-level metrics, use <a href=\"https://prometheus.io/docs/visualization/grafana/\">Prometheus/Grafana</a> to:</p>\n<ul>\n<li>Track resource utilisation (CPU, memory, latency)</li>\n<li>Monitor prediction throughput</li>\n<li>Create dashboards for key performance indicators</li>\n</ul>\n<p>Implement adaptive sampling for high-volume systems:</p>\n<pre><code class=\"language-python\">def should_log(request_id, sampling_rate=0.1):\n    return hash(request_id) % 100 &lt; (sampling_rate * 100)\n</code></pre>\n<h3 id=\"2-feedback\">2. Feedback</h3>\n<p>Use <a href=\"https://prometheus.io/docs/visualization/grafana/\">Prometheus/Grafana</a> for real-time monitoring and alerting:</p>\n<ul>\n<li>Set up alerts for model performance degradation</li>\n<li>Monitor data distribution shifts</li>\n<li>Track system health metrics</li>\n<li>Configure tiered alerting based on severity</li>\n</ul>\n<p>Example metric collection:</p>\n<pre><code class=\"language-python\">from prometheus_client import Counter, Histogram\n\nPREDICTIONS = Counter(&#39;model_predictions_total&#39;, &#39;Total predictions made&#39;)\nLATENCY = Histogram(&#39;prediction_latency_seconds&#39;, &#39;Time spent processing prediction&#39;)\n\ndef predict(features):\n    with LATENCY.time():\n        prediction = model.predict(features)\n        PREDICTIONS.inc()\n        return prediction\n</code></pre>\n<h3 id=\"3-constraints\">3. Constraints</h3>\n<p>Implement data and model guardrails using <a href=\"https://greatexpectations.io/\">Great Expectations</a>:</p>\n<ul>\n<li>Define data quality expectations</li>\n<li>Set distribution bounds for features</li>\n<li>Monitor for data drift</li>\n<li>Generate validation reports</li>\n</ul>\n<p>Example constraint implementation:</p>\n<pre><code class=\"language-python\">from great_expectations.dataset import Dataset\n\ndef validate_features(df):\n    dataset = Dataset(df)\n    dataset.expect_column_values_to_be_between(&quot;age&quot;, 0, 120)\n    dataset.expect_column_values_to_not_be_null(&quot;critical_feature&quot;)\n    validation_result = dataset.validate()\n    return validation_result.success\n</code></pre>\n<h3 id=\"4-mappings\">4. Mappings</h3>\n<p>Use <a href=\"https://mlflow.org/\">MLflow</a> to maintain clear relationships between:</p>\n<ul>\n<li>Experiments and business objectives</li>\n<li>Models and their training data</li>\n<li>Predictions and outcomes</li>\n<li>Performance metrics and business KPIs</li>\n</ul>\n<p>Example mapping structure:</p>\n<pre><code class=\"language-python\">with mlflow.start_run(run_name=&quot;production_model_v1&quot;):\n    mlflow.log_param(&quot;business_objective&quot;, &quot;customer_churn&quot;)\n    mlflow.log_param(&quot;data_version&quot;, data_hash)\n    mlflow.log_metric(&quot;business_impact&quot;, revenue_improvement)\n    mlflow.log_artifact(&quot;feature_importance.json&quot;)\n</code></pre>\n<h3 id=\"5-error-prevention-and-recovery\">5. Error Prevention and Recovery</h3>\n<p>Integrate safeguards using your core toolset:</p>\n<p><a href=\"https://mlflow.org/\">MLflow</a>:</p>\n<ul>\n<li>Version control for models and artefacts</li>\n<li>Rollback capabilities</li>\n<li>Experiment tracking for reproducibility</li>\n</ul>\n<p><a href=\"https://prometheus.io/docs/visualization/grafana/\">Prometheus/Grafana</a>:</p>\n<ul>\n<li>Early warning system for issues</li>\n<li>Performance degradation detection</li>\n<li>Resource exhaustion prevention</li>\n</ul>\n<p><a href=\"https://greatexpectations.io/\">Great Expectations</a>:</p>\n<ul>\n<li>Data quality validation</li>\n<li>Schema enforcement</li>\n<li>Distribution monitoring</li>\n</ul>\n<p>Example error prevention:</p>\n<pre><code class=\"language-python\">def safe_predict(features):\n    if not validate_features(features):\n        return fallback_prediction()\n    \n    try:\n        with LATENCY.time():\n            prediction = model.predict(features)\n            PREDICTIONS.inc()\n            return prediction\n    except Exception as e:\n        ERROR_COUNTER.inc()\n        return fallback_prediction()\n</code></pre>\n<h2 id=\"implementation-strategy\">Implementation Strategy</h2>\n<ol>\n<li><p>Start with <a href=\"https://mlflow.org/\">MLflow</a></p>\n<ul>\n<li>Set up experiment tracking</li>\n<li>Implement model versioning</li>\n<li>Configure basic logging</li>\n</ul>\n</li>\n<li><p>Add <a href=\"https://prometheus.io/docs/visualization/grafana/\">Prometheus/Grafana</a></p>\n<ul>\n<li>Deploy basic monitoring</li>\n<li>Set up key alerts</li>\n<li>Create essential dashboards</li>\n</ul>\n</li>\n<li><p>Integrate <a href=\"https://greatexpectations.io/\">Great Expectations</a></p>\n<ul>\n<li>Define core data quality rules</li>\n<li>Implement validation pipelines</li>\n<li>Monitor data distributions</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>By focusing on a minimal set of powerful tools (<a href=\"https://mlflow.org/\">MLflow</a>, <a href=\"https://prometheus.io/docs/visualization/grafana/\">Prometheus/Grafana</a>, and <a href=\"https://greatexpectations.io/\">Great Expectations</a>), you can implement Norman&#39;s design principles effectively while maintaining system simplicity. This approach provides:</p>\n<ul>\n<li>Comprehensive visibility through unified logging and monitoring</li>\n<li>Immediate feedback via real-time alerts</li>\n<li>Strong constraints through data validation</li>\n<li>Clear mappings between components</li>\n<li>Robust error prevention and recovery</li>\n</ul>\n<p>The key is to fully utilise these core tools rather than adding complexity with additional solutions. This creates maintainable, observable, and reliable AI systems that can scale with your needs.</p>\n"
  },
  {
    "title": "🐼 Pandas or 🐻‍❄️ Polars?",
    "date": "2024-12-02T00:00:00.000Z",
    "tags": [
      "python",
      "pandas",
      "polars",
      "data-processing",
      "code-quality",
      "toolchain",
      "data-science"
    ],
    "url": "/posts/pandas-polars.html",
    "content": "<p><strong>TL;DR:</strong> While Pandas excels at interactive exploration and smaller datasets with its Python-centric ecosystem, Polars leverages Rust&#39;s performance for parallel processing and memory efficiency in large-scale data operations. Choose Pandas for rapid prototyping and datasets under 1GB, and Polars for production environments with demanding performance requirements or cross-language development needs in Python, Node.js, and Rust.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>The world of Python data processing has long revolved around the well established <a href=\"https://pandas.pydata.org/\">Pandas</a> library, but in recent years, a new contender has emerged in the form of <a href=\"https://pola.rs/\">Polars</a>. This post aims to provide a comparison of these two powerful data processing tools, that empowers the reader to make an informed choice on a case-by-case basis.  </p>\n<h2 id=\"architecture-and-design-comparison\">Architecture and Design Comparison</h2>\n<p>At the core, Pandas and Polars differ in their underlying implementation and design philosophies.</p>\n<h3 id=\"implementation-and-performance\">Implementation and Performance</h3>\n<p>The Pandas library is written in Python/Cython, with a focus on single-threaded operations. In contrast, Polars is built upon the Rust programming language, leveraging its performance and concurrency capabilities to enable parallel processing by default.<br>This distinction in implementation has significant implications for memory management and query optimisation. Pandas typically works with multiple copies of data, while Polars utilises the Arrow data format, which allows for more efficient memory usage. Additionally, Polars offers automatic query optimisation, whereas Pandas users must rely on a more sequential, manual approach to optimising their data processing pipelines.</p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Pandas</th>\n<th>Polars</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Implementation</td>\n<td>Python/Cython</td>\n<td>Rust</td>\n</tr>\n<tr>\n<td>Processing</td>\n<td>Single-threaded</td>\n<td>Parallel by default</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Multiple copies</td>\n<td>Arrow format</td>\n</tr>\n<tr>\n<td>Query Optimisation</td>\n<td>Sequential</td>\n<td>Automatic</td>\n</tr>\n</tbody></table>\n<h3 id=\"api-and-language-support\">API and Language Support</h3>\n<p>The API and language support differences between Pandas and Polars are quite notable. Pandas -being a Python-only library- offers a mix of method chaining and attribute access approaches. In contrast, Polars takes a more expansive approach, providing implementations in Python, Node.js, and the Rust programming language itself.<br>This language versatility of Polars enables seamless JavaScript and TypeScript integration, allowing data scientists and developers to leverage the same performance benefits regardless of their preferred language. Additionally, Polars maintains a consistent method chaining syntax across these different language environments, simplifying the learning curve for users who may work with the library in multiple contexts.</p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Pandas</th>\n<th>Polars</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Language Support</td>\n<td>Python-only</td>\n<td>Python, Node.js, Rust</td>\n</tr>\n<tr>\n<td>API Style</td>\n<td>Mixed method chaining and attribute access</td>\n<td>Consistent method chaining</td>\n</tr>\n<tr>\n<td>Language Integration</td>\n<td>N/A</td>\n<td>JavaScript/TypeScript</td>\n</tr>\n</tbody></table>\n<h2 id=\"use-cases-and-trade-offs\">Use Cases and Trade-offs</h2>\n<p>While both Pandas and Polars excel in the realm of data processing, each library has distinct strengths and weaknesses that make them better suited for different use cases and scenarios.</p>\n<h3 id=\"when-to-choose-pandas\">When to Choose Pandas</h3>\n<p>Pandas shines when it comes to interactive data exploration and working with smaller datasets, typically under 1GB in size. The library&#39;s deep integration with the broader scientific computing ecosystem in Python, along with its intuitive syntax and extensive documentation, make it an excellent choice for rapid prototyping, educational contexts, and projects that require seamless compatibility with the Python-centric data science toolchain.</p>\n<h3 id=\"when-to-choose-polars\">When to Choose Polars</h3>\n<p>On the other hand, Polars emerges as the preferred choice for large-scale data processing, particularly for datasets exceeding 1GB. The library&#39;s Rust-based implementation and parallel processing capabilities make it a more suitable option for production environments with demanding performance requirements. Polars also excels in memory-constrained systems, thanks to its efficient use of the Arrow data format, and it is an attractive choice for cross-language development teams due to its implementations in Python, Node.js, and Rust.<br>Furthermore, Polars demonstrates strengths in handling complex data transformations and time series processing at scale, areas where its optimised query engine and parallel processing features can truly shine.</p>\n<p>To summarise the key differences:  </p>\n<table>\n<thead>\n<tr>\n<th>Consideration</th>\n<th>Pandas</th>\n<th>Polars</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Dataset Size</td>\n<td>Small to medium (&lt;1GB)</td>\n<td>Scales to larger datasets</td>\n</tr>\n<tr>\n<td>Performance</td>\n<td>Suitable for interactive exploration</td>\n<td>Excels at large-scale processing</td>\n</tr>\n<tr>\n<td>Memory Efficiency</td>\n<td>Works with multiple data copies</td>\n<td>Utilises Arrow format for efficiency</td>\n</tr>\n<tr>\n<td>Query Optimisation</td>\n<td>Sequential, manual approach</td>\n<td>Automatic optimisation</td>\n</tr>\n<tr>\n<td>Language Support</td>\n<td>Python-only</td>\n<td>Python, Node.js, Rust</td>\n</tr>\n<tr>\n<td>Ecosystem Integration</td>\n<td>Strong in Python scientific computing</td>\n<td>Limited cross-language integration</td>\n</tr>\n<tr>\n<td>Learning Resources</td>\n<td>Extensive documentation and community support</td>\n<td>Younger ecosystem, less comprehensive resources</td>\n</tr>\n</tbody></table>\n<p>Ultimately, the choice between Pandas and Polars should be guided by the specific requirements of your project, such as data volume, performance needs, language preferences, and ecosystem integration requirements. Both libraries offer powerful data processing capabilities, and selecting the right one can significantly impact the success and efficiency of your data-driven initiatives.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>After carefully evaluating the key differences between Pandas and Polars, the choice between the two data processing libraries ultimately comes down to the specific requirements of your project and use case.<br>For projects focused on interactive data exploration and working with smaller datasets (under 1GB), Pandas remains the go-to choice. Its deep integration with the broader Python scientific computing ecosystem, extensive documentation, and large community make it a reliable and familiar option for many data scientists and developers.<br>However, for large-scale data processing, production environments, and cross-language teams, Polars presents a compelling alternative. Its performance advantages, memory efficiency, and multi-language support (Python, Node.js, Rust) make it an increasingly attractive choice for modern data-intensive applications.<br>When deciding between Pandas and Polars, consider factors such as dataset size, performance requirements, memory constraints, language preferences, and the level of ecosystem integration needed. Pandas may be the better fit for projects focused on rapid prototyping and educational use, while Polars can shine in mission-critical, large-scale data processing tasks.<br>Ultimately, both Pandas and Polars are powerful data processing tools, and the choice between them should be guided by the specific needs and constraints of your project. As the data processing landscape continues to evolve, it&#39;s valuable to stay informed about the trade-offs and emerging alternatives to ensure you make the most informed decision for your team and organisation.</p>\n"
  },
  {
    "title": "📊 Ten Ways to Model Data",
    "date": "2024-11-27T00:00:00.000Z",
    "tags": [
      "modelling-mindsets",
      "data-science",
      "ai",
      "data-modeling",
      "neural-network",
      "best-practices",
      "statistics",
      "machine-learning",
      "decision-making"
    ],
    "url": "/posts/modelling-mindsets.html",
    "content": "<p><strong>TL;DR:</strong> This comprehensive guide explores ten distinct modelling approaches across statistics, machine learning, and causal inference-advocating for &quot;T-shaped&quot; expertise where practitioners develop deep knowledge in one or two mindsets aligned with their domain needs whilst maintaining sufficient breadth to recognise when different approaches are required, with specific recommendations for research, business, and product development contexts.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>As a practitioner looking to work effectively with real-world data and generate meaningful insights, I face a crucial decision: which modelling approaches should I invest my time and energy in learning? After discovering Christoph Molnar&#39;s <a href=\"https://christophmolnar.com/books/modeling-mindsets/\">Modeling Mindsets</a>, I realised this isn&#39;t about picking the &quot;best&quot; approach. It&#39;s about becoming what he calls a &quot;T-shaped modeller&quot;.<br>The concept is elegantly simple: rather than trying to master every possible approach (impossible) or limiting myself to just one (ineffective), I should aim to develop:</p>\n<ul>\n<li>Deep expertise in one or two mindsets that align with my goals and problems</li>\n<li>Working knowledge of other approaches to recognise when my primary tools aren&#39;t optimal</li>\n</ul>\n<p>This systematic exploration serves two purposes:  </p>\n<ol>\n<li>To understand the landscape: What are the main modelling mindsets available today? What are their core premises, strengths, and limitations?</li>\n<li>To make an informed choice: Which mindset(s) should I focus on mastering, given my goals and constraints?</li>\n</ol>\n<p>Each mindset represents a different way of approaching problems through data. From the probability-focused world of statistical modelling to the interactive realm of reinforcement learning, from the causality-oriented approach to the pattern-finding nature of unsupervised learning, each offers unique tools and perspectives.<br>By examining these mindsets systematically, I aim to make an informed decision about where to focus my learning efforts while maintaining enough breadth to recognise when I should switch approaches. This isn&#39;t just about theoretical understanding, it&#39;s about practical effectiveness in solving real-world problems.</p>\n<p>Let&#39;s explore each mindset in turn, focusing on their fundamental premises, key strengths, and limitations to guide this decision.</p>\n<h2 id=\"statistical-modelling-the-foundation-of-data-driven-inference\">Statistical Modelling: The Foundation of Data-Driven Inference</h2>\n<p><em>This mindset sees the world through probability distributions. At its core, it&#39;s about modelling how data is generated and making inferences under uncertainty.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Everything has a distribution, from dice rolls to customer behaviours</li>\n<li>Models encode assumptions about how data is generated</li>\n<li>Models are evaluated by both checking if their assumptions make sense and measuring how well they match the data</li>\n<li>Uses same data for fitting and evaluation, unlike machine learning approaches</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Provides rigorous mathematical framework for handling uncertainty</li>\n<li>Strong theoretical foundation spanning decades of research</li>\n<li>Forces explicit consideration of data-generating processes</li>\n<li>Versatile for decisions, predictions, and understanding</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Manual and often tedious modelling process</li>\n<li>Struggles with complex data types like images and text</li>\n<li>Good model fit doesn&#39;t guarantee good predictions</li>\n<li>Less automatable than modern machine learning approaches</li>\n</ol>\n<p>This mindset serves as the foundation for three important sub-approaches: Frequentism, Bayesianism, and Likelihoodism, each with its own interpretation of probability and evidence. For someone starting in data science, understanding statistical modelling provides crucial groundwork for understanding both traditional statistics and modern machine learning approaches.</p>\n<h2 id=\"frequentism-making-decisions-through-repeated-experiments\">Frequentism: Making Decisions Through Repeated Experiments</h2>\n<p><em>Frequentism views probability as long-run frequency and assumes that parameters in the world are fixed but unknown. It&#39;s the dominant approach in many scientific fields, particularly in medicine and psychology.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Interprets probability as frequency in infinite repetitions</li>\n<li>Makes decisions through hypothesis tests and confidence intervals</li>\n<li>Relies on &quot;imagined experiments&quot; to draw conclusions</li>\n<li>Focuses on estimating fixed, true parameters</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Enables clear, binary decisions</li>\n<li>Computationally fast compared to other approaches</li>\n<li>No need for prior information</li>\n<li>Widely accepted in scientific research</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Often oversimplifies complex questions into yes/no decisions</li>\n<li>Vulnerable to p-hacking (searching for significant results)</li>\n<li>Interpretation can be counterintuitive, especially for confidence intervals</li>\n<li>Results depend on the experimental design, not just the data</li>\n</ol>\n<p>For practitioners, Frequentism offers a well-established framework with clear decision rules and strong scientific acceptance. However, its limitations in handling uncertainty and tendency toward oversimplification have led to growing interest in alternative approaches like Bayesian inference.</p>\n<h2 id=\"bayesianism-learning-through-updated-beliefs\">Bayesianism: Learning Through Updated Beliefs</h2>\n<p><em>Bayesianism stands out by treating parameters themselves as random variables with distributions, fundamentally different from Frequentism&#39;s fixed-parameter view. It focuses on updating beliefs about parameters as new data arrives.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Requires prior distributions before seeing data</li>\n<li>Updates beliefs through Bayes&#39; theorem</li>\n<li>Produces complete posterior distributions, not just point estimates</li>\n<li>Naturally propagates uncertainty through all calculations[^1]</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Can incorporate prior knowledge and expert opinions</li>\n<li>Provides complete probability distributions for parameters</li>\n<li>More intuitive interpretation of uncertainty</li>\n<li>Cleanly separates inference (getting posteriors) from decisions (using them)</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Choosing priors can be difficult and controversial</li>\n<li>Computationally intensive, especially for complex models</li>\n<li>Mathematically more demanding than frequentist approaches</li>\n<li>Can seem like overkill for simple decisions</li>\n</ol>\n<p>Bayesianism offers a more complete and intuitive framework for handling uncertainty, but requires more computational resources and mathematical sophistication. It&#39;s particularly valuable when prior knowledge is important or when understanding full uncertainty is crucial.</p>\n<h2 id=\"likelihoodism-pure-evidence-through-likelihood\">Likelihoodism: Pure Evidence Through Likelihood</h2>\n<p><em>Likelihoodism attempts to reform statistical inference by focusing solely on likelihood as evidence, avoiding both Frequentism&#39;s imagined experiments and Bayesianism&#39;s subjective priors.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Uses likelihood ratios to compare hypotheses</li>\n<li>Adheres strictly to the likelihood principle</li>\n<li>Rejects both prior probabilities and sampling distributions</li>\n<li>Compares models based on their relative evidence</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>More coherent than Frequentism&#39;s mixed toolkit</li>\n<li>Avoids subjective elements of Bayesianism</li>\n<li>Ideas work well within other statistical mindsets</li>\n<li>Adheres to likelihood principle (evidence depends only on observed data)</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Cannot make absolute statements, only relative comparisons</li>\n<li>No clear mechanism for making final decisions</li>\n<li>Lacks tools for expressing beliefs or uncertainty</li>\n<li>Less practical than other statistical approaches</li>\n</ol>\n<p>Likelihoodism offers interesting theoretical insights but may be less immediately useful than Frequentist or Bayesian approaches. It&#39;s more valuable for understanding the foundations of statistical inference than for day-to-day data analysis.</p>\n<h2 id=\"causal-inference-from-association-to-causation\">Causal Inference: From Association to Causation</h2>\n<p><em>Causal inference moves beyond correlation to understand what actually causes observed effects, providing a framework for analysing interventions and their impacts.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Uses Directed Acyclic Graphs (DAGs) to visualise relationships</li>\n<li>Distinguishes between association and causation</li>\n<li>Requires explicit encoding of causal assumptions</li>\n<li>Can work with both statistical models and machine learning</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Addresses fundamental questions about cause and effect</li>\n<li>Makes assumptions explicit through DAGs</li>\n<li>Models tend to be more robust than pure association-based approaches</li>\n<li>Provides clear framework for analysing interventions</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Requires identifying all relevant confounders</li>\n<li>Cannot verify all causal assumptions from data alone</li>\n<li>Multiple competing frameworks can confuse newcomers</li>\n<li>May sacrifice predictive performance for causal understanding</li>\n</ol>\n<p>For practitioners, causal inference is essential when decisions about interventions are needed, though it requires careful consideration of assumptions and domain knowledge. It&#39;s particularly valuable in fields like medicine, policy-making, and business strategy where understanding cause-effect relationships is crucial.</p>\n<h2 id=\"machine-learning-algorithms-learning-from-data\">Machine Learning: Algorithms Learning from Data</h2>\n<p><em>Machine learning approaches problems by having computers learn algorithms from data, focusing on task performance rather than theoretical underpinning.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Computer-first approach to learning from data</li>\n<li>External evaluation based on task performance</li>\n<li>Less constrained by statistical assumptions</li>\n<li>Includes supervised, unsupervised, reinforcement, and deep learning</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Task-oriented and pragmatic approach</li>\n<li>Highly automatable</li>\n<li>Well-suited for building digital products</li>\n<li>Strong industry adoption and career opportunities</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Less principled than statistical approaches</li>\n<li>Many competing approaches can be overwhelming</li>\n<li>Models often prioritise performance over interpretability</li>\n<li>Usually requires substantial data and computation</li>\n</ol>\n<p>For practitioners, machine learning offers powerful tools for automation and prediction, particularly valuable in industry settings. It&#39;s especially useful when theoretical understanding is less important than practical performance.</p>\n<h3 id=\"supervised-learning-the-art-of-prediction\">Supervised Learning: The Art of Prediction</h3>\n<p><em>Supervised learning frames everything as a prediction problem, using labelled data to learn mappings from inputs to outputs.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Learning is optimisation and search in hypothesis space</li>\n<li>Models evaluated on unseen data, not training data</li>\n<li>Focuses on generalising to new cases</li>\n<li>Highly automatable and competition-friendly</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Clear evaluation metrics</li>\n<li>Highly automatable</li>\n<li>Strong performance on prediction tasks</li>\n<li>Well-defined optimisation objectives</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Requires labelled data</li>\n<li>Models often black-box (uninterpretable)</li>\n<li>Not hypothesis-driven</li>\n<li>May miss causal relationships</li>\n<li>Can fail in unexpected ways when patterns change</li>\n</ol>\n<p>For practitioners, supervised learning excels in prediction tasks where good labelled data exists and interpretability isn&#39;t crucial. It&#39;s particularly valuable in industry settings for automation and decision support.</p>\n<h3 id=\"unsupervised-learning-discovering-hidden-patterns\">Unsupervised Learning: Discovering Hidden Patterns</h3>\n<p><em>This mindset focuses on finding inherent structures in data without labelled outcomes, making it ideal for exploratory analysis and pattern discovery.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Discovers patterns in data distributions</li>\n<li>Includes clustering, dimensionality reduction, anomaly detection</li>\n<li>No ground truth for validation</li>\n<li>More open-ended than supervised learning</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Finds patterns other approaches might miss</li>\n<li>Excellent for initial data exploration</li>\n<li>Flexible for undefined problems</li>\n<li>Can reveal natural groupings in data</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Hard to validate results objectively</li>\n<li>Feature weighting is often arbitrary</li>\n<li>Suffers from curse of dimensionality[^2]</li>\n<li>No guarantee of finding meaningful patterns</li>\n</ol>\n<p>For practitioners, unsupervised learning is valuable for initial data exploration and when labelled data isn&#39;t available. It&#39;s particularly useful in customer segmentation, anomaly detection, and dimension reduction.</p>\n<h3 id=\"reinforcement-learning-learning-through-interaction\">Reinforcement Learning: Learning Through Interaction</h3>\n<p><em>This mindset models an agent interacting with an environment, making decisions and learning from rewards.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Agent learns by taking actions and receiving rewards</li>\n<li>Handles delayed and sparse rewards</li>\n<li>Balances exploration and exploitation</li>\n<li>Creates its own training data through interaction</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Models dynamic real-world interactions</li>\n<li>Excellent for sequential decision-making</li>\n<li>Can discover novel strategies</li>\n<li>Learns through direct experience</li>\n<li>Combines well with deep learning</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Not all problems fit agent-environment framework</li>\n<li>Often unstable or difficult to train</li>\n<li>May perform poorly in real-world conditions</li>\n<li>Requires careful reward design</li>\n<li>Complex implementation choices</li>\n</ol>\n<p>For practitioners, reinforcement learning is valuable for problems involving sequential decisions or control, particularly in robotics, game playing, and resource management.</p>\n<h3 id=\"deep-learning-end-to-end-neural-networks\">Deep Learning: End-to-End Neural Networks</h3>\n<p><em>This mindset approaches problems through deep neural networks, letting the model learn both features and relationships.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Models tasks end-to-end through neural networks</li>\n<li>Learns hierarchical representations automatically</li>\n<li>Highly modular architecture design</li>\n<li>Benefits from transfer learning and pre-trained models</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Excels at complex data (images, text, speech)</li>\n<li>Learns useful feature representations</li>\n<li>Highly modular and customisable</li>\n<li>Strong tooling and community support</li>\n<li>Can handle multiple inputs/outputs seamlessly</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Underperforms on tabular data versus tree methods</li>\n<li>Requires large amounts of data</li>\n<li>Computationally intensive</li>\n<li>Hard to train and tune effectively</li>\n<li>Results can be difficult to interpret</li>\n</ol>\n<p>For practitioners, deep learning is essential for complex data types but may be overkill for simpler problems. Most valuable in computer vision, natural language processing, and other complex pattern recognition tasks.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p><em><strong>aka Choosing Your Modelling Path</strong></em>  </p>\n<p>For developing T-shaped expertise in modelling, the practitioner&#39;s choice should align with their primary domain while maintaining broader awareness. Here&#39;s how to approach this decision:</p>\n<ul>\n<li><p><em>Scientific Research</em> demands Statistical Modelling for its rigorous uncertainty quantification and established peer review frameworks. </p>\n</li>\n<li><p><em>Business Predictions</em> benefit most from Supervised Learning, optimising prediction accuracy while enabling automation and scalability.</p>\n</li>\n<li><p><em>Complex Data</em> (images/text) requires Deep Learning to handle unstructured data and learn hierarchical features effectively.</p>\n</li>\n<li><p><em>Interventions/Policies</em> need Causal Inference to distinguish correlation from causation and understand intervention effects.</p>\n</li>\n<li><p><em>Control Systems</em> thrive with Reinforcement Learning for sequential decisions and environment interaction.</p>\n</li>\n</ul>\n<p>For practical applications, certain combinations prove particularly effective:</p>\n<ul>\n<li><p><em>Industry/Business</em> combines Supervised Learning with Unsupervised Learning, enabling accurate predictions while discovering valuable patterns in customer data.</p>\n</li>\n<li><p><em>Research</em> pairs Statistical Modelling with Machine Learning, balancing academic rigour with modern capabilities.</p>\n</li>\n<li><p><em>Product Development</em> merges Deep Learning with Supervised Learning for end-to-end features with clear metrics.</p>\n</li>\n<li><p><em>Medical Diagnostics</em> unites Supervised Learning with Statistical Modelling, crucial for evidence-based decisions with proper uncertainty quantification.</p>\n</li>\n</ul>\n<p>The choice should be based on the practitioner&#39;s domain requirements, computational resources, interpretability needs, and available time for mastery. <em>Remember:</em> Mastery of one mindset with broad awareness surpasses superficial knowledge of many.</p>\n<hr>\n<p>[^1]: Because Bayesian models treat everything as probability distributions (rather than fixed values), any predictions or conclusions automatically include their associated uncertainty. For example, if you predict someone&#39;s future income using multiple uncertain factors, the final prediction comes as a range of possibilities with their probabilities, rather than just a single number.\n[^2]: Here is a <a href=\"https://bsky.app/profile/chrisalbon.com/post/3lbendflq2w2n\">nice digital flashcard</a> by Chris Albon, on the concept of <em>curse of dimensionality</em></p>\n"
  },
  {
    "title": "💡 TIL: Pydantic, Python's Data Validation Guard",
    "date": "2024-11-26T00:00:00.000Z",
    "tags": [
      "til",
      "data-validation",
      "python",
      "type-checking",
      "data-modeling",
      "code-quality",
      "pydantic",
      "error-handling"
    ],
    "url": "/posts/TIL-pydantic.html",
    "content": "<p><strong>TL;DR:</strong> Pydantic transforms Python&#39;s type hints from passive documentation into active runtime validators, automatically converting and validating data types while providing intelligent error handling-significantly reducing boilerplate code and catching potential errors at system boundaries for more reliable API development, configuration management, and data processing pipelines.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Today I started using <a href=\"https://docs.pydantic.dev/latest/\">Pydantic</a>, a Python library that handles data validation through Python type annotations. Pydantic brings runtime type checking and data validation that catches errors before they cause mysterious bugs in an application. It uses type hints to validate data at runtime, automatically converting and validating data types, preventing bugs, and reducing boilerplate code. It&#39;s essential for robust API development, configuration management, and data processing pipelines.</p>\n<h2 id=\"understanding-pydantic-and-its-value\">Understanding Pydantic and Its Value</h2>\n<p>Pydantic leverages Python&#39;s type hints to validate data structures. It converts your type hints from mere documentation into active runtime checks, ensuring data consistency throughout your application. Here are Pydantic&#39;s key features:  </p>\n<h3 id=\"type-enforcement\">Type Enforcement</h3>\n<pre><code class=\"language-python\">from pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n    email: str\n\n# This raises a ValidationError\nuser = User(name=&quot;John&quot;, age=&quot;not_a_number&quot;, email=&quot;john@example.com&quot;)\n</code></pre>\n<h3 id=\"automatic-type-coercion\">Automatic Type Coercion</h3>\n<pre><code class=\"language-python\">class Order(BaseModel):\n    quantity: int\n    price: float\n\n# Pydantic automatically converts valid strings to numbers\norder = Order(quantity=&quot;3&quot;, price=&quot;9.99&quot;)\nprint(order.quantity)  # 3 (int)\nprint(order.price)    # 9.99 (float)\n</code></pre>\n<h3 id=\"real-world-benefits\">Real-World Benefits</h3>\n<ul>\n<li><strong>API Development</strong>: Validates incoming JSON data automatically</li>\n<li><strong>Configuration Management</strong>: Ensures config files meet your specifications</li>\n<li><strong>Database Operations</strong>: Validates data before insertion</li>\n<li><strong>Data Parsing</strong>: Converts between JSON, dictionaries, and model instances seamlessly</li>\n</ul>\n<h3 id=\"why-it-matters\">Why It Matters</h3>\n<ol>\n<li><strong>Error Prevention</strong>: Catches data issues at system boundaries</li>\n<li><strong>Clean Code</strong>: Reduces validation boilerplate</li>\n<li><strong>Self-Documenting</strong>: Type hints serve as both validation rules and documentation</li>\n<li><strong>Performance</strong>: Compiled validation code runs efficiently</li>\n</ol>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Pydantic transforms Python&#39;s type hints from passive documentation into active data validators, significantly reducing runtime errors and improving code reliability.</p>\n"
  },
  {
    "title": "🏺 Historical Evolution of AI",
    "date": "2024-11-23T00:00:00.000Z",
    "tags": [
      "ai",
      "evolution",
      "llm",
      "symbolic",
      "neural-network",
      "data-science"
    ],
    "url": "/posts/ai-evolution.html",
    "content": "<p><strong>TL;DR:</strong> This historical overview traces AI&#39;s evolution through four major paradigms: from symbolic reasoning and expert systems (1950s-1970s), through neural networks and Bayesian approaches (1980s-1990s), to the Big Data revolution (2000s-2010s), culminating in today&#39;s integrated systems that combine multiple philosophical approaches-suggesting future progress requires unifying these diverse methodologies rather than choosing between them.</p>\n<!--more-->\n\n<h1 id=\"ais-historical-evolution\">AI&#39;s Historical Evolution</h1>\n<h2 id=\"introduction\">Introduction</h2>\n<p>The field of Artificial Intelligence has undergone several paradigm shifts since its inception, each representing a distinct approach to creating intelligent systems. Drawing from Pedro Domingos&#39; framework in <a href=\"https://en.wikipedia.org/wiki/The_Master_Algorithm\">The Master Algorithm</a> we can trace how different schools of thought have shaped our understanding and implementation of AI technologies.</p>\n<h2 id=\"historical-evolution\">Historical Evolution</h2>\n<h3 id=\"early-foundations-symbolic-ai-and-expert-systems-1950s-1970s\">Early Foundations: Symbolic AI and Expert Systems (1950s-1970s)</h3>\n<p>The pioneers of AI began with symbolic reasoning, believing intelligence could be reduced to symbol manipulation. This <em>symbolist</em> approach offered explicit reasoning chains and interpretability but struggled with real-world complexity. Expert Systems followed, successfully applying rule-based reasoning to narrow domains while revealing the challenges of scaling knowledge-based systems.</p>\n<h3 id=\"the-rise-of-neural-approaches-1980s-1990s\">The Rise of Neural Approaches (1980s-1990s)</h3>\n<p>The <em>connectionist</em> movement emerged with neural networks, drawing inspiration from biological systems. This era introduced pattern recognition capabilities and learning from examples. Simultaneously, the <em>Bayesian</em> school brought statistical methods to the forefront, offering principled approaches to handling uncertainty but requiring significant data and computational resources.</p>\n<h3 id=\"the-data-revolution-2000s-2010s\">The Data Revolution (2000s-2010s)</h3>\n<p>Big Data and Deep Learning foundations emerged as the <em>analogiser</em> school gained prominence. This period saw the convergence of massive datasets, computational power, and sophisticated architectures. Deep Learning breakthrough demonstrated the power of automatic feature learning, though at the cost of increased computational demands and reduced interpretability.</p>\n<h3 id=\"contemporary-ai-the-era-of-integration-2020s\">Contemporary AI: The Era of Integration (2020s)</h3>\n<p>Current AI systems, particularly large language models, represent a synthesis of multiple schools. They combine symbolic reasoning[^1], neural architectures[^2], and statistical learning[^3], achieving impressive generative capabilities and few-shot learning. However, they face challenges in resource requirements, reliability, and alignment with human values. A nicely distilled overview of what is today&#39;s AI, comes from an <a href=\"https://xcancel.com/karpathy/status/1864033537479135369\">Andrej Karpathy Tweet</a>.  </p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The evolution of AI reveals a field shaped by competing philosophies, each contributing essential insights. As Domingos argues, the future likely lies not in the dominance of any single approach but in their unification. While recent advances demonstrate the potential of synthesising different methods, significant challenges remain in creating truly intelligent systems that are both powerful and reliable.</p>\n<p>The path forward requires building on these foundations while addressing core challenges in efficiency, interpretability, and alignment. Rather than choosing between different schools of thought, the field must continue to integrate their strengths while mitigating their individual weaknesses.</p>\n<hr>\n<p>[^1]: Symbolic reasoning in modern AI manifests through attention mechanisms and transformers&#39; ability to process structured input like code or mathematical expressions. While not explicitly rule-based like early AI, these systems can learn and apply symbolic patterns.\n[^2]: Neural architectures in contemporary AI primarily use the transformer architecture, where self-attention layers process information in parallel, allowing the model to weigh the importance of different inputs contextually.\n[^3]: Statistical learning appears in the form of probabilistic token prediction and the use of large-scale statistical patterns learned during training. Models learn probability distributions over sequences, enabling them to generate coherent outputs.</p>\n"
  },
  {
    "title": "🔄 Considering Iterative Refinement Over Unit Testing",
    "date": "2024-11-22T00:00:00.000Z",
    "tags": [
      "fast-ai",
      "answer-ai",
      "iterative-refinement",
      "doctests",
      "best-practices",
      "llm",
      "dialogue-engineering",
      "code-quality"
    ],
    "url": "/posts/iterative-refinement.html",
    "content": "<p><strong>TL;DR:</strong> Drawing inspiration from Norvig, Howard, and Sanderson, this article advocates for iterative refinement over traditional unit testing, emphasising techniques like doctests that keep verification close to code-reducing maintenance burden whilst improving reliability by focusing on actual usage patterns rather than rigid test-driven development that can lead to outdated tests and ossified code structures.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>In the realm of software development and related fields, three influential figures -Peter Norvig (former Director of Research at Google), Jeremy Howard (founder of fast.ai), and Grant Sanderson (creator of 3Blue1Brown)- demonstrate the power of iterative refinement over rigid test-driven development. Their approaches, while applied in different domains, share common principles that challenge traditional development practices.</p>\n<h2 id=\"iterative-refinement\">Iterative Refinement</h2>\n<h3 id=\"peter-norvigs-software-development\">Peter Norvig&#39;s Software Development</h3>\n<p>Norvig&#39;s approach, demonstrated in both his <a href=\"https://norvig.com/docex.html\">original <code>docex</code> module</a> and his <a href=\"https://norvig.com/spell-correct.html\">spell corrector implementation</a>, emphasises tests that are tightly coupled with the code they verify. Before Python&#39;s doctests[^1] were officially supported, he created the <code>docex</code> module specifically to write tests in docstrings using a concise syntax like </p>\n<pre><code class=\"language-python\">def factorial(n):\n    &quot;&quot;&quot;Return the factorial of n, an exact integer &gt;= 0.\n       &gt;&gt;&gt; [factorial(n) for n in range(6)]\n       [1, 1, 2, 6, 24, 120]\n       It must also not be ridiculously large:\n       &gt;&gt;&gt; factorial(1e100)\n       Traceback (most recent call last):\n       ...\n       OverflowError: n too large\n    &quot;&quot;&quot;\n    ...\n\nif __name__ == &quot;__main__&quot;:\n    import doctest\n    doctest.testmod()\n</code></pre>\n<pre><code class=\"language-console\">$ python fact.py -v\nTrying:\n    [factorial(n) for n in range(6)]\nExpecting:\n    [1, 1, 2, 6, 24, 120]\nok\nTrying:\n    factorial(1e100)\nExpecting:\n    Traceback (most recent call last):\n        ...\n    OverflowError: n too large\nok\n2 items passed all tests:\n   1 test in __main__\n   6 tests in __main__.factorial\n7 tests in 2 items.\n7 passed.\nTest passed.\n$\n</code></pre>\n<p>Even in his spell corrector, Norvig uses simple functions with in-line test cases rather than separate test files. This approach keeps tests close to the code they verify, making them part of the living documentation rather than separate artefacts that can drift out of sync.<br><em>Update: While randomly skimming through PyTorch code, it was good to stumble across examples of <a href=\"https://github.com/pytorch/pytorch/blob/main/torch/autograd/grad_mode.py\">code containing doctests</a>.</em>  </p>\n<h3 id=\"jeremy-howards-machine-learning-development\">Jeremy Howard&#39;s Machine Learning Development</h3>\n<p>Howard&#39;s methodology, evidenced in fast.ai&#39;s development and his book &quot;Deep Learning for Coders&quot; advocates for rapid prototyping in notebooks. His emphasis lies in getting end-to-end solutions working quickly, then iteratively improving them based on actual usage patterns. In his latest course <a href=\"https://solveit.fast.ai/\">SolveIt</a>, Howard extends this iterative philosophy to [Dialogue Engineering]({{ site.baseurl }}{% link _posts/2024-11-15-dialogue-engineering.md %}), i.e. using Large Language Models in an iterative conversation to develop solutions, demonstrating how modern AI can be integrated into the development workflow while maintaining the principles of continuous refinement.</p>\n<h3 id=\"grant-sandersons-visual-mathematics\">Grant Sanderson&#39;s Visual Mathematics</h3>\n<p>This iterative philosophy extends to mathematical animations. In Grant Sanderson&#39;s <a href=\"https://www.youtube.com/watch?v=rbu7Zu5X1zI\">How I animate</a> video, he demonstrates how he builds visualisations incrementally, starting with basic shapes and gradually refining them while continuously previewing the results. This approach allows for creative exploration while maintaining momentum.</p>\n<h3 id=\"the-problem-with-traditional-testing\">The Problem with Traditional Testing</h3>\n<p>Traditional unit testing often fragments development workflow by requiring separate test maintenance and can lead to ossified code structures. When tests aren&#39;t exercised regularly, they become outdated, creating false confidence. This is particularly problematic in rapidly evolving domains like AI, where interfaces and requirements frequently change.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Instead of extensive unit test suites, it&#39;s worth considering:</p>\n<ol>\n<li>Writing working code first</li>\n<li>Using doctests for critical functions</li>\n<li>Relying on end-to-end validation</li>\n<li>Refactoring based on actual usage patterns</li>\n<li>Keeping tests focused on stable interfaces</li>\n</ol>\n<p>This approach reduces maintenance burden while ensuring code remains reliable where it matters most, that is in production.</p>\n<p>&quot;<em>Programs must be written for people to read, and only incidentally for machines to execute</em>&quot; - Abelson &amp; Sussman. The same applies to tests.</p>\n<hr>\n<p>[^1]: Python has supported <a href=\"https://docs.python.org/3/library/doctest.html\">doctests</a> natively since v2.6.9</p>\n"
  },
  {
    "title": "シ Back to Basics: A Modern, Minimal Python Toolchain",
    "date": "2024-11-21T00:00:00.000Z",
    "tags": [
      "python",
      "type-checking",
      "code-quality",
      "github-actions",
      "ci-cd",
      "cross-platform",
      "minimal",
      "toolchain"
    ],
    "url": "/posts/bring-it-back-to-basics.html",
    "content": "<p><strong>TL;DR:</strong> This article presents a streamlined Python toolchain that reduces cognitive load while maintaining the language&#39;s data science capabilities, featuring Rust-based tools like uv (package manager) and Ruff (linter/formatter), along with pyright for type checking-all configured through a single pyproject.toml file and complemented by essential libraries for data processing, visualisation, and AI development.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Python&#39;s ecosystem for Data Science and AI is unmatched in its depth and maturity. Yet, its fragmented tooling landscape often leads to decision paralysis and opinions galore: virtualenv or venv? pip or conda? black or flake8? These choices, while providing flexibility, can create unnecessary cognitive load and often foster dogmatic opinions about &quot;the right way&quot; to do things.\nAfter exploring alternative stacks, I&#39;m returning to Python. Not least because it&#39;s perfect, but because it&#39;s productive. The challenge isn&#39;t Python&#39;s capabilities; it&#39;s the abundance and complexity of its tooling. This article presents a carefully curated, minimal toolkit that leverages Python&#39;s ecosystem while avoiding its common setup pitfalls.</p>\n<h2 id=\"motivation\">Motivation</h2>\n<p>The appeal of integrated toolchains like Deno 2.0 is undeniable. Zero setup, immediate productivity, and a cohesive development experience. My recent exploration of alternative stacks revealed the value of unified tools that just work. While JavaScript&#39;s ecosystem for Data Science and AI is growing rapidly, it still lacks the depth and maturity of Python&#39;s scientific computing stack.<br>This exploration led to an important realisation: aside from an expansive Data and AI ecosystem, Python development can be achieved with a streamlined workflow that increases productivity and decreases complexity. Rather than accepting the cognitive overhead of multiple competing tools, I decided to create my own compact toolchain that meets most Data Science and AI requirements with minimalism, simplicity, and clarity in mind.\nThe goal isn&#39;t to prescribe another &quot;right way&quot; of doing things, but rather to demonstrate how a carefully chosen set of modern tools can create a development experience that rivals the integrated approaches of newer platforms while leveraging Python&#39;s mature ecosystem.  </p>\n<h2 id=\"my-approach\">My Approach</h2>\n<h3 id=\"local-development\">Local Development</h3>\n<p>My toolchain starts with the following foundational choices that eliminate common Python setup headaches:</p>\n<ol start=\"0\">\n<li><p><a href=\"https://peps.python.org/pep-0008/\">PEP8</a>: Let&#39;s start with a style guide, so that the team is on the same page  </p>\n</li>\n<li><p><a href=\"https://docs.astral.sh/uv/\">uv</a>: A blazing-fast Python package and project manager, written in Rust. It replaces pip, pip-tools, pipx, poetry, pyenv, twine, virtualenv, and more, providing: </p>\n<ul>\n<li>Consistent dependency resolution</li>\n<li>Lightning-fast package installations</li>\n<li>Built-in virtual environment management</li>\n<li>Direct integration with <code>pyproject.toml</code></li>\n</ul>\n</li>\n<li><p><a href=\"https://packaging.python.org/en/latest/guides/writing-pyproject-toml/\"><code>pyproject.toml</code></a>: The single source of truth for project configuration. For example:</p>\n</li>\n</ol>\n<pre><code class=\"language-toml\">    [project]\n    name = &quot;my-ds-project&quot;\n    version = &quot;0.1.0&quot;\n    dependencies = [\n        &quot;polars&quot;,\n        &quot;tensorflow&quot;,\n        &quot;plotly&quot;\n    ]\n\n    [tool.ruff]\n    line-length = 90 \n    select = [&quot;E&quot;, &quot;F&quot;, &quot;I&quot;]\n\n    # Required only if you use pytest for unit testing\n    [tool.pytest.ini_options]\n    testpaths = [&quot;tests&quot;]\n</code></pre>\n<ol start=\"3\">\n<li><p><a href=\"https://docs.astral.sh/ruff/\">Ruff</a>: A Rust-based tool that combines formatting and linting, replacing the need for black, flake8, isort etc.:</p>\n<ul>\n<li>Single-tool code quality enforcement</li>\n<li>Configurable through <code>pyproject.toml</code></li>\n<li>Significantly faster than Python-based alternatives</li>\n</ul>\n</li>\n<li><p><a href=\"https://github.com/microsoft/pyright\">pyright</a>: Static Type Checker for Python</p>\n<ul>\n<li>Static type checker </li>\n<li><a href=\"https://htmlpreview.github.io/?https://github.com/python/typing/blob/main/conformance/results/results.html\">Standards</a> compliant</li>\n<li><a href=\"https://microsoft.github.io/pyright/#/configuration?id=sample-pyprojecttoml-file\">Configurable</a> within <code>pyproject.toml</code></li>\n</ul>\n</li>\n<li><p>[iterative refinement]({{ site.baseurl }}{% link _posts/2024-11-22-iterative-refinement.md %}): An approach that tightly couples (doc)tests with code, ensuring <a href=\"https://www.merriam-webster.com/thesaurus/up-to-dateness\">up-to-dateness</a><br><del><a href=\"https://docs.pytest.org/en/stable/\">pytest</a>: Handles testing with minimal boilerplate and rich assertions</del></p>\n</li>\n</ol>\n<h3 id=\"cross-platform-distribution\">Cross-Platform Distribution</h3>\n<ol>\n<li>PyInstaller for creating stand-alone executables</li>\n<li>GitHub Actions workflow for automated builds:</li>\n</ol>\n<pre><code class=\"language-yaml\">- name: Build executables\n  run: |\n    pyinstaller --onefile src/main.py\n</code></pre>\n<ol start=\"3\">\n<li>Local cross-compilation using <a href=\"https://podman.io/\">Podman</a>:</li>\n</ol>\n<pre><code class=\"language-Dockerfile\">FROM python:3.13-slim\nCOPY . /app\nWORKDIR /app\nRUN pip install pyinstaller\nCMD pyinstaller --onefile src/main.py\n</code></pre>\n<h3 id=\"data-science\">Data Science</h3>\n<p>A carefully selected set of powerful libraries that minimize overlap:</p>\n<ul>\n<li><a href=\"https://pola.rs/\">Polars</a>: Fast DataFrame operations with a cohesive API. <a href=\"https://xcancel.com/charliermarsh/status/1860388882015223835\">Why?</a></li>\n</ul>\n<pre><code class=\"language-python\">    import polars as pl\n\n    def analyse_customer_behavior(path: str):\n        return (\n            pl.scan_parquet(path)\n            .with_columns([\n                pl.col(&quot;purchase_date&quot;).str.to_datetime(),\n                (pl.col(&quot;amount&quot;) * pl.col(&quot;quantity&quot;)).alias(&quot;total_spend&quot;)\n            ])\n            .group_by([\n                pl.col(&quot;customer_id&quot;),\n                pl.col(&quot;purchase_date&quot;).dt.month().alias(&quot;month&quot;)\n            ])\n            .agg([\n                pl.col(&quot;total_spend&quot;).sum().alias(&quot;monthly_spend&quot;),\n                pl.col(&quot;product_id&quot;).n_unique().alias(&quot;unique_products&quot;),\n                pl.col(&quot;purchase_date&quot;).count().alias(&quot;purchase_frequency&quot;)\n            ])\n            .sort([&quot;customer_id&quot;, &quot;month&quot;])\n            .collect()\n        )\n</code></pre>\n<ul>\n<li><a href=\"https://www.tensorflow.org/\">TensorFlow 2</a>: Deep learning when needed</li>\n</ul>\n<pre><code class=\"language-python\">    import tensorflow as tf\n    mnist = tf.keras.datasets.mnist\n\n    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n    x_train, x_test = x_train / 255.0, x_test / 255.0\n\n    model = tf.keras.models.Sequential([\n      tf.keras.layers.Flatten(input_shape=(28, 28)),\n      tf.keras.layers.Dense(128, activation=&#39;relu&#39;),\n      tf.keras.layers.Dropout(0.2),\n      tf.keras.layers.Dense(10, activation=&#39;softmax&#39;)\n    ])\n\n    model.compile(optimiser=&#39;adam&#39;,\n      loss=&#39;sparse_categorical_crossentropy&#39;,\n      metrics=[&#39;accuracy&#39;])\n\n    model.fit(x_train, y_train, epochs=5)\n    model.evaluate(x_test, y_test)\n</code></pre>\n<ul>\n<li><a href=\"https://xgboost.ai/\">XGBoost</a>: Gradient boosting for structured data</li>\n</ul>\n<pre><code class=\"language-python\">    from xgboost import XGBClassifier\n    # read data\n    from sklearn.datasets import load_iris\n    from sklearn.model_selection import train_test_split\n    data = load_iris()\n    X_train, X_test, y_train, y_test = train_test_split(data[&#39;data&#39;], data[&#39;target&#39;], test_size=.2)\n    # create model instance\n    bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective=&#39;binary:logistic&#39;)\n    # fit model\n    bst.fit(X_train, y_train)\n    # make predictions\n    preds = bst.predict(X_test)\n</code></pre>\n<ul>\n<li><a href=\"https://plotly.com/python/\">Plotly</a>: Interactive visualizations</li>\n</ul>\n<pre><code class=\"language-python\">    import plotly.express as px\n    df = px.data.iris()\n    fig = px.scatter(df, x=&quot;sepal_width&quot;, y=&quot;sepal_length&quot;, color=&quot;species&quot;, symbol=&quot;species&quot;)\n    fig.show()\n</code></pre>\n<ul>\n<li><a href=\"https://mlflow.org\">MLFlow</a>: Managing the Machine Learning Lifecycle<center>\n  <img src=\"https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/refs/heads/main/images/40_MLFlow.png\"/>  \n</center><br /></li>\n</ul>\n<h3 id=\"ai-engineering\">AI Engineering</h3>\n<p>With hybrid solutions becoming more prevalent nowadays, we can use a combination of tools.</p>\n<ul>\n<li><a href=\"https://ollama.com/\">Ollama</a>: Local model deployment and inference</li>\n</ul>\n<pre><code class=\"language-python\">    import ollama\n\n    def technical_advisor():\n        messages = [\n            {\n                &quot;role&quot;: &quot;system&quot;,\n                &quot;content&quot;: &quot;You are a technical advisor specializing in Python architecture.&quot;\n            },\n            {\n                &quot;role&quot;: &quot;user&quot;,\n                &quot;content&quot;: &quot;What&#39;s the best way to handle database migrations?&quot;\n            }\n        ]\n        \n        response = ollama.chat(model=&#39;llama2&#39;, messages=messages)\n        messages.append(response[&#39;message&#39;])\n        \n        # Follow-up question with context\n        messages.append({\n            &quot;role&quot;: &quot;user&quot;,\n            &quot;content&quot;: &quot;How would that work with SQLAlchemy specifically?&quot;\n        })\n        \n        return ollama.chat(model=&#39;llama2&#39;, messages=messages)\n</code></pre>\n<ul>\n<li><a href=\"https://docs.llamaindex.ai/\">LlamaIndex</a>: RAG pipeline construction using local LLMs or external APIs</li>\n</ul>\n<pre><code class=\"language-python\">    from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n    from llama_index.core.node_parser import SentenceSplitter\n    from llama_index.core.retrievers import VectorIndexRetriever\n    from llama_index.core.query_engine import RetrieverQueryEngine\n\n    def create_custom_rag():\n        # Load and parse documents\n        documents = SimpleDirectoryReader(&quot;technical_docs&quot;).load_data()\n        parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n        nodes = parser.get_nodes_from_documents(documents)\n        \n        # Create index with custom settings\n        index = VectorStoreIndex(nodes)\n        \n        # Custom retriever with similarity threshold\n        retriever = VectorIndexRetriever(\n            index=index,\n            similarity_top_k=3,\n            filters=lambda x: float(x.get_score()) &gt; 0.7\n        )\n        \n        # Create query engine with custom retriever\n        query_engine = RetrieverQueryEngine(retriever=retriever)\n        return query_engine\n</code></pre>\n<ul>\n<li><a href=\"https://www.mongodb.com/\">MongoDB</a>: A distributed document DB that supports vector storage and graph operations</li>\n</ul>\n<pre><code class=\"language-python\">    from pymongo import MongoClient\n    import numpy as np\n\n    def vector_search(text_embedding: np.ndarray, threshold: float = 0.8):\n        client = MongoClient(&quot;mongodb://localhost:27017/&quot;)\n        db = client.vector_db\n        \n        pipeline = [\n            {\n                &quot;$search&quot;: {\n                    &quot;index&quot;: &quot;vector_index&quot;,\n                    &quot;knnBeta&quot;: {\n                        &quot;vector&quot;: text_embedding.tolist(),\n                        &quot;path&quot;: &quot;embedding&quot;,\n                        &quot;k&quot;: 5\n                    }\n                }\n            },\n            {\n                &quot;$match&quot;: {\n                    &quot;score&quot;: {&quot;$gt&quot;: threshold}\n                }\n            },\n            {\n                &quot;$project&quot;: {\n                    &quot;_id&quot;: 0,\n                    &quot;text&quot;: 1,\n                    &quot;score&quot;: {&quot;$meta&quot;: &quot;searchScore&quot;}\n                }\n            }\n        ]\n        \n        return list(db.documents.aggregate(pipeline))\n</code></pre>\n<p><em>Update: Looking into <a href=\"https://weaviate.io/\">Weaviate</a> as an all-in-one DB solution.</em></p>\n<p>This stack provides everything needed for modern Data Science and AI work while maintaining clarity and minimising tool overlap.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Returning to Python with this minimal, modern toolchain has proven to be a pragmatic choice. The combination of uv, Ruff, and Pytest creates a more unified development workflow, while retaining access to Python&#39;s mature scientific computing ecosystem.</p>\n<p>Key benefits of this approach:</p>\n<ol>\n<li><strong>Reduced Cognitive Load</strong>: One tool per task eliminates decision fatigue</li>\n<li><strong>Modern Performance</strong>: Rust-based tools (uv, Ruff) provide near-instant feedback</li>\n<li><strong>Simplified Configuration</strong>: Single <code>pyproject.toml</code> as source of truth</li>\n<li><strong>Production Ready</strong>: Direct path from development to cross-platform deployment</li>\n<li><strong>Full Feature Set</strong>: Complete Data Science and AI capabilities without bloat</li>\n<li><strong>Flexible AI Stack</strong>: Seamless integration between local models (Ollama), RAG pipelines (LlamaIndex), and vector storage (MongoDB)</li>\n<li><strong>Production AI</strong>: Easy transition from experimentation to production AI systems with consistent tooling</li>\n</ol>\n<p>While Python&#39;s ecosystem will likely remain fragmented, we don&#39;t have to accept the complexity. By carefully choosing modern tools that prioritise speed, simplicity, and clarity, we can create a development environment that&#39;s both powerful and pleasant to use.</p>\n<p>The beauty of this approach lies not in its prescriptiveness, but in its principles: <em>minimize tooling</em>, <em>maximise capability</em>, and <em>maintain clarity</em>. Whether you adopt this exact stack or use it as inspiration for your own, the goal remains the same: bring the focus back to solving problems rather than managing tools.</p>\n"
  },
  {
    "title": "💡 TIL: TF-IDF vs BM25",
    "date": "2024-11-20T00:00:00.000Z",
    "tags": [
      "til",
      "tf-idf",
      "bm25",
      "text-ranking",
      "nlp"
    ],
    "url": "/posts/TIL-BM25-TFIDF.html",
    "content": "<p><strong>TL;DR:</strong> While TF-IDF ranks documents based on term frequency weighted by rarity across a corpus, BM25 improves upon this foundation by adding term frequency saturation and document length normalisation. Choose TF-IDF for simpler tasks with uniformly-sized documents, but prefer BM25 for search engines handling varied document lengths where its sophisticated algorithm delivers superior retrieval performance despite requiring more complex implementation and parameter tuning.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>When building search engines or document retrieval systems, two algorithms often come up: <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">TF-IDF</a> and <a href=\"https://en.wikipedia.org/wiki/Okapi_BM25\">Okapi BM25</a>. While both aim to rank documents by relevance, they differ significantly in their approach and effectiveness. Today, I learned the key differences between these techniques and when to use each one.</p>\n<h2 id=\"tf-idf-the-classic-approach\">TF-IDF: The Classic Approach</h2>\n<p>TF-IDF (Term Frequency-Inverse Document Frequency) ranks documents based on how frequently terms appear in a document, weighted by how rare those terms are across all documents. It&#39;s straightforward: if a word appears often in a document but is rare across the corpus, it&#39;s probably important[^1]. $idf$ is calculated as follows:  </p>\n<p>$$idf(t) = \\log\\frac{N}{n_t}$$  </p>\n<p>where:<br>$N$ : Total number of documents in corpus<br>$n_t$ : Number of documents containing term $t$  </p>\n<p>TF-IDF is derived by the following calculation:  </p>\n<p>$$TF\\text{-}IDF(t,d) = tf(t,d) \\cdot idf(t)$$  </p>\n<p>where:<br>$tf(t,d)$ : Frequency of term $t$ in document $d$  </p>\n<h3 id=\"advantages\">Advantages</h3>\n<ul>\n<li>Simple to understand and implement</li>\n<li>Computationally efficient</li>\n<li>Works well for documents of similar length</li>\n<li>Great for basic document classification</li>\n</ul>\n<h3 id=\"disadvantages\">Disadvantages</h3>\n<ul>\n<li>No term frequency saturation (more occurrences always mean higher scores)</li>\n<li>Doesn&#39;t handle varying document lengths well</li>\n<li>Can overemphasise common terms in long documents</li>\n</ul>\n<h2 id=\"bm25-the-modern-evolution\">BM25: The Modern Evolution</h2>\n<p>BM25 (Best Match 25) builds upon TF-IDF&#39;s foundation but adds two crucial improvements: term frequency saturation and document length normalisation. Note how the $idf_{BM25}$ component differs from TF-IDF&#39;s:  </p>\n<p>$$idf_{BM25}(t) = \\log\\frac{N - n_t + 0.5}{n_t + 0.5}$$</p>\n<p>This modification provides smoother IDF weights and better handles edge cases.</p>\n<p>$$BM25(t,d) = \\frac{tf(t,d) \\cdot (k_1 + 1)}{tf(t,d) + k_1 \\cdot (1 - b + b \\cdot \\frac{|d|}{avgdl})} \\cdot idf_{BM25}$$</p>\n<p>where:<br>$tf(t,d)$ : Frequency of term $t$ in document $d$<br>$|d|$ : Length of document $d$ (in words)<br>$avgdl$ : Average document length in corpus<br>$k_1$ : Term frequency saturation parameter (typically 1.2-2.0)<br>$b$ : Length normalisation parameter (typically 0.75)<br>$N$ : Total number of documents in corpus<br>$n_t$ : Number of documents containing term $t$  </p>\n<h3 id=\"advantages-1\">Advantages</h3>\n<ul>\n<li>Better handles varying document lengths</li>\n<li>Prevents term frequency from dominating scores</li>\n<li>More nuanced relevance rankings</li>\n<li>Industry standard for search engines</li>\n</ul>\n<h3 id=\"disadvantages-1\">Disadvantages</h3>\n<ul>\n<li>More complex implementation</li>\n<li>Requires parameter tuning</li>\n<li>Slightly higher computational cost</li>\n<li>Less interpretable than TF-IDF</li>\n</ul>\n<h2 id=\"which-to-choose\">Which to Choose?</h2>\n<h3 id=\"choose-tf-idf-when\">Choose TF-IDF when:</h3>\n<ul>\n<li>Building basic document classification systems</li>\n<li>Working with uniformly-sized documents</li>\n<li>Needing interpretable results</li>\n<li>Prioritising implementation simplicity</li>\n</ul>\n<h3 id=\"choose-bm25-when\">Choose BM25 when:</h3>\n<ul>\n<li>Building a search engine</li>\n<li>Handling documents of varying lengths</li>\n<li>Requiring state-of-the-art retrieval performance</li>\n<li>Working with user queries</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>While TF-IDF remains valuable for simpler tasks and educational purposes, BM25 is generally superior for serious search applications. The choice between them often comes down to the trade-off between simplicity and sophistication. For modern search engines, BM25 is the clear winner, but TF-IDF&#39;s simplicity makes it perfect for learning and basic applications.</p>\n<p>Remember: the best algorithm is the one that meets your specific needs. Don&#39;t automatically reach for BM25 just because it&#39;s more advanced – sometimes, simpler is better.</p>\n<p>[^1]: This is why TF-IDF is effective at identifying characteristic terms in documents. It automatically downweights common words like &quot;the&quot;, &quot;and&quot;, &quot;is&quot; while highlighting distinctive terms that appear frequently in specific documents.</p>\n"
  },
  {
    "title": "🆙 Level Up With Dialogue Engineering",
    "date": "2024-11-15T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "dialogue-engineering",
      "prompt",
      "iterative-refinement",
      "rag"
    ],
    "url": "/posts/dialogue-engineering.html",
    "content": "<p><strong>TL;DR:</strong> Dialogue Engineering transforms AI interactions by replacing one-shot prompts with structured, multi-turn conversations that break complex tasks into manageable steps: setting scenarios, gathering information, creating structured outlines, generating content iteratively, and refining conclusions. This systematic approach dramatically improves productivity across research, business, and content creation while maintaining human oversight to address AI limitations like accuracy and contextual understanding.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Dialogue Engineering is transforming how we interact with AI[^1]. Rather than relying on one-shot prompts, it&#39;s an iterative approach where we engage in structured, multi-turn conversations with LLMs (Large Language Models) to achieve complex goals. While I first encountered the term through Jeremy Howard[^2] [^3], the concept has deeper roots in human-AI interaction research. Though Howard popularised it recently through fast.ai and answer.ai, the concept has been discussed since 1986[^4].\nDialogue engineering dramatically improves productivity by breaking down complex tasks, maintaining context across interactions, and guiding AI through iterative refinement. This systematic approach helps produce better results while reducing the cognitive load of prompt crafting. A nice overview of Dialogue Engineering comes from the Medium article <a href=\"https://medium.com/@fabioc/dialog-engineering-ai-as-your-research-assistant-616a625e9853\">Dialog Engineering: AI as Your Research Assistant</a>.<br>Below, I&#39;ll summarise what I inferred from that article.</p>\n<h2 id=\"how-dialogue-engineering-works\">How Dialogue Engineering Works</h2>\n<ol>\n<li><strong>Setting the Scenario</strong><br>This first step involves defining clear objectives and research questions before engaging with AI. Rather than diving into broad topics, we frame specific goals and provide relevant context. For example, when starting a research project, we outline exactly what we need to investigate and any important background information the AI should consider.<br><em>Best Practice:</em> Be clear and specific about goals, provide relevant background information to help AI understand context.  </li>\n<li><strong>Gathering Information</strong><br>Once the scenario is set, we guide the AI in collecting and organising relevant data. This could involve creating annotated bibliographies, summarising key sources, or compiling research findings. The AI helps structure this information in a way that&#39;s useful for the next steps.<br><em>Best Practice:</em> Request structured formats like annotated bibliographies, ask for citations and evidence to ensure accuracy.  </li>\n<li><strong>Structuring the Outline</strong><br>Before diving into content creation, we work with the AI to develop a clear roadmap. This outline breaks down the task into logical sections, ensuring a coherent flow and manageable chunks of work.<br><em>Best Practice:</em> Break the task into clear sections, ensure logical connections between parts that reflect overall goals.  </li>\n<li><strong>Generating Content Iteratively</strong><br>With the outline in place, we tackle each section individually through iterative refinement. Rather than expecting perfect content immediately, we provide feedback and guide the AI to improve its outputs progressively.<br><em>Best Practice:</em> Work on sections individually to maintain focus, use feedback loops to guide AI toward more specific, accurate outputs.  </li>\n<li><strong>Conclusion and Introduction Refinement</strong><br>The final step involves revisiting the opening and closing sections once the main content is complete. This ensures these crucial parts accurately reflect and synthesise the entire piece.<br><em>Best Practice:</em> Write introduction last to accurately reflect content, craft conclusion by synthesising main takeaways from each section.</li>\n</ol>\n<p>Throughout all steps, I maintain active oversight, checking for accuracy and providing clear feedback. This systematic approach has dramatically improved my productivity while ensuring high-quality outputs.</p>\n<h2 id=\"practical-applications\">Practical Applications</h2>\n<p>Here are the key areas where dialogue engineering proves particularly valuable:</p>\n<ul>\n<li><strong>Academic Research</strong><br>Researchers can leverage dialogue engineering to synthesise vast amounts of information, structure complex arguments, and ensure accurate citations. The iterative approach is particularly useful for literature reviews and thesis development.<br><em>Example:</em> A researcher prompts AI to generate an annotated bibliography on AI-driven diagnostics, focusing on recent studies, then iteratively refines the summaries and findings.  </li>\n<li><strong>Business Strategy and Reporting</strong><br>For corporate applications, dialogue engineering helps generate market reports, analyse trends, and produce comprehensive strategy documents. This systematic approach ensures consistency while maintaining analytical depth.<br><em>Example:</em> Business analysts use iterative prompts to draft sections of market reports, starting with &quot;<em>Generate a section on e-commerce trends focusing on AI-driven personalisation</em>&quot; then refining based on specific data points.  </li>\n<li><strong>Report Automation</strong><br>Dialogue Engineering excels at automating recurring business reports, such as quarterly financial reviews or performance summaries. The structured approach ensures consistency while allowing for customisation.<br><em>Example:</em> Teams automate quarterly reports by structuring templates with AI, feeding relevant data, and using iterative refinement to maintain accuracy and freshness.  </li>\n<li><strong>Content Creation and Media</strong><br>Content creators can streamline the production of articles, blogs, and multimedia scripts through structured dialogue with AI. This approach particularly shines in drafting and revising content iteratively.<br><em>Example:</em> Writers use dialogue engineering to draft introductory paragraphs, then iterate with prompts for more engaging language or additional examples.  </li>\n<li><strong>Technical Writing and Documentation</strong><br>In fields requiring precise technical documentation, dialogue engineering helps ensure clarity, accuracy, and consistency across complex documents.<br><em>Example:</em> Software engineers use dialogue engineering to draft technical documentation for new features, prompting &quot;<em>Draft a technical overview of the new user authentication feature</em>&quot; then refining for clarity and technical accuracy.</li>\n</ul>\n<p>Each of these applications benefits from dialogue engineering&#39;s structured, iterative approach, leading to more efficient workflows and higher-quality outputs.</p>\n<h2 id=\"best-practices\">Best Practices</h2>\n<p>Key best practices include:  </p>\n<ul>\n<li><strong>Precision in Prompts</strong><br>Craft prompts that are neither too vague nor overly specific. Focus on clear, well-structured queries that guide AI towards relevant outputs.\n<em>Example:</em> Instead of &quot;<em>Tell me about AI in healthcare</em>&quot; use &quot;<em>What are the latest advancements in AI-driven diagnostics in healthcare, particularly in image recognition?</em>&quot;</li>\n<li><strong>Iterative Refinement</strong><br>Build on each interaction, using feedback to improve outputs gradually rather than expecting perfection immediately.\n<em>Example:</em> Start with a draft section, then refine with follow-up prompts like &quot;<em>Expand on the use of dialogue engineering in business reporting, specifically market trend analysis.</em>&quot;</li>\n<li><strong>Leverage Feedback Loops</strong><br>Maintain continuous cycles of prompting, feedback, and refinement to improve output quality over time.\n<em>Example:</em> When creating an outline, start broad, then use feedback to add specific sections on practical examples in different domains.</li>\n<li><strong>Source and Citation Checking</strong><br>Verify AI-generated sources and citations manually, as AI models lack real-time access to databases.\n<em>Example:</em> Cross-reference any cited statistics or research papers with trusted external sources before including them in final outputs.</li>\n<li><strong>Structure Before Diving In</strong><br>Create clear outlines or plans before generating detailed content to ensure logical flow and completeness.\n<em>Example:</em> Start with a structured outline for a Medium article, then develop each section iteratively.</li>\n<li><strong>Mind Token Limits</strong><br>Break down long content into manageable chunks to work within AI model token limits.<br><em>Example:</em> Generate long-form content section by section, refining each piece before moving to the next.</li>\n</ul>\n<p>However, we should be aware of the limitations (and challenges) of Dialogue Engineering too. </p>\n<h2 id=\"understanding-the-limitations\">Understanding the Limitations</h2>\n<p>While these best practices enhance the use of dialogue engineering, it&#39;s essential to acknowledge its constraints and challenges. Like any powerful tool, dialogue engineering comes with limitations that require careful consideration and management. Here&#39;s what we need to keep in mind:</p>\n<h3 id=\"key-limitations-and-challenges\">Key Limitations and Challenges</h3>\n<p>The foremost concern when using generative AI is <em>accuracy</em> and <em>hallucinations</em>. LLMs can sometimes generate plausible-sounding but false information, necessitating rigorous fact-checking processes. This is particularly critical in professional contexts where accuracy is paramount.\n<em>Ethical implications</em> also demand attention. While AI can streamline work processes, maintaining authenticity and proper attribution is crucial. This connects directly to the need for <em>consistent human oversight</em>, that is users must <em>actively review outputs</em>, <em>ensure quality control</em>, and <em>make ethical judgements</em> about the content&#39;s appropriateness and accuracy.\nAI&#39;s current <em>limitations in understanding context and nuance</em> present another challenge. Models may struggle with subtle distinctions or produce oversimplified explanations, especially in specialised fields. Technical constraints, particularly token limits and handling complex, multi-layered reasoning tasks, further necessitate careful planning and task breakdown.\nThese limitations underscore a crucial point: dialogue engineering works best as a <em>collaborative tool</em> that <em>enhances</em>, rather than replaces, human expertise and judgement.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Dialogue Engineering represents a significant evolution in human-AI interaction, moving beyond simple prompt engineering to create a dynamic, iterative approach. Through structured conversations and systematic refinement, it enables us to tackle complex tasks more efficiently across academic, business, and creative domains. While the technique requires careful attention to limitations like AI hallucinations and demands consistent human oversight, its power lies in treating AI as a collaborative partner rather than a one-shot tool. By following best practices and understanding its constraints, dialogue engineering becomes a force multiplier for productivity, helping us create better outputs while maintaining human expertise at the core of the process. This balance of systematic interaction and human judgement makes dialogue engineering a valuable framework for anyone looking to maximise the potential of AI tools in their workflow.</p>\n<hr>\n<p>[^1]: AI is an umbrella term that has meant different things over the years. Since 2022, it has become a synonym of Generative AI. Here&#39;s a short AI timeline: <a href=\"https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence\">Symbolic AI</a> (1950-60s), <a href=\"https://en.wikipedia.org/wiki/Expert_system\">Expert Systems</a> (1970s), <a href=\"https://en.wikipedia.org/wiki/Neural_network\">Neural Networks</a> and <a href=\"https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning\">Knowledge Representation</a> (1980s), <a href=\"https://en.wikipedia.org/wiki/Machine_learning\">Machine Learning</a> and <a href=\"https://en.wikipedia.org/wiki/Statistics\">Statistical Methods</a> (1990s), <a href=\"https://en.wikipedia.org/wiki/Big_data\">Big Data</a> and Deep Learning foundations (2000s), <a href=\"https://en.wikipedia.org/wiki/Deep_learning\">Deep Learning</a> (2010s), <a href=\"https://en.wikipedia.org/wiki/Generative_artificial_intelligence\">Generative AI</a> and <a href=\"https://en.wikipedia.org/wiki/Large_language_model\">Large Language Models</a> (2020s)\n[^2]: <a href=\"https://www.youtube.com/watch?v=qO-YqJm0Q1U&t=16\">Answer.ai &amp; AI Magic with Jeremy Howard</a>\n[^3]: <a href=\"https://www.answer.ai/posts/2024-11-07-solveit.html\">How To Solve It With Code</a> \n[^4]: <em>Foundations of dialog engineering: the development of human-computer interaction. Part II</em> <a href=\"https://www.sciencedirect.com/science/article/pii/S0020737386800438\">(Gaines et al., 1986)</a></p>\n"
  },
  {
    "title": "🖥 The On-Prem Comeback (aka Cloud Repatriation)",
    "date": "2024-11-14T00:00:00.000Z",
    "tags": [
      "cloud",
      "on-prem"
    ],
    "url": "/posts/cloud-repatriation.html",
    "content": "<p><strong>TL;DR:</strong> Cloud repatriation-the strategic migration of applications and data from public clouds back to on-premises infrastructure-is gaining traction as organisations seek better cost control, performance, data privacy, and reduced vendor lock-in, with companies like 37signals projecting significant savings whilst maintaining a balanced hybrid approach rather than abandoning cloud entirely.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>More recently, a notable shift is emerging in how organisations approach their cloud infrastructure. Some companies are beginning to move their applications and data away from public cloud providers like AWS, GCP and Azure. This marks a shift in the computing landscape.</p>\n<h2 id=\"what-does-cloud-repatriation-mean\">What Does Cloud Repatriation Mean?</h2>\n<p>Cloud repatriation refers to the process of moving applications, services and data from public cloud environments back to on-premises data centres, private clouds or hybrid set-ups. This reverse migration represents a pivot from the &quot;cloud-first&quot; mindset that has dominated in the last few years.</p>\n<h2 id=\"why-its-happening\">Why It&#39;s Happening</h2>\n<p>Several key factors are driving this trend. For larger companies, cost is a real consideration, with scale-ups such as 37signals projecting savings of £2 million p.a. by leaving AWS. Performance issues and rising cloud costs have led major organisations like GEICO to reconsider their cloud strategy after experiencing 2.5x increases in their bills alongside reliability challenges.\n<em>Data privacy</em>, <em>compliance</em> requirements and the desire to avoid <em>vendor lock-in</em> are also significant motivators aside from growing costs. Many organisations are finding that running certain workloads on-premises or in hybrid environments offers better control over their infrastructure and data. See <a href=\"https://www.youtube.com/watch?v=kyJJeik9loU\">optimising infrastructure for AI</a> for a nice overview on cloud&lt;-&gt;on-prem.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Cloud repatriation isn&#39;t about completely abandoning public clouds but rather about finding the right balance. For organisations with predictable workloads and sufficient technical expertise, a strategic combination of on-premises, private cloud and public cloud infrastructure might prove more effective than a public-cloud-only approach.</p>\n"
  },
  {
    "title": "🐢 Slow Down And Grow Smart, Not Fast",
    "date": "2024-11-14T00:00:00.000Z",
    "tags": [
      "slow-down",
      "advantage",
      "best-practices",
      "company-culture",
      "productivity",
      "decision-making",
      "business-value",
      "real-value"
    ],
    "url": "/posts/slow-down.html",
    "content": "<p><strong>TL;DR:</strong> Sustainable business growth prioritises measured expansion over explosive scaling, focusing on leadership development, strategic capital deployment, organisational health through clear alignment, and strong communication-creating resilient companies where profitability, employee retention, and team autonomy become key metrics of success rather than breakneck speed.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>In an industry obsessed with &quot;move fast and break things,&quot; some companies prove that measured growth creates lasting success. I was happy to listen to the <a href=\"https://saasscalingsecrets.buzzsprout.com/2172375/episodes/15926541-why-slower-growth-could-be-your-fast-track-to-success-with-roan-lavery-ceo-of-freeagent\">CEO of a successful company</a> recounting their success, which was largely thanks to slow and steady growth. </p>\n<h2 id=\"sustainable-development-over-explosive-growth\">Sustainable Development Over Explosive Growth</h2>\n<p>It&#39;s safe to say that consistency and growing at a controlled pace is a recipe for a successful sustainable [insert word here]. This strategy applies to most things in life, in my view. Here is what caught my attention from this discussion:  </p>\n<ul>\n<li>The company grew at a pace that allowed leaders to develop alongside the business </li>\n<li>They focused on reaching profitability within 18-24 months after each funding round </li>\n<li>They raised capital from a position of strength, not necessity </li>\n<li>Capital was then deployed strategically rather than burning through runway</li>\n</ul>\n<h2 id=\"building-strong-foundations\">Building Strong Foundations</h2>\n<p>Organisational health was centred around <em>clear alignment</em> through frameworks like <a href=\"https://www.tablegroup.com/product/the-advantage/\">The Advantage</a>[^1]. Company values have been <em>embedded into daily processes</em>. Goals are <em>integrated into regular team workflows</em>. Finally, maintaining <em>strong communication across all levels</em> has been key to the company&#39;s success. </p>\n<h2 id=\"culture-and-retention\">Culture And Retention</h2>\n<p>Success metrics go beyond financial growth. Key employees have stayed with the company <em>long-term</em>. Teams have maintained <em>autonomy</em> while staying <em>accountable through balanced scorecards</em>. Leadership has focused on creating <em>clarity</em> and <em>empowering teams</em>. Regular reinforcement of values has been achieved through <em><a href=\"https://youtube.com/watch?v=Og7NzaVpceE\">good onboarding</a></em>[^2], and daily operations.</p>\n<h2 id=\"key-takeaways\">Key Takeaways</h2>\n<ul>\n<li><strong>Match growth to capability</strong>: Ensure your organisation can sustainably support its growth rate</li>\n<li><strong>Focus on fundamentals</strong>: Build strong processes and systems that scale gradually</li>\n<li><strong>Invest in people</strong>: Give teams time and resources to develop alongside the company</li>\n<li><strong>Deploy capital wisely</strong>: Prioritise sustainable growth over rapid cash burn</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p><strong>This is the TL;DR really:</strong> Building a successful tech company doesn&#39;t require breakneck speed or unsustainable growth. Smart, measured expansion with a focus on people and processes creates resilient businesses that stand the test of time.</p>\n<hr>\n<p>[^1]: &quot;<em>[the author] makes an overwhelming case that organisational health will surpass all other disciplines in business as the greatest opportunity for improvement and competitive advantage.</em>&quot;\n[^2]: Characteristics of a good onboarding program: a) One thing at a time, b) Lots of practice (with feedback), c) Attain proficiency, then move forward, d) The learner follows rather than guides, e) &quot;Minimum Productive Competency&quot; </p>\n"
  },
  {
    "title": "💪 The Advantage",
    "date": "2024-11-14T00:00:00.000Z",
    "tags": [
      "slow-down",
      "advantage"
    ],
    "url": "/posts/the-advantage.html",
    "content": "<p><strong>TL;DR:</strong> Patrick Lencioni&#39;s &quot;The Advantage&quot; argues that organisational health-built through leadership teams founded on trust, clear communication, and aligned values-is the single greatest competitive advantage companies can achieve, outweighing traditional &quot;smart&quot; business strategies and metrics for sustainable long-term success.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Recently I listened to a <a href=\"https://saasscalingsecrets.buzzsprout.com/2172375/episodes/15926541-why-slower-growth-could-be-your-fast-track-to-success-with-roan-lavery-ceo-of-freeagent\">successful company&#39;s CEO interview</a>, where he explained how growing sustainably contributed to the company&#39;s success. The CEO said that their growth strategy was inspired by <a href=\"https://www.tablegroup.com/pat/\">Patrick Lencioni</a>&#39;s book <a href=\"https://www.tablegroup.com/product/the-advantage/\">The Advantage</a>. I found the book&#39;s premise very interesting, hence I&#39;ll attempt to summarise important points as a note to self.   </p>\n<h2 id=\"central-thesis\">Central Thesis</h2>\n<p>Organisational health is the <em>single greatest competitive advantage</em> a company can achieve, yet it&#39;s often overlooked in favour of &quot;smart&quot; business decisions.</p>\n<h2 id=\"key-components-of-organisational-health\">Key components of organisational health</h2>\n<h3 id=\"leadership-team-structure\">Leadership team structure</h3>\n<ul>\n<li>Optimal size: 3-10 people</li>\n<li>Built on trust and vulnerability</li>\n<li>Values collective success over individual achievement</li>\n</ul>\n<h3 id=\"core-principles\">Core principles</h3>\n<ul>\n<li>Trust and vulnerability as foundations</li>\n<li>Accountability at all levels</li>\n<li>Commitment to collective goals</li>\n<li>Clear communication and expectations</li>\n</ul>\n<h3 id=\"operational-excellence\">Operational excellence</h3>\n<ul>\n<li>Regular, focused meetings with specific purposes</li>\n<li>Clear distinction between strategic and tactical discussions</li>\n<li>Emphasis on debate and healthy conflict resolution</li>\n<li>Continuous progress monitoring</li>\n</ul>\n<h3 id=\"people-and-culture\">People and culture</h3>\n<ul>\n<li>Hire for cultural fit rather than training after hiring</li>\n<li>Focus on values alignment in recruitment</li>\n<li>Reward behaviour that aligns with organisational values</li>\n<li>Foster an environment of open communication</li>\n</ul>\n<h3 id=\"critical-success-factors\">Critical success factors</h3>\n<ul>\n<li>Minimal internal politics</li>\n<li>High clarity in communication</li>\n<li>Clear decision-making processes</li>\n<li>Low employee turnover</li>\n<li>High morale and productivity</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The book&#39;s fundamental message is that creating a healthy organisation through strong leadership, clear communication, and aligned values is more important than traditional business metrics for long-term success. It&#39;s not about being the &quot;smartest&quot; in the market, but about creating the healthiest internal environment.</p>\n"
  },
  {
    "title": "✅ On-boarding that works",
    "date": "2024-11-14T00:00:00.000Z",
    "tags": [
      "onboarding",
      "best-practices",
      "productivity",
      "company-culture",
      "decision-making",
      "talent",
      "learning",
      "efficiency"
    ],
    "url": "/posts/good-onboarding.html",
    "content": "<p><strong>TL;DR:</strong> Effective onboarding programmes dramatically reduce the productivity gap for new hires by applying evidence-based learning principles: breaking skills into manageable components, providing structured learning paths, targeting 80% proficiency before moving on, defining minimum productive competency, using checklists with frequent feedback, and focusing on mechanical competency - all based on how human memory and learning actually work, transforming the costly standard approach where developers take months to become fully productive.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>I recently watched a lively presentation titled <a href=\"https://www.youtube.com/watch?v=Og7NzaVpceE\">opinionated onboarding</a>, that discussed the dramatic impact of onboarding practices on new staff and business success. The speaker, drawing from his experience, articulated that poor onboarding is actively harming companies while good onboarding can transform team productivity and retention.\nHaving experienced both ends of the spectrum myself, I strongly agree with the presenter&#39;s central thesis. Companies can&#39;t afford to waste months getting new staff up to speed, yet that&#39;s what&#39;s happening very frequently. The costs are staggering, both in lost productivity and squandered talent.</p>\n<h2 id=\"the-problem\">The Problem</h2>\n<p>New employees face an overwhelming cognitive burden as they simultaneously navigate multiple learning curves: mastering the tech stack, deciphering an unfamiliar codebase, adapting to team workflows, understanding the business domain, and learning organisational structures. Many companies worsen this situation through ineffective approaches, either expecting staff to self-direct their learning with vague instructions like &quot;go learn X and tell us when you&#39;re done&quot;, or by immediately assigning them tickets without proper context or support. This inefficiency comes at a significant cost though. According to the presenter, the average software developer staying at a company for only 20 months, taking 6 months to become productive means losing nearly a third of their effective tenure[^1]. Rather than fixing their onboarding processes, many companies respond by exclusively hiring senior professionals who can &quot;hit the ground running&quot; - an approach that not only limits their talent pool but proves unrealistic even for experienced hires.</p>\n<h2 id=\"effective-on-boarding-strategies\">Effective On-boarding Strategies</h2>\n<p>According to the presenter, research and experience show that effective onboarding follows clear cognitive science principles. Rather than overwhelming new hires with broad, unfocused (occasionally learning) objectives, successful onboarding programs recognise how human learning actually works and structure their approach accordingly. Two key principles emerge as foundational to any effective onboarding strategy:</p>\n<ol>\n<li>Focus on building specific capabilities rather than general &quot;understanding&quot;</li>\n<li>Account for cognitive limitations:<ul>\n<li>People can only hold ~4 concepts in working memory</li>\n<li>New concepts take more mental space than familiar ones</li>\n<li>Skills must be practiced close to when they&#39;re learned</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"best-practices\">Best Practices</h3>\n<p>These principles translate into concrete best practices that any organisation can implement. While the specific skills and technologies may vary between teams, successful onboarding programs share common structural elements that maximize learning efficiency while minimizing cognitive overload:</p>\n<ul>\n<li><strong>Break down complex skills into smaller, manageable components</strong>. For example, rather than asking someone to &quot;learn LiveView&quot;[^2] break it down into specific tasks like creating forms, handling events, or managing state </li>\n<li><strong>Provide structured learning paths rather than self-directed exploration</strong>. When new hires must decide what to learn next, they waste valuable mental capacity on planning rather than learning. A clear, predefined path eliminates this overhead </li>\n<li><strong>Aim for 80% proficiency before moving to next skill</strong>. This threshold ensures sufficient mastery while avoiding diminishing returns from pursuing perfection </li>\n<li><strong>Define minimum productive competency for the role</strong>. Not every skill needs to be mastered immediately - identify what&#39;s truly needed for day-one productivity and focus there first </li>\n<li><strong>Use checklists and frequent practice with feedback</strong>. Clear checkpoints provide motivation and progress tracking, while regular feedback prevents learners from developing incorrect habits </li>\n<li><strong>Focus on mechanical competency to reduce cognitive load</strong>. When basic operations become automatic, developers can focus their mental energy on solving more complex problems</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Effective onboarding is not just a nice-to-have, it&#39;s a competitive necessity in today&#39;s software industry. While poor onboarding practices continue to cost companies valuable time and talent, the path to improvement is clear. By <em>breaking down</em> complex skills, providing <em>structured learning</em> paths, and <em>respecting</em> cognitive limitations, organisations can dramatically reduce the time it takes for new hires to become productive team members. This investment in structured onboarding not only accelerates developer productivity but also expands hiring possibilities, allowing companies to tap into a broader talent pool. The choice is simple: continue losing months of productivity to ineffective onboarding, or implement these evidence-based practices to build stronger, more capable engineering teams.</p>\n<hr>\n<p>[^1]: Since the presentation focused on software developers, I use it here as a proxy for various technical positions including AI Engineering, Data Science and others. Also, the tenure statistic may be skewed towards the U.S. market, however it&#39;s true that many employees job hop in pursuit of a higher salary or a better job altogether<br>[^2]: <a href=\"https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html\">Phoenix LiveView</a> &quot;<em>is a process that received events, updates its state and renders updates to a page as diffs</em>&quot;</p>\n"
  },
  {
    "title": "💡 TIL: vLLM Is A High-Performance Engine For LLM Serving",
    "date": "2024-11-13T00:00:00.000Z",
    "tags": [
      "til",
      "llm",
      "ai",
      "python",
      "on-prem",
      "performance"
    ],
    "url": "/posts/TIL-vLLM.html",
    "content": "<p><strong>TL;DR:</strong> vLLM revolutionises LLM deployment through its PagedAttention algorithm, which applies virtual memory principles to key-value caches, enabling more efficient memory management and significantly improving throughput for resource-constrained environments whilst supporting popular open-source models.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>As a Data Scientist / AI Engineer exploring local-first solutions[^1] [^2], deploying Large Language Models (LLMs) presents significant resource management challenges. vLLM emerges as a breakthrough solution that fundamentally reimagines how we deploy and utilise these resource-intensive models[^4].</p>\n<h2 id=\"what-is-vllm\">What Is vLLM?</h2>\n<p>vLLM is an open-source serving engine that optimises LLM deployment through virtualisation techniques[^4]. At its core, vLLM introduces PagedAttention, a novel attention algorithm that improves memory utilisation through paged memory management[^3]. Similar to how operating systems manage virtual memory, PagedAttention segments the key-value memory into non-continuous pages, enabling more efficient memory usage and request handling.</p>\n<p>Key features[^4]:</p>\n<ul>\n<li>Efficient memory management through PagedAttention</li>\n<li>Continuous batching for request handling</li>\n<li>Support for popular open-source models (Llama, Mistral, Falcon)</li>\n</ul>\n<h2 id=\"implementation\">Implementation</h2>\n<p>Here&#39;s a simple example of using vLLM:</p>\n<pre><code class=\"language-python\">from vllm import LLM, SamplingParams\n\n# Initialise the model\nllm = LLM(model=&quot;meta-llama/Llama-3.1-8B&quot;)\n\n# Define sampling parameters\nsampling_params = SamplingParams(\n    temperature=0.7,\n    max_tokens=128\n)\n\n# Generate text\noutputs = llm.generate([&quot;Your prompt goes here&quot;], sampling_params)\n</code></pre>\n<h2 id=\"applications\">Applications</h2>\n<p>According to industry analysis[^4], vLLM&#39;s applications span multiple domains:</p>\n<ol>\n<li><p>Natural Language Processing</p>\n<ul>\n<li>Enhances chatbots and sentiment analysis</li>\n<li>Improves language translation services</li>\n</ul>\n</li>\n<li><p>Healthcare</p>\n<ul>\n<li>Enables secure patient data analysis</li>\n<li>Assists in medical diagnostics</li>\n</ul>\n</li>\n<li><p>Financial Services</p>\n<ul>\n<li>Powers fraud detection systems</li>\n<li>Enhances automated customer service</li>\n</ul>\n</li>\n<li><p>Education</p>\n<ul>\n<li>Facilitates intelligent tutoring systems</li>\n<li>Enables automated assessment tools</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"best-practices-for-implementation4\">Best Practices for Implementation[^4]</h2>\n<p>For optimal vLLM deployment:</p>\n<ul>\n<li>Implement model optimisation techniques</li>\n<li>Utilise containerisation for scalable deployment</li>\n<li>Maintain robust monitoring systems</li>\n<li>Regular performance optimisation</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>vLLM represents a significant advancement in LLM serving technology[^3], offering an efficient, scalable solution for resource-constrained environments. Its innovative approach to memory management through PagedAttention and broad applicability across industries makes it an essential tool for modern AI development.</p>\n<hr>\n<p>[^1]: <a href=\"https://www.puppet.com/blog/cloud-repatriation\">Cloud Repatriation: Examples, Unpacking 2024 Trends &amp; Tips for Reverse Migration</a>\n[^2]: <a href=\"https://thenewstack.io/why-companies-are-ditching-the-cloud-the-rise-of-cloud-repatriation/\">Why Companies Are Ditching the Cloud: The Rise of Cloud Repatriation</a>\n[^3]: Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.H., Gonzalez, J.E., Zhang, H., and Stoica, I. (2023). &quot;Efficient Memory Management for Large Language Model Serving with PagedAttention.&quot; <a href=\"https://arxiv.org/abs/2309.06180\">arXiv:2309.06180</a>.\n[^4]: <a href=\"https://aijobs.net/insights/vllm-explained/\">vLLM Explained</a> </p>\n"
  },
  {
    "title": "🔀 Cross-Platform Builds In Python",
    "date": "2024-11-11T00:00:00.000Z",
    "tags": [
      "python",
      "github-actions",
      "ci-cd",
      "cross-platform",
      "deno",
      "typescript"
    ],
    "url": "/posts/py-cross-compile.html",
    "content": "<p><strong>TL;DR:</strong> Creating cross-platform Python application packages requires CI/CD solutions like GitHub Actions since tools like PyInstaller can&#39;t natively build for multiple platforms; alternatives like Julia and Elixir offer promising but still-maturing packaging options, while Deno emerges as an appealing alternative with its straightforward cross-platform packaging capabilities, lightweight footprint, and growing data ecosystem - though Python remains dominant for data analysis despite its packaging limitations.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>In the last couple of years I&#39;ve spoken to 3-4 people who had needed a bespoke data analysis tool that could be used locally, with privacy in mind. In some cases they&#39;d need to work in a sandboxed environment for security reasons, other times they had IP protection concerns. A desktop app or a CLI tool[^1] seemed to fit the bill in all those cases.<br>In the last decade+, Data and Python have become a synonym. <a href=\"https://pyinstaller.org/\">PyInstaller</a> seemed like an obvious choice. However, PyInstaller cannot cross-compile code as it stands. Being a <em>Linux</em> user, offering to help <em>Windows</em> users, meant I should find a workaround e.g. leveraging GitHub Actions for cross-compilation.  </p>\n<h2 id=\"taking-the-scenic-route\">Taking The Scenic Route</h2>\n<p>Out of curiosity, I decided to have a poke around a couple of different languages and ecosystems that could teach me a few things while helping me understand what is a viable alternative to Python.  </p>\n<h3 id=\"julia\">Julia</h3>\n<p>The <a href=\"https://julialang.org/\">Julia</a> programming language has caught my eye since 2014. It partially reminded me of MATLAB, that I used for my PhD. Familiarity aside, it&#39;s a great language to develop with. It&#39;s fast, interactive, with the best REPL I&#39;ve encountered, highly promising overall especially so after the release of Julia v1.9.<br>To my understanding Julia, being a JIT compiled language, wasn&#39;t designed for static compilation per se. Community efforts have enabled the generation of compiled packages, with <a href=\"https://binarybuilder.org/\">BinaryBuilder</a>, <a href=\"https://julialang.github.io/PackageCompiler.jl\">PackageCompiler</a> and <a href=\"https://github.com/tshort/StaticCompiler.jl\">StaticCompiler</a> being the most well known compilation tools available at the time of writing. From an intermediate Julia user&#39;s point of view, I&#39;ve found that compilation results may vary. Also, to the best of my knowledge most compilation tools actually <em>package</em> code rather than statically compile it, which may expose valuable IP. Therefore, I concluded that Julia <em>probably</em> isn&#39;t as easy to compile as I initially thought (and hoped).  </p>\n<h3 id=\"elixir\">Elixir</h3>\n<p><a href=\"https://elixir-lang.org/\">Elixir</a> is a fantastic hosted functional language, running on the tried and tested <a href=\"https://en.wikipedia.org/wiki/BEAM_(Erlang_virtual_machine)\">BEAM (Erlang VM)</a>. One of the many things Elixir has going for it, is its strong drive towards <em>good</em> documentation. The language&#39;s REPL is also excellent. All in all, Elixir is rapidly evolving and it&#39;s worth experimenting with.<br>Starting from 2021, <a href=\"https://github.com/elixir-nx\">Numerical Elixir (Nx)</a> has progressed by leaps and bounds. The Nx community has managed to produce excellent libraries, with <a href=\"https://livebook.dev/\">Livebook</a> being the best literate programming environment I&#39;ve ever used. As far as data applications are concerned, Elixir will become a <em>very strong</em> contender, it&#39;s well worth keeping a close eye on the language.<br>As for cross-compilation, to my understanding <a href=\"https://hex.pm/packages/burrito\">Burrito</a> is the only tool that allows for packaged Elixir code to be truly portable albeit producing sizeable executables. Burrito is still WIP, not a guaranteed solution for the time being but a noteworthy tool that&#39;s improving fast.<br>Being doubtful as to whether this tech stack could meet all my current needs, beside being a niche language in Data and AI, led me to search for another tech stack for fun and profit.   </p>\n<h3 id=\"deno-typescript\">Deno (TypeScript)</h3>\n<p>More recently, especially given many AI Engineering APIs are written both in Python <em>and</em> TypeScript, I started using Deno. The idea is to leverage Deno for all my computational needs, since it&#39;s an all-in-one, straightforward to use runtime. Installation and setup were a breeze, Deno comes with <em>all</em> the tools a developer requires (formatter, linter, testing suite, package manager etc.), it plays very nicely with Vim, it&#39;s lightweight, secure, compatible with NPM packages and the list goes on. Importantly, it can easily cross-compile executables. The data and AI ecosystem is not yet as mature as that of Python. However, if someone is willing to put in the effort, I&#39;ve found that it&#39;s well worth the investment. This is why I am betting on Deno for my Data Science and AI needs. </p>\n<h2 id=\"what-about-python-cross-compilation\">What About Python Cross-Compilation?</h2>\n<p><strong>How hard could it be?</strong> 🤔<br><strong>TL;DR:</strong> it&#39;s an involved process that requires access to a CI/CD platform such as GitHub Actions. Once a pipeline is in place, it&#39;s a straightforward process that requires internet access and registering to a hosting service such as GitHub.  </p>\n<p>I wrote two pipelines, one for generating a <a href=\"https://github.com/ai-mindset/py-cross-compile/blob/main/.github/workflows/unix-build.yml\">Unix build</a> and one for <a href=\"https://github.com/ai-mindset/py-cross-compile/blob/main/.github/workflows/win-build.yml\">Windows</a>. The result is pretty decent, however the cumbersome process and reliance on third party tech (GitHub Actions in this case) strengthened my conviction that Deno and TypeScript are worth investing in, for a more complete solution. The JS/DS Data[^2] ecosystem is not as mature yet but it&#39;s evolving pretty fast.  </p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The process of cross-compiling a simple Python app was pretty instructive. The main downside I see is the reliance on a hosting service and a CI/CD platform. Frankly, having access to a hosting service and using CI/CD is almost a given in my line of work. Still, it&#39;s nowhere near as straightforward as running <code>$ deno compile main.ts</code>\nI am considering attempting the same using <a href=\"https://podman.io/\">Podman</a>, since <a href=\"https://learn.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/container-base-images\">Windows</a>, <a href=\"https://github.com/sickcodes/Docker-OSX\">macOS</a> and <a href=\"https://hub.docker.com/_/ubuntu/\">Linux</a> containers are available. Stay tuned for updates!</p>\n<hr>\n<p>[^1]: One might argue that running statically compiled executables in a sandboxed environment is a security risk. Static malware analysis tools exist for this exact reason<br>[^2]: To be fair, the Data ecosystem is pretty decent and continuously improving. It&#39;s the ML and statistical ecosystem and specifically the lack of a native Scikit-learn and Scipy-like packages that&#39;s still somewhat lacking </p>\n"
  },
  {
    "title": "💡 TIL: 1.58-bit LLMs Match Full Performance @ 98.6% Energy Reduction",
    "date": "2024-10-30T00:00:00.000Z",
    "tags": [
      "til",
      "llm",
      "performance",
      "energy-reduction"
    ],
    "url": "/posts/TIL-1bitLLM.html",
    "content": "<p><strong>TL;DR:</strong> Ternary-weighted LLMs using only {-1, 0, 1} values (1.58 bits) can match full-precision performance while delivering dramatic efficiency improvements: 2.71x faster inference, 2.55x lower memory usage, and 71.4x lower energy consumption for matrix operations at 3B parameter scale.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Back in February 2024, a preprint titled <a href=\"https://arxiv.org/abs/2402.17764\">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a> was released. Lots of people picked up on it, simply search for <em>1.58-bits</em> on YouTube for instance, however it escaped me due to <a href=\"https://xcancel.com/AdamMGrant/status/1851348990589354464\">a busy time at work</a>. It was only when I stumbled across this preprint again recently, that I realised what a fantastic idea it is to <a href=\"https://www.youtube.com/watch?v=wCDGiys-nLA\">substitute multiplication with addition or subtraction</a>. </p>\n<h2 id=\"contributions\">Contributions</h2>\n<p>The TL;DR is that all LLM weights can be ternary i.e. {-1, 0, 1}. Ternary weights are 1.58-bits. Activations are 8-bits. This highly quantised model matches full-precision performance at 3B parameter scale.<br>This highly quantised model exhibits 2.71x faster inference, 2.55x lower memory usage at 3B scale, 71.4x lower energy consumption for matrix multiplication operations. Benefits increase with model scale e.g. 4.1x speed-up at 70B parameters, 8.9x higher throughput, 11x larger batch size. </p>\n<h2 id=\"what-does-158-bits-mean\">What Does 1.58-bits Mean?</h2>\n<p>Carnegie Mellon University has a great reference on the <a href=\"https://www.cs.cmu.edu/~dst/Tutorials/Info-Theory/\">basics of Information theory</a>. Learning how to measure information content for a ternary system {-1, 0, 1}, we notice that:<br>Each value {-1, 0, 1} has an equal probability $P = \\frac{1}{3}$ for each state. \nThe information content is $-(P \\log_2{P})$ summed over all states </p>\n<p>$$\n-(\\frac{1}{3} \\log_2(\\frac{1}{3}) + \\frac{1}{3} \\log_2(\\frac{1}{3}) + \\frac{1}{3} \\log_2(\\frac{1}{3}))<br>= -(3 × (\\frac{1}{3} \\log_2(\\frac{1}{3})))<br>= -\\log_2(\\frac{1}{3})\n\\approx 1.58496... bits<br>$$  </p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>LLMs can achieve comparable performance to full-precision models while using only three weight values {-1, 0, 1}, achieving up to 71.4x lower energy consumption for matrix operations and 3.55x lower memory usage at 3B scale. This breakthrough suggests a new direction for efficient LLM deployment, particularly promising for edge devices and mobile applications, while also opening opportunities for specialized hardware optimized for 1-bit operations.</p>\n"
  },
  {
    "title": "🗃️ RAG Is Here To Stay",
    "date": "2024-10-29T00:00:00.000Z",
    "tags": [
      "rag",
      "llm",
      "ai",
      "performance"
    ],
    "url": "/posts/rag-is-here-to-stay.html",
    "content": "<p><strong>TL;DR:</strong> Despite larger LLM context windows, Retrieval-Augmented Generation (RAG) remains essential for information curation, data provenance, and overcoming the &quot;lost in the middle&quot; effect where models struggle with information placed centrally in long contexts-making careful retrieval strategies more valuable than simply dumping large amounts of text into expanded context windows.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>This morning I noticed that <a href=\"https://xcancel.com/simonw/status/1850928417363149049\">Simon Willison shared some views on RAG</a>, <a href=\"https://xcancel.com/burkov/status/1851159933913280647\">Andryi Burkov criticised</a> people who claim that RAG is obsolete, and other RAG-related discussions taking place sparked by recent longer LLM context windows. Below I&#39;m sharing some thoughts based on personal experience.   </p>\n<h2 id=\"rag\">RAG</h2>\n<p>RAG is not simply a workaround to context limits, it&#39;s a way to carefully curate information and data. It enables provenance and visibility of the data flowing through an LLM pipeline -compared to fine-tuning which bakes knowledge into the model itself. Importantly, RAG is not a synonym of embeddings. Embedding text is a fantastic way to enable semantic search, especially if it is done in a smart way (word, sentence, paragraph, or document) given project needs.<br>I have successfully reused existing infrastructure to provide one of the largest companies in the world with the ability to quickly retrieve information through Q &amp; A. To achieve this, in the context of simplicity and leveraging existing infrastructure, I opted against adding moving parts like a Vector DB. Instead, I used plain JSON objects and an agentic system to meet the client&#39;s needs. It worked very well, with feedback from higher management being &quot;<em><strong>thank you</strong>, this is mind-blowing</em>&quot;.<br>A nice overview of RAG comes from <a href=\"https://www.latent.space/p/llamaindex\">Jerry Liu&#39;s interview on Latent Space</a>.<br><em>Update: a useful open-source tool for <a href=\"https://github.com/Brandon-c-tech/RAG-logger\">RAGLogging</a> just came out.</em> </p>\n<h2 id=\"u-shaped-performance\">U-Shaped Performance</h2>\n<p>One LLM behaviour that should be considered, before regarding RAG obsolete, is their tendency to attend to information from the beginning and end of the context window. See <a href=\"https://arxiv.org/abs/2307.03172\">Lost in the Middle: How Language Models Use Long Contexts</a> for an empirical analysis.<br>The paper concludes</p>\n<blockquote>\n<p>We empirically study how language models use long input contexts via a series of controlled experiments. We show that language model performance degrades significantly when changing the position of relevant information, indicating that models struggle to robustly access and use information in long input contexts. In particular, performance is often lowest when models must use information in the middle of long input contexts.We conduct a preliminary investigation of the role of (i) model architecture, (ii) query-aware contextualisation, and (iii) instruction fine-tuning to better understand how they affect how language models use context. Finally, we conclude with a practical case study of open-domain question answering,finding that the performance of language model readers saturates far before retriever recall. Our results and analysis provide a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.<br>In other words, simply dumping loads of text or embeddings into an LLM with a big context window -say 2M tokens- won&#39;t yield great results. There&#39;s more to it than brute forcing.    </p>\n</blockquote>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Extending context length, as appealing as it may sound, neither simplifies nor solves the issue of creating a good quality AI system that is enriched by large text corpora. It seems that when it comes to larger data volumes, <a href=\"https://www.youtube.com/watch?v=5e1Wzbr8wGU\">semantic search augmented with Graph search</a> could be a more robust, albeit more involved, approach. Solid prompt engineering approaches, including <a href=\"https://www.promptingguide.ai/techniques/cot\">Chain-of-Thought</a>, <a href=\"https://www.promptingguide.ai/techniques/fewshot\">Few-shot prompting</a> etc. are also powerful tools to keep in our toolbox.    </p>\n"
  },
  {
    "title": "💡 TIL: Useful Nuggets from AI Engineers",
    "date": "2024-10-25T00:00:00.000Z",
    "tags": [
      "til",
      "ai",
      "llm"
    ],
    "url": "/posts/TIL-useful-AI-nuggets.html",
    "content": "<p><strong>TL;DR:</strong> Leading AI researchers emphasise that success in the field now depends more on engineering skills and adaptability than academic credentials, with the most valuable skills being prioritisation, communication, project selection, and willingness to manually inspect data-suggesting that choosing the right problem can multiply output by 10-100x more effectively than simply working longer hours.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>As I was going through some <a href=\"https://web.stanford.edu/class/cs25/\">CS25: Transformers United V4</a> lectures from Stanford, I stumbled across some pertinent and useful quotes from guest lecturers.   </p>\n<h2 id=\"📖\">📖</h2>\n<h3 id=\"best-ai-skillset\"><a href=\"https://xcancel.com/_jasonwei/status/1631017964286922753?cxt=HHwWgsDS-c2MxaItAAAA\">Best AI skillset</a></h3>\n<blockquote>\n<p>Best AI skillset in 2018: PhD + long publication record in a specific area<br>Best AI skillset in 2023: strong engineering abilities + adapting quickly to new directions without sunk cost fallacy</p>\n</blockquote>\n<h3 id=\"advice-on-choosing-a-topic\"><a href=\"https://xcancel.com/_jasonwei/status/1514327894746574851\">Advice on choosing a topic</a></h3>\n<blockquote>\n<p>[...] the project you choose defines the upper-bound for your success. </p>\n</blockquote>\n<h3 id=\"study-the-change-itself\"><a href=\"https://docs.google.com/presentation/d/1u05yQQaw4QXLVYGLI6o3YoFHv6eC3YN8GvWD8JMumpE\">Study the change itself</a></h3>\n<blockquote>\n<p>AI is advancing so fast it is hard to keep up. People spend a lot of time and energy catching up with the latest developments. But not enough attention goes to the old things. It is more important to <em>study the change itself</em></p>\n</blockquote>\n<h3 id=\"why-im-100-transparent-with-my-manager\"><a href=\"https://xcancel.com/_jasonwei/status/1699860824053911558\">Why I&#39;m 100% transparent with my manager</a></h3>\n<blockquote>\n<p>I try to open this [performance] conversation [with my line manager] by asking &quot;what can I do better&quot;.<br>I tend to use my 1-1s to talk about bigger picture stuff. [...] since that&#39;s where managers can help the most.\nOf course, all this [honesty with your manager] going well is conditional on working in a healthy company and having a decent manager. [...] do you want to keep working for someone who doesn&#39;t ask for feedback, or who doesn&#39;t take your problems seriously?  </p>\n</blockquote>\n<h3 id=\"my-strengths-are-communication-and-prioritization\"><a href=\"https://xcancel.com/_jasonwei/status/1689346627428036608\">My strengths are communication and prioritization</a></h3>\n<blockquote>\n<p>[...] a friend recently asked me what were the best skills I had. [...] I said prioritization and communication. These skills are relatively general but happen to be very important for AI research.   </p>\n</blockquote>\n<h3 id=\"many-great-managers-do-ic-individual-contributor-work\"><a href=\"https://xcancel.com/_jasonwei/status/1701665241652945283\">Many great managers do IC (Individual Contributor) work</a></h3>\n<blockquote>\n<p>It seems to be not a coincidence that some of the strongest leaders in AI who manage large teams frequently do very low-level technical work.</p>\n</blockquote>\n<h3 id=\"manually-inspect-data\"><a href=\"https://xcancel.com/_jasonwei/status/1708921475829481683\">Manually inspect data</a></h3>\n<blockquote>\n<p>[...] great AI researchers are willing to manually inspect lots of data. And more than that, they build infrastructure that allows them to manually inspect data quickly. Though not glamorous, manually examining data gives valuable intuitions about the problem.</p>\n</blockquote>\n<h3 id=\"read-informal-write-ups\"><a href=\"https://xcancel.com/_jasonwei/status/1731780538405716078\">Read informal write-ups</a></h3>\n<blockquote>\n<p>[...] I like to look at the process of how they [great researchers] got there. </p>\n</blockquote>\n<h3 id=\"advice-from-bryan-johnson\"><a href=\"https://xcancel.com/_jasonwei/status/1766692847078756557\">Advice from Bryan Johnson</a></h3>\n<blockquote>\n<p>[...]  having good health enables clear thinking, which is by far the biggest leverage in AI. [...] While it&#39;s possible to double output by working twice as many hours, choosing a better project has the potential to 10x or even 100x output.  </p>\n</blockquote>\n"
  },
  {
    "title": "🔁 GitHub Actions for yt-dlp-hq",
    "date": "2024-10-08T00:00:00.000Z",
    "tags": [
      "github-actions",
      "ci-cd",
      "yt-dlp",
      "deno",
      "typescript",
      "cross-platform"
    ],
    "url": "/posts/gh-actions-ytdlphq.html",
    "content": "<p><strong>TL;DR:</strong> This article details the development of a Deno-based tool for downloading high-quality videos with audio using yt-dlp, highlighting unexpected challenges with GitHub Actions where compiled executables became unusable after release-ultimately solved by compressing executables into .tar files, preserving functionality whilst revealing potential limitations in GitHub&#39;s release mechanisms.</p>\n<!--more-->\n<p><strong><a href=\"https://xkcd.com/1205/\">Was it worth my time</a>?</strong> </p>\n<h2 id=\"introduction\">Introduction</h2>\n<p>There are times where I need to use my computer offline, e.g. when I&#39;m travelling. Having to stay offline is a good opportunity for me to study some lectures of interest, without distractions. For that, I need offline access to the videos I&#39;m interested in.<br><a href=\"https://github.com/yt-dlp/yt-dlp\">yt-dlp</a> is a great open-source project that allows the user to download audio and/or video from a wide array of platforms including YouTube. Recently, I noticed that it&#39;s no longer as straightforward to download a video with audio, using <code>yt-dlp</code>. One workaround is to download the audio and video streams separately, and merge them using <a href=\"https://ffmpeg.org/\">FFmpeg</a>. This was a good opportunity to write a small automation project in a language I&#39;m interested in.  </p>\n<h3 id=\"yt-dlp-and-youtube\"><code>yt-dlp</code> And YouTube</h3>\n<p>Here&#39;s an example that motivates implementing this project. Imagine I&#39;d like to download a video from the excellent <a href=\"https://www.youtube.com/channel/UCKWaEZ-_VweaEx1j62do_vQ\">IBM Technology</a> YouTube channel, for instance <a href=\"https://www.youtube.com/watch?v=F8NKVhkZZWI\">What are AI Agents</a>. Listing the video&#39;s available formats, returns the following table   </p>\n<pre><code class=\"language-console\">$ yt-dlp -F https://www.youtube.com/watch\\?v\\=F8NKVhkZZWI\n[youtube] Extracting URL: https://www.youtube.com/watch?v=F8NKVhkZZWI\n[youtube] F8NKVhkZZWI: Downloading webpage\n[youtube] F8NKVhkZZWI: Downloading ios player API JSON\n[youtube] F8NKVhkZZWI: Downloading web creator player API JSON\n[youtube] F8NKVhkZZWI: Downloading m3u8 information\n[info] Available formats for F8NKVhkZZWI:\nID      EXT   RESOLUTION FPS CH │   FILESIZE   TBR PROTO │ VCODEC          VBR ACODEC      ABR ASR MORE INFO\n─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\nsb3     mhtml 48x27        0    │                  mhtml │ images                                  storyboard\nsb2     mhtml 80x45        0    │                  mhtml │ images                                  storyboard\nsb1     mhtml 160x90       0    │                  mhtml │ images                                  storyboard\nsb0     mhtml 320x180      0    │                  mhtml │ images                                  storyboard\n233     mp4   audio only        │                  m3u8  │ audio only          unknown             [en] Default\n234     mp4   audio only        │                  m3u8  │ audio only          unknown             [en] Default\n139-drc m4a   audio only      2 │    4.35MiB   49k https │ audio only          mp4a.40.5   49k 22k [en] low, DRC, m4a_dash\n139     m4a   audio only      2 │    4.35MiB   49k https │ audio only          mp4a.40.5   49k 22k [en] low, m4a_dash\n249     webm  audio only      2 │    4.37MiB   49k https │ audio only          opus        49k 48k [en] low, webm_dash\n250     webm  audio only      2 │    5.27MiB   59k https │ audio only          opus        59k 48k [en] low, webm_dash\n140-drc m4a   audio only      2 │   11.55MiB  129k https │ audio only          mp4a.40.2  129k 44k [en] medium, DRC, m4a_dash\n140     m4a   audio only      2 │   11.55MiB  129k https │ audio only          mp4a.40.2  129k 44k [en] medium, m4a_dash\n251     webm  audio only      2 │    9.45MiB  106k https │ audio only          opus       106k 48k [en] medium, webm_dash\n602     mp4   256x144     15    │ ~  7.26MiB   81k m3u8  │ vp09.00.10.08   81k video only\n394     mp4   256x144     30    │    4.36MiB   49k https │ av01.0.00M.08   49k video only          144p, mp4_dash\n269     mp4   256x144     30    │ ~ 11.26MiB  126k m3u8  │ avc1.4D400C    126k video only\n160     mp4   256x144     30    │    2.97MiB   33k https │ avc1.4D400C     33k video only          144p, mp4_dash\n603     mp4   256x144     30    │ ~ 13.63MiB  153k m3u8  │ vp09.00.11.08  153k video only\n278     webm  256x144     30    │    7.23MiB   81k https │ vp09.00.11.08   81k video only          144p, webm_dash\n395     mp4   426x240     30    │    5.90MiB   66k https │ av01.0.00M.08   66k video only          240p, mp4_dash\n229     mp4   426x240     30    │ ~ 14.99MiB  168k m3u8  │ avc1.4D4015    168k video only\n133     mp4   426x240     30    │    4.49MiB   50k https │ avc1.4D4015     50k video only          240p, mp4_dash\n604     mp4   426x240     30    │ ~ 21.46MiB  241k m3u8  │ vp09.00.20.08  241k video only\n242     webm  426x240     30    │    7.38MiB   83k https │ vp09.00.20.08   83k video only          240p, webm_dash\n396     mp4   640x360     30    │   10.27MiB  115k https │ av01.0.01M.08  115k video only          360p, mp4_dash\n230     mp4   640x360     30    │ ~ 29.86MiB  335k m3u8  │ avc1.4D401E    335k video only\n134     mp4   640x360     30    │    7.76MiB   87k https │ avc1.4D401E     87k video only          360p, mp4_dash\n18      mp4   640x360     30  2 │   32.38MiB  363k https │ avc1.42001E         mp4a.40.2       44k [en] 360p\n605     mp4   640x360     30    │ ~ 39.56MiB  444k m3u8  │ vp09.00.21.08  444k video only\n243     webm  640x360     30    │   12.52MiB  140k https │ vp09.00.21.08  140k video only          360p, webm_dash\n397     mp4   854x480     30    │   17.06MiB  191k https │ av01.0.04M.08  191k video only          480p, mp4_dash\n231     mp4   854x480     30    │ ~ 37.90MiB  425k m3u8  │ avc1.4D401F    425k video only\n135     mp4   854x480     30    │   11.28MiB  126k https │ avc1.4D401F    126k video only          480p, mp4_dash\n606     mp4   854x480     30    │ ~ 50.26MiB  564k m3u8  │ vp09.00.30.08  564k video only\n244     webm  854x480     30    │   17.41MiB  195k https │ vp09.00.30.08  195k video only          480p, webm_dash\n398     mp4   1280x720    30    │   31.31MiB  351k https │ av01.0.05M.08  351k video only          720p, mp4_dash\n232     mp4   1280x720    30    │ ~ 57.85MiB  649k m3u8  │ avc1.4D401F    649k video only\n136     mp4   1280x720    30    │   20.16MiB  226k https │ avc1.4D401F    226k video only          720p, mp4_dash\n609     mp4   1280x720    30    │ ~ 72.26MiB  810k m3u8  │ vp09.00.31.08  810k video only\n247     webm  1280x720    30    │   27.69MiB  310k https │ vp09.00.31.08  310k video only          720p, webm_dash\n399     mp4   1920x1080   30    │   63.06MiB  707k https │ av01.0.08M.08  707k video only          1080p, mp4_dash\n270     mp4   1920x1080   30    │ ~193.27MiB 2167k m3u8  │ avc1.640028   2167k video only\n137     mp4   1920x1080   30    │   96.17MiB 1078k https │ avc1.640028   1078k video only          1080p, mp4_dash\n614     mp4   1920x1080   30    │ ~164.68MiB 1847k m3u8  │ vp09.00.40.08 1847k video only\n248     webm  1920x1080   30    │   91.43MiB 1025k https │ vp09.00.40.08 1025k video only          1080p, webm_dash\n616     mp4   1920x1080   30    │ ~322.53MiB 3617k m3u8  │ vp09.00.40.08 3617k video only          Premium\n</code></pre>\n<p>It looks like the only ID containing a video <em>with</em> audio, is <code>18</code>, i.e. a 420p, 640x360 video according to my media player. This might be sufficient for a video like the above, but such low resolution would make it almost impossible to read code or smaller writing.</p>\n<h2 id=\"my-solution\">My Solution</h2>\n<p>Given I have started leveraging Deno for my needs, I wrote a small tool called <a href=\"https://github.com/ai-mindset/yt-dlp-hq\">yt-dlp-hq</a>. It&#39;s certainly basic, with lots of room for improvement. However it does exactly what I need and I&#39;m relatively happy with the result, pending some improvements[^1].<br>Deno is great for cross-compilation. Also, GitHub Actions can be a good method for automating testing, running, compiling etc.[^2] Is it though? Let&#39;s see.  </p>\n<h2 id=\"my-ci-pipeline\">My CI Pipeline</h2>\n<p>I started off by using <a href=\"https://nektosact.com/introduction.html\">act</a>, a very nice tool that allows for testing pipelines locally. The main downside I found was that for an intermediate Docker user with little <code>act</code> experience, sometimes GitHub Actions don&#39;t behave the same way locally as they would online. Also, I like <a href=\"https://podman.io/\">podman</a> considerably better, since it&#39;s daemonless and not as resource-hungry among others.<br>Putting <code>act</code> aside, I focused on setting up a <a href=\"https://github.com/ai-mindset/yt-dlp-hq/blob/main/.github/workflows/ci.yml\">pipeline</a> that&#39;d work well enough with every new PR opened against <code>main</code> aside from others.<br>The pipeline ran successfully, where in theory it built and released <code>yt-dlp-hq</code> executables. However, when I downloaded the corresponding executable for my OS and CPU architecture, it did not run. When I locally built the same set of executables, running <code>deno task build</code>, the executable for my OS &amp; arch worked as expected. This made me wonder whether I&#39;m doing something wrong, if it&#39;s a GitHub Action intricacy or some other issue I needed to resolve. \nInspired by Medicine, I tried approaching the issue through differential diagnosis, which to my understanding works by excluding other causes in order to hone in on the actual medical condition. I.e. I first created a release directory locally. I then manually created a release on GitHub. To my dismay, the executable I manually uploaded didn&#39;t run when I downloaded it back from GitHub. This made me wonder if there is a conversion involved when a pipeline generates executables or the user uploads them manually for release. Spoiler alert: I still don&#39;t know if that&#39;s the case, but I suspect that GitHub indeed doesn&#39;t save executables without some change taking place during upload. </p>\n<h3 id=\"fixing-executables-github-release\">Fixing Executables GitHub Release</h3>\n<p>Initially, I changed the following setting on my repository:<br><em>&quot;Settings -&gt; Actions -&gt; General -&gt; Workflow permissions&quot;</em> select  <em>&quot;Read and write permissions&quot;</em>.\nThen, I experimented with compressing each generated executable into a .tar file. This did the trick. Simply compressing an executable is enough to maintain its function. Thus, the way to install <code>yt-dlp-hq</code> takes one extra step.<br>For example, if you&#39;re a Linux user on an Intel-based machine, here&#39;s how you can use my tool   </p>\n<pre><code class=\"language-console\">$ curl -L -O https://github.com/ai-mindset/yt-dlp-hq/releases/download/1.0.0/yt-dlp-hq-intel-linux.tar &amp;&amp; tar xvf yt-dlp-hq-intel-linux.tar &amp;&amp; cd release\n$ ./yt-dlp-hq-intel-linux https://www.youtube.com/watch?v=dQw4w9WgXcQ\n</code></pre>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>I&#39;m glad I learned something more about GitHub and Actions, its idiosyncrasies and abilities. It took me a couple days, which made me consider the benefits of <a href=\"https://xkcd.com/1319/\">automation</a>. Being more minimalistic, I tend to opt for simple automation when possible <a href=\"https://xkcd.com/1205/\"><em>if</em> it&#39;s worth it</a>. To quote <a href=\"https://en.wikiquote.org/wiki/Alan_Perlis\">Alan Perlis</a>, &quot;<em>Simplicity does not precede complexity, but follows it</em>&quot;.</p>\n<hr>\n<p>[^1]: Some improvements I&#39;m planning include unit testing, automatic audio &amp; video ID selection and possibly automatic FFmpeg installation when it&#39;s not available in <code>$PATH</code>.<br>[^2]: A <a href=\"https://julialang.org/\">Juiia</a> enthusiast introduced me to <a href=\"https://woodpecker-ci.org/\">Woodpecker CI</a> and <a href=\"https://codeberg.org/\">Codeberg</a>. I&#39;m definitely considering switching, following my recent GitHub Actions experience 🤔 </p>\n"
  },
  {
    "title": "📖 Python To TypeScript Cheatsheet",
    "date": "2024-09-06T00:00:00.000Z",
    "tags": [
      "python",
      "typescript",
      "cheatsheet"
    ],
    "url": "/posts/python-typescript-cheatsheet.html",
    "content": "<p><strong>TL;DR:</strong> This compact reference guide provides side-by-side comparisons of Python and TypeScript syntax for common programming constructs including variable declarations, functions, classes, control flow structures, and error handling-serving as a quick reference for Python developers exploring TypeScript within the context of Deno development.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>I&#39;ve been curious as to how Python and TypeScript compare at a high level, for someone new to TypeScript. Below is a chatsheet I put together with the help of Claude 3.5 Sonnet. It covers basic syntax, it&#39;s by no means complete or exhaustive. However it gives a first taste of the similarities and differences between the two languages. The reason I am looking into TypeScript is explained in my <a href=\"../deno/\">Deno article</a>.</p>\n<h2 id=\"variables-and-data-types\">Variables And Data Types</h2>\n<table>\n<thead>\n<tr>\n<th>Concept</th>\n<th>Python</th>\n<th>TypeScript</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Int</td>\n<td><code>x = 5</code></td>\n<td><code>let x: number = 5;</code></td>\n</tr>\n<tr>\n<td>Float</td>\n<td><code>y = 3.14</code></td>\n<td><code>let y: number = 3.14;</code></td>\n</tr>\n<tr>\n<td>Str</td>\n<td><code>name = &quot;John&quot;</code></td>\n<td><code>let name: string = &quot;John&quot;;</code></td>\n</tr>\n<tr>\n<td>Bool</td>\n<td><code>is_valid = True</code></td>\n<td><code>let isValid: boolean = true;</code></td>\n</tr>\n<tr>\n<td>List</td>\n<td><code>numbers = [1, 2, 3]</code></td>\n<td><code>let numbers: number[] = [1, 2, 3];</code></td>\n</tr>\n<tr>\n<td>Dict</td>\n<td><code>person = {&quot;name&quot;: &quot;John&quot;, &quot;age&quot;: 30}</code></td>\n<td><code>let person: { name: string; age: number } = { name: &quot;John&quot;, age: 30 };</code></td>\n</tr>\n</tbody></table>\n<h2 id=\"functions\">Functions</h2>\n<table>\n<thead>\n<tr>\n<th>Concept</th>\n<th>Python</th>\n<th>TypeScript</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Func def</td>\n<td><code>def greet(name: str) -&gt; str:</code></td>\n<td><code>function greet(name: string): string {</code></td>\n</tr>\n<tr>\n<td>Func return</td>\n<td><code>return f&quot;Hello, {name}!&quot;</code></td>\n<td><code>return `Hello, ${name}!`;</code></td>\n</tr>\n<tr>\n<td>Lambda</td>\n<td><code>lambda x: x * 2</code></td>\n<td><code>(x: number): number =&gt; x * 2</code></td>\n</tr>\n</tbody></table>\n<h2 id=\"classes\">Classes</h2>\n<table>\n<thead>\n<tr>\n<th>Concept</th>\n<th>Python</th>\n<th>TypeScript</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Class def</td>\n<td><code>class Person:</code></td>\n<td><code>class Person {</code></td>\n</tr>\n<tr>\n<td>Constructor</td>\n<td><code>def __init__(self, name: str, age: int):</code></td>\n<td><code>constructor(name: string, age: number) {</code></td>\n</tr>\n<tr>\n<td>Instance vars</td>\n<td><code>self.name = name</code></td>\n<td><code>this.name = name;</code></td>\n</tr>\n<tr>\n<td></td>\n<td><code>self.age = age</code></td>\n<td><code>this.age = age;</code></td>\n</tr>\n<tr>\n<td>Method def</td>\n<td><code>def greet(self) -&gt; str:</code></td>\n<td><code>greet(): string {</code></td>\n</tr>\n<tr>\n<td>Method return</td>\n<td><code>return f&quot;Hello, I&#39;m {self.name}!&quot;</code></td>\n<td><code>return `Hello, I&#39;m ${this.name}!`;</code></td>\n</tr>\n</tbody></table>\n<h2 id=\"control-flow\">Control Flow</h2>\n<table>\n<thead>\n<tr>\n<th>Concept</th>\n<th>Python</th>\n<th>TypeScript</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>If</td>\n<td><code>if x &gt; 0:</code></td>\n<td><code>if (x &gt; 0) {</code></td>\n</tr>\n<tr>\n<td>Else if</td>\n<td><code>elif x &lt; 0:</code></td>\n<td><code>} else if (x &lt; 0) {</code></td>\n</tr>\n<tr>\n<td>Else</td>\n<td><code>else:</code></td>\n<td><code>} else {</code></td>\n</tr>\n<tr>\n<td>For loop</td>\n<td><code>for i in range(5):</code></td>\n<td><code>for (let i = 0; i &lt; 5; i++) {</code></td>\n</tr>\n<tr>\n<td>While loop</td>\n<td><code>while x &gt; 0:</code></td>\n<td><code>while (x &gt; 0) {</code></td>\n</tr>\n</tbody></table>\n<h2 id=\"error-handling\">Error Handling</h2>\n<table>\n<thead>\n<tr>\n<th>Concept</th>\n<th>Python</th>\n<th>TypeScript</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Try</td>\n<td><code>try:</code></td>\n<td><code>try {</code></td>\n</tr>\n<tr>\n<td>Except/Catch</td>\n<td><code>except ZeroDivisionError:</code></td>\n<td><code>} catch (error) {</code></td>\n</tr>\n<tr>\n<td>Finally</td>\n<td><code>finally:</code></td>\n<td><code>} finally {</code></td>\n</tr>\n</tbody></table>\n"
  },
  {
    "title": "🏗️ Modern Data Science and AI Engineering with Deno 2.0",
    "date": "2024-09-05T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "cross-platform",
      "data-processing",
      "data-science",
      "deno",
      "machine-learning",
      "minimal",
      "polars",
      "production",
      "deployment",
      "toolchain",
      "typescript",
      "security",
      "zero-config"
    ],
    "url": "/posts/deno.html",
    "content": "<p><strong>TL;DR:</strong> Deno 2.0 offers a compelling alternative to Python for AI and data science workflows by providing zero-configuration TypeScript support, native security features, cross-compilation capabilities, and an ecosystem of essential tools-addressing Python&#39;s environment management complexities and deployment frictions whilst enabling production-ready development from proof-of-concept through to single-binary distribution.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>The landscape of Data Science and AI engineering is at a critical inflection point. While Python has dominated data science and machine learning, its fragmented ecosystem and deployment complexities increasingly impede production systems. I&#39;ve already touched on [my solution to Python&#39;s fragmentation]({{ site.baseurl }}{% link _posts/2024-11-21-bring-it-back-to-basics.md %}).<br>Deno 2.0 emerges as a compelling solution to these challenges, bringing together a number of technologies that cover most computing requirements across a very wide range of domains. Having said that, JavaScript (JS) and its superset TypeScript (TS) are <a href=\"https://www.youtube.com/watch?v=aXOChLn5ZdQ\">far from perfect languages</a>[^1], but this discussion is outside the scope of this blog post.</p>\n<p>The key factors driving change are:</p>\n<ul>\n<li>Python environment management complexity</li>\n<li>Production security requirements</li>\n<li>Deployment workflow friction</li>\n<li>Need for type safety in large-scale AI applications</li>\n</ul>\n<h2 id=\"the-deno-advantage-and-ecosystem\">The Deno Advantage and Ecosystem</h2>\n<h3 id=\"core-capabilities\">Core Capabilities</h3>\n<p>Deno 2.0 provides a comprehensive, zero-configuration solution with:</p>\n<ul>\n<li>Native TS support</li>\n<li>First-class security features</li>\n<li>Cross-compilation through <code>deno compile</code></li>\n<li>Built-in development tools</li>\n</ul>\n<p>As Ryan Dahl emphasized in a recent <a href=\"https://www.youtube.com/watch?v=tZBCq8Ijkgw\">Syntax podcast episode</a>: &quot;<em>Deno works really great as a single file. It&#39;s really great for scripting, [...] you can just put some imports in and start working from a single file. And that is actually exactly what you want from notebooks</em>&quot;. This aligns with recent work by <a href=\"https://www.answer.ai/\">Answer.AI</a>&#39;s <a href=\"https://www.alexisgallagher.com/\">Alexis Gallagher</a> on <a href=\"https://youtube.com/watch?v=t6-Uup-Alfs\">single-script Python development</a>.</p>\n<h3 id=\"ai-and-data-processing-tools\">AI and Data Processing Tools</h3>\n<p>The ecosystem provides direct parallels to Python&#39;s essential tools:</p>\n<p>Data Processing:</p>\n<ul>\n<li><a href=\"https://pola-rs.github.io/nodejs-polars/\">nodejs-polars</a> for high-performance DataFrame operations</li>\n<li><a href=\"https://observablehq.com/plot/\">Observable Plot</a> for modern visualisation</li>\n</ul>\n<p>Machine Learning:</p>\n<ul>\n<li><a href=\"https://js.langchain.com/\">LangChain.js</a> and <a href=\"https://ts.llamaindex.ai/\">LlamaIndex.ts</a> for LLM applications</li>\n<li><a href=\"https://huggingface.co/docs/transformers.js/index\">Transformers.js</a> for Hugging Face integration</li>\n<li><a href=\"https://www.tensorflow.org/js\">TensorFlow.js</a> and <a href=\"https://github.com/nuanio/xgboost-node\">XGBoost-node</a> for ML tasks</li>\n</ul>\n<p>Infrastructure:</p>\n<ul>\n<li>Native <a href=\"https://docs.deno.com/runtime/manual/basics/connecting_to_databases/#postgres\">Postgres</a> and <a href=\"https://docs.deno.com/runtime/manual/basics/connecting_to_databases/#mongodb\">MongoDB</a> support</li>\n<li><a href=\"https://github.com/qdrant/qdrant-js\">Qdrant JS</a> for vector storage</li>\n<li><a href=\"https://stdlib.io/docs/api/latest\">STDLib</a> for extended functionality</li>\n<li><a href=\"https://github.com/axa-group/nlp.js/\">NLP.js</a> and <a href=\"https://github.com/spencermountain/compromise\">compromise</a> for NLP</li>\n</ul>\n<h2 id=\"a-pragmatic-decision-framework\">A Pragmatic Decision Framework</h2>\n<p>Modern AI systems need to balance rapid experimentation with production-ready stability. Through experimentation, I&#39;ve found that a TS-based approach using Deno provides an elegant solution to both needs.</p>\n<h3 id=\"production-first-design\">Production-First Design</h3>\n<p>For a start, a zero-configuration Deno-based environment makes it easy to produce code spanning proof of concept (POC) to production. This gives the user native security, cross-compilation capabilities and simple single-binary distribution, eliminating many traditional deployment headaches. While Python remains popular for Data Science and AI research, Deno with simple TS[^2] has been able to handle most of my computational equally well, in a lightweight and productive way.  </p>\n<h3 id=\"practical-implementation-guide\">Practical Implementation Guide</h3>\n<p>Transitioning to this kind of all-in-one Deno-driven architecture can start by utilising tools like LangChain.js or LlamaIndex.ts for LLM applications. Data processing can be handled efficiently through nodejs-polars, while Observable Plots provides powerful visualisation.<br>Emphasising simplicity, we can use REST/GraphQL to handle service communication, with shared data stores and container-based deployment maintaining clear service boundaries. This approach supports both monolithic and microservice architectures, based on project needs.</p>\n<h3 id=\"development-best-practices\">Development Best Practices</h3>\n<p>[Iterative refinement development]({{ site.baseurl }}{% link _posts/2024-11-22-iterative-refinement.md %}) remains an equally productive approach. Strong typing helps with development and code robustness, while correctly used async/await patterns ensure system responsiveness. This approach enables rapid prototyping without sacrificing production readiness.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Leveraging Deno with TS as a replacement for Python is a possible, viable and usually more lightweight alternative for developing more maintainable, secure and production-ready Data and AI systems. Deno&#39;s zero-config setup, extensive tooling, security focus and stability address key pain points I have encountered in my Python development journey.<br>The Deno ecosystem has reached maturity, making it a viable and often superior alternative -in my experience- to traditional Python-based approaches for modern AI engineering workflows.  </p>\n<hr>\n<p>[^1]: There are noteworthy -sadly not as widely used- languages such as <a href=\"https://clojure.org/\">Clojure</a> and <a href=\"https://racket-lang.org/\">Racket</a>, backed by <a href=\"https://en.wikipedia.org/wiki/Lisp_(programming_language)\">computer science research</a>, that pioneered concepts like iterative refinement (aka REPL-driven development) among others.\n[^2]: &quot;simple&quot; in this context refers to leveraging types but avoiding more involved TypeScript ideas.</p>\n"
  }
]