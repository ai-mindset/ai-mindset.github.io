[
  {
    "title": "🦀 Transitioning from Python to Rust: A Minimalist Approach",
    "date": "2025-09-25T00:00:00.000Z",
    "tags": [
      "rust",
      "python",
      "ai-engineering",
      "data-science",
      "migration",
      "type-safety",
      "performance",
      "productivity",
      "software-minimalism"
    ],
    "url": "/posts/transitioning-from-python-to-rust-for-ai.html",
    "content": "<p><strong>TL;DR:</strong> Moving from Python to Rust for AI work requires a phased approach\nfocusing on self-contained utilities first, leveraging PyO3 for hybrid\nintegration, and adopting a minimal subset of Rust features before expanding.\nThis strategy maintains productivity while gradually unlocking Rust&#39;s type\nsafety, performance, and cross-platform deployment advantages.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>After previously discussing the potential of doing AI and Data Science with\n<a href=\"https://ai-mindset.github.io/posts/deno.html\">Deno</a> or\n<a href=\"https://ai-mindset.github.io/posts/go-pragmatic-modern-development.html\">Go</a>,\nI&#39;ve found Rust to be a compelling alternative, offering an ecosystem that\ncovers my needs, memory safety without garbage collection, and a single-binary\ndeployment model.<br>Four Rust libraries, namely</p>\n<ul>\n<li><a href=\"https://rig.rs/\">Rig</a> for LLM applications</li>\n<li><a href=\"https://docs.rs/ndarray/\">ndarray</a> for linear algebra</li>\n<li><a href=\"https://plotters-rs.github.io/home/\">plotters</a> for visualisation</li>\n<li><a href=\"https://docs.pola.rs/\">Polars</a> for DataFrames<br>already cover 90+% of an AI Engineer&#39;s and a Data Scientist&#39;s needs.</li>\n</ul>\n<h2 id=\"phased-migration-strategy\">Phased Migration Strategy</h2>\n<p><strong>1. Start with small, self-contained utilities</strong></p>\n<ul>\n<li>Begin by rewriting simple command-line tools or utilities</li>\n<li>Focus on pure functions with clear inputs/outputs</li>\n<li>Examples: data processors, validators, or simple APIs</li>\n</ul>\n<p><strong>2. Learn incrementally through practical patterns</strong></p>\n<pre><code class=\"language-rust\">// Python:\ndef process_data(items):\n    return [x * 2 for x in items if x &gt; 0]\n\n// Rust equivalent:\nfn process_data(items: &amp;[i32]) -&gt; Vec&lt;i32&gt; {\n    items.iter().filter(|x| **x &gt; 0).map(|x| x * 2).collect()\n}\n</code></pre>\n<p><strong>3. Adopt a hybrid approach during transition</strong></p>\n<ul>\n<li>Use <a href=\"https://pyo3.rs/\">PyO3</a> to call your new Rust code from existing Python</li>\n<li>Gradually replace performance-critical components first</li>\n<li>Keep Python for rapid prototyping until comfortable with Rust</li>\n</ul>\n<p><strong>4. Leverage familiar concepts across languages</strong></p>\n<ul>\n<li>Rust&#39;s iterators ≈ Python&#39;s generators</li>\n<li>Rust&#39;s closures ≈ Python&#39;s lambda functions</li>\n<li>Rust&#39;s Option/Result ≈ Python&#39;s Optional/try-except</li>\n</ul>\n<h2 id=\"keeping-rust-simple-and-robust\">Keeping Rust Simple and Robust</h2>\n<p><strong>1. Start with a subset of Rust features</strong></p>\n<ul>\n<li>Focus on structs, enums, and basic pattern matching</li>\n<li>Defer learning advanced traits, lifetimes, and generics</li>\n<li>Use <code>#[derive]</code> macros to avoid boilerplate</li>\n</ul>\n<p><strong>2. Adopt consistent patterns</strong></p>\n<pre><code class=\"language-rust\">// Prefer simple error handling patterns\nfn get_user(id: u64) -&gt; Result&lt;User, Error&gt; {\n    let user = db.find_user(id)?; // Early return on error\n    Ok(user)\n}\n</code></pre>\n<p><strong>3. Minimise complexity with good defaults</strong></p>\n<ul>\n<li>Use <code>String</code> over <code>&amp;str</code> for return values until comfortable with lifetimes</li>\n<li>Start with <code>Vec&lt;T&gt;</code> before learning specialised collections</li>\n<li>Prefer <code>.clone()</code> initially where ownership is complex</li>\n</ul>\n<p><strong>4. Focus on idiomatic Rust patterns</strong></p>\n<ul>\n<li>Prefer composition over inheritance</li>\n<li>Use enums for representing state</li>\n<li>Leverage the type system to make invalid states unrepresentable</li>\n</ul>\n<p><strong>5. Practical tooling setup</strong></p>\n<ul>\n<li>Install <code>rust-analyzer</code> for your Editor / IDE of choice</li>\n<li>Use <code>clippy</code> to learn idiomatic Rust: <code>cargo clippy</code></li>\n<li>Adopt <code>cargo fmt</code> for consistent formatting</li>\n</ul>\n<p>This approach prioritises practical learning over theoretical completeness,\nallowing you to become productive quickly while gradually adopting Rust&#39;s more\npowerful features as needed.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Migrating from Python to Rust can offer considerable long-term benefits,\nincluding a cohesite ecosystem, native performance, and streamlined deployment,\nwithout requiring complete rewrites. Following a gradual migration path,\nleveraging hybrid integration, and deliberately limiting the developer&#39;s initial\nexposure to Rust&#39;s complexity, one can maintain productivity while acquiring\nexperience. This approach can lead to a minimalist software development cycle\nthat will result in increasingly robust software over time.</p>\n"
  },
  {
    "title": "💡 TIL: Incremental AI Problem-Solving with Solveit",
    "date": "2025-08-26T00:00:00.000Z",
    "tags": [
      "til",
      "fast-ai",
      "answer-ai",
      "solveit",
      "ai",
      "best-practices",
      "llm",
      "performance",
      "productivity",
      "prompt-engineering"
    ],
    "url": "/posts/TIL-solveit.html",
    "content": "<p><strong>TL;DR:</strong> Answer.ai&#39;s Solveit approach mitigates LLM deterioration by breaking\ntasks into small steps, editing AI responses, and providing curated context,\naddressing three key issues: RLHF-trained overenthusiasm, autoregressive\ndecline, and training data flaws.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>A student from fast.ai&#39;s &quot;<a href=\"http://solveit.fast.ai/\">Solve It With Code</a>&quot; course\ndocumented three LLM properties that cause deteriorating AI responses and\ncorresponding mitigation techniques. The course, led by\n<a href=\"https://nitter.poast.org/jeremyphoward\">Jeremy Howard</a> and\n<a href=\"https://nitter.poast.org/johnowhitaker\">Johno Whitaker</a>, focuses on\ntransforming problematic AI interactions into learning experiences through\nsystematic problem-solving.</p>\n<p>The approach addresses what the author terms the &quot;deteriorating response\npattern&quot; -where AI tools initially appear helpful but produce increasingly\nbroken code through subsequent iterations. The common scenario: Request a\nweather app from ChatGPT, receive 100 lines of code that doesn&#39;t work, request\nfixes, encounter additional bugs. This occurs due to fundamental LLM properties,\nnot implementation flaws.</p>\n<h2 id=\"the-three-properties--solutions\">The Three Properties &amp; Solutions</h2>\n<ol>\n<li><em>RLHF creates overly eager helpers</em></li>\n</ol>\n<p>Issue: Human raters prefer complete responses, so models provide overwhelming\namounts of information at once Solution: Work in small steps, ask clarifying\nquestions first Based on Pólya&#39;s problem-solving framework: understand → plan →\nimplement → review</p>\n<ol start=\"2\">\n<li><em>Autoregression leads to deterioration</em></li>\n</ol>\n<p>Issue: Responses degrade over long conversations as models revert to mediocre\ntraining patterns Solution: Edit the LLM&#39;s responses to shape better patterns,\npre-fill outputs, use examples This involves rewriting AI responses to match\npreferred style, then using those as context for subsequent interactions</p>\n<ol start=\"3\">\n<li><em>Training data is flawed/outdated</em></li>\n</ol>\n<p>Issue: Hallucinations and outdated information from lossy compression of\ntraining data Solution: &quot;Jeremy RAG&quot; - manually curating relevant context rather\nthan relying on automated retrieval systems Tools like\n<a href=\"https://github.com/AnswerDotAI/contextkit\">contextkit</a> enable inclusion of\nspecific documentation, followed by verification that the LLM correctly\ninterprets the provided context</p>\n<h2 id=\"application-to-modern-ai-systems\">Application to Modern AI Systems</h2>\n<p>The methodology remains relevant for reasoning models like Claude Code or\nOpenAI&#39;s Deep Research. The primary challenge isn&#39;t that models cannot answer\nquestions, but rather that users often don&#39;t know which questions to ask\ninitially.</p>\n<p>Jeremy connected this to Eric Ries&#39;\n<a href=\"https://theleanstartup.com/principles\">Lean Startup methodology</a>: working in\nsmall steps enables adaptation of thinking and refinement of the actual question\nbeing posed (paraphrased from the original).</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The Solveit approach transforms problematic AI interactions into learning\nexperiences through iterative, step-by-step problem-solving where each stage\nbuilds understanding. By breaking down complex tasks and maintaining control\nover the conversation flow, users can achieve more reliable results with AI\nassistants.</p>\n<p><em>Note: Solveit remains unreleased, but these principles apply to existing AI\ntools.</em></p>\n<p>Do you find that decomposing complex problems into smaller components reveals\ndifferent requirements than initially anticipated?</p>\n"
  },
  {
    "title": "💡 TIL: Engineering Prompts Double as Human Checklists",
    "date": "2025-07-03T00:00:00.000Z",
    "tags": [
      "ai",
      "best-practices",
      "prompt-engineering",
      "system-prompts",
      "code-quality",
      "productivity",
      "til",
      "debugging"
    ],
    "url": "/posts/TIL-prompts-as-human-checklists.html",
    "content": "<p><strong>TL;DR:</strong> Well-crafted AI system prompts, like those in Microsoft&#39;s VSCode\nCopilot Chat extension, serve as excellent process documentation and\nstep-by-step checklists that human developers can follow to improve their own\nworkflows and debugging methodologies.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>I was exploring Microsoft&#39;s recently open-sourced\n<a href=\"https://github.com/microsoft/vscode-copilot-chat/\">VSCode Copilot Chat extension</a>\ncodebase when I noticed something interesting: the prompts that power AI coding\nassistants make excellent checklists for human developers too.</p>\n<h2 id=\"engineering-prompts-as-process-documentation\">Engineering Prompts as Process Documentation</h2>\n<p>Take this\n<a href=\"https://github.com/microsoft/vscode-copilot-chat/blob/main/src/extension/prompts/node/agent/agentInstructions.tsx#L197\">agent instruction prompt</a>\nfor example; it&#39;s essentially a 24-step debugging methodology distilled from\ncountless hours of human engineering experience:</p>\n<ol>\n<li>Initialize Git and explore the repository structure</li>\n<li>Create a reproduction script to confirm the issue</li>\n<li>Execute the script to document the exact error</li>\n<li>Analyse the root cause</li>\n<li>Read relevant code blocks before making changes</li>\n<li>Develop comprehensive test cases</li>\n<li>Stage files in Git before editing</li>\n<li>Apply fixes iteratively...</li>\n</ol>\n<p>And so on. Each step represents a best practice that seasoned developers follow\ninstinctively.</p>\n<p>The\n<a href=\"https://github.com/microsoft/vscode-copilot-chat/blob/40d039d8e08c2d17435a2e65846120c394d0727b/src/extension/xtab/common/promptCrafting.ts#L34\">system prompt template</a>\nis equally instructive. It emphasises context analysis, consistency, and\nunderstanding developer intent before suggesting changes.</p>\n<p>What&#39;s brilliant is that these prompts aren&#39;t just instructions for AI, they&#39;re\ncodified human expertise. When we craft prompts for AI systems, we&#39;re\nessentially documenting our own thought processes and best practices. The better\nthe prompt, the better the human process it represents.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Next time you&#39;re debugging a tricky issue or refactoring complex code, consider\nfollowing the same systematic approach these AI prompts encourage. After all,\ngood prompts are just good processes made explicit.</p>\n"
  },
  {
    "title": "🔨 REWORK",
    "date": "2025-03-28T00:00:00.000Z",
    "tags": [
      "37signals",
      "best-practices",
      "productivity",
      "efficiency",
      "company-culture",
      "remote-work",
      "minimal",
      "business-value"
    ],
    "url": "/posts/rework-the-art-of-working-smarter.html",
    "content": "<p><strong>TL;DR:</strong> Basecamp founders challenge conventional business wisdom in their\nbook &quot;Rework,&quot; advocating for simplicity, constraints, sustainable work hours,\nand focused execution over endless planning, rapid growth, and workaholism -\npresenting practical principles for building profitable, sustainable businesses\nwith minimal resources.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>The traditional approach to business often involves comprehensive planning,\nrapid growth, long hours, and complex processes. But is this truly the most\neffective way to succeed? <a href=\"https://world.hey.com/jason\">Jason Fried</a> and\n<a href=\"https://world.hey.com/david\">David Heinemeier Hansson</a>, founders of\n<a href=\"https://en.wikipedia.org/w/index.php?title=Basecamp_(company)&redirect=no\">Basecamp</a>\n(now <a href=\"https://en.wikipedia.org/wiki/37signals\">37signals</a>), challenge these\nconventional notions in their influential book\n&quot;<a href=\"https://basecamp.com/books/rework\">Rework</a>&quot;. Published in 2010, this manifesto\npresents an alternative philosophy for building successful businesses in the\ndigital age -one that emphasises simplicity, efficiency, and balance. Drawing\nfrom their experience creating profitable web applications with a small team,\nFried and Hansson offer practical insights for entrepreneurs and companies of\nall sizes. Their approach advocates working smarter rather than harder, focusing\non what truly matters, and challenging business orthodoxy at every turn.</p>\n<h2 id=\"foundational-principles-of-rework\">Foundational Principles of &quot;Rework&quot;</h2>\n<h3 id=\"embrace-simplicity-and-constraints\">Embrace Simplicity and Constraints</h3>\n<p>Fried and Hansson consistently emphasise that constraints aren&#39;t limitations but\nadvantages. With limited resources, you&#39;re forced to focus on what&#39;s essential:</p>\n<ul>\n<li><strong>Build half a product, not a half-ar🤬ed product</strong>: It&#39;s better to do fewer\nthings exceptionally well than to attempt everything poorly. Quality trumps\nquantity.</li>\n<li><strong>Embrace constraints</strong>: Limited time, budget, or people can spark creativity\nand force efficiency. They make you focus on doing more with less.</li>\n<li><strong>Underdo your competition</strong>: Instead of adding more features than\ncompetitors, focus on solving core problems elegantly. Simplicity is a\ncompetitive advantage.</li>\n</ul>\n<p>The authors point to specific examples, such as how Basecamp launched without\nbilling functionality (adding it 30 days later) and how the Flip video camera\nsucceeded by deliberately omitting features that competitors deemed essential.</p>\n<h3 id=\"challenge-traditional-business-thinking\">Challenge Traditional Business Thinking</h3>\n<p>&quot;Rework&quot; consistently questions business conventions that many take for granted:</p>\n<ul>\n<li><strong>Planning is guessing</strong>: Detailed long-term business plans are often\nexercises in fiction. Instead, make decisions just in time with the most\ncurrent information available.</li>\n<li><strong>Working long hours is counterproductive</strong>: &quot;Workaholism&quot; leads to burnout\nand mediocre output. Productivity isn&#39;t about hours worked but about focused,\nquality work.</li>\n<li><strong>Growth isn&#39;t always good</strong>: The authors argue against the obsession with\nexpansion, suggesting companies find their &quot;right size&quot; and focus on\nsustainability rather than constant growth.</li>\n<li><strong>Skip the &quot;rock stars&quot;</strong>: Instead of obsessing over hiring &quot;ninjas&quot; or &quot;rock\nstars,&quot; create an environment where ordinary people can do extraordinary work.</li>\n</ul>\n<h3 id=\"focus-on-action-over-discussion\">Focus on Action Over Discussion</h3>\n<p>A central theme in &quot;Rework&quot; is the importance of creating rather than just\ntalking about creating:</p>\n<ul>\n<li><strong>Start making something</strong>: Ideas are worthless without execution. The world\nis filled with people who &quot;had that idea first&quot; but never acted on it.</li>\n<li><strong>Launch now</strong>: Perfection is unattainable; get your product out quickly and\niterate based on real feedback rather than assumptions.</li>\n<li><strong>Meetings are toxic</strong>: They interrupt productivity, waste collective time,\nand often accomplish little. Minimise them ruthlessly.</li>\n</ul>\n<p>The authors illustrate this with their own experience building Basecamp,\nlaunching quickly with core functionality and improving based on actual customer\nfeedback rather than theoretical market research.</p>\n<h3 id=\"build-an-audience-focused-business\">Build an Audience-Focused Business</h3>\n<p>Fried and Hansson outline a customer-centric approach to business development:</p>\n<ul>\n<li><strong>Out-teach your competition</strong>: Share knowledge generously through blogs,\narticles, and tutorials. Teaching establishes authority and builds trust with\npotential customers.</li>\n<li><strong>Build an audience</strong>: Develop a following of people interested in what you\nhave to say. When you launch products, you&#39;ll already have an engaged\naudience.</li>\n<li><strong>Emulate chefs</strong>: Like celebrity chefs who share their recipes freely,\nsharing your expertise doesn&#39;t diminish your business -it enhances it.</li>\n</ul>\n<p>Their company blog, Signal vs. Noise, exemplifies this approach, having built an\naudience of over 100,000 daily readers who became a natural customer base.</p>\n<h3 id=\"create-a-sustainable-work-culture\">Create a Sustainable Work Culture</h3>\n<p>The authors advocate for work environments that prioritise sustainability over\nburnout:</p>\n<ul>\n<li><strong>Send people home at 5</strong>: Reasonable working hours increase per-hour\nproductivity and lead to more creative solutions.</li>\n<li><strong>Avoid policies that treat people like children</strong>: Trust adults to manage\ntheir time and make good decisions.</li>\n<li><strong>Avoid unnecessary formality</strong>: Communicate in a human voice rather than\ncorporate-speak. Sound like yourself, not like a faceless entity.</li>\n</ul>\n<h2 id=\"potential-limitations\">Potential Limitations</h2>\n<p>While &quot;Rework&quot; offers valuable counter-conventional wisdom, some of its\napproaches may not suit all business contexts. The authors&#39; philosophy works\nparticularly well for software and service businesses with low overhead, but\nmanufacturing or capital-intensive industries may require more traditional\nplanning. Additionally, their &quot;do less&quot; approach might not always scale for\nbusinesses with complex regulatory requirements or those serving enterprise\nclients with extensive needs.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>&quot;Rework&quot; offers a refreshing alternative to conventional business wisdom,\nadvocating for a more thoughtful, balanced, and human approach to work. The\nbook&#39;s central message is that success doesn&#39;t require sixty-hour workweeks,\nventure capital, or extensive planning -it requires focus on what matters,\nelimination of what doesn&#39;t, and dedication to quality execution.<br>By challenging assumptions about growth, working hours, planning, and hiring,\nFried and Hansson present a blueprint for building businesses that are not only\nprofitable but also sustainable and enjoyable to run. Their philosophy can be\ndistilled to a few key principles: embrace constraints, focus on quality over\nquantity, prioritise action over planning, and build businesses that respect\nboth customers and employees.<br>Whether you&#39;re running a startup, managing a team, or simply looking to work\nmore effectively, &quot;Rework&quot; provides valuable insights for doing more with less\nand building something that lasts. It&#39;s not about working more -it&#39;s about\nworking smarter.</p>\n"
  },
  {
    "title": "🌐 Remote: Office Not Required",
    "date": "2025-03-26T00:00:00.000Z",
    "tags": [
      "37signals",
      "remote-work",
      "advantage",
      "company-culture",
      "productivity",
      "best-practices",
      "decision-making",
      "onboarding"
    ],
    "url": "/posts/remote-office-not-required.html",
    "content": "<p><strong>TL;DR:</strong> Basecamp founders present a comprehensive guide to remote work,\narguing that distributed teams offer increased productivity, access to global\ntalent, and better work-life balance whilst outlining practical strategies for\neffective communication, maintaining culture, and overcoming common objections\nto remote collaboration.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>In the rapidly evolving landscape of modern business, few changes have been as\ntransformative as the shift toward remote work. In their insightful book\n&quot;<a href=\"https://basecamp.com/books#remote\">Remote: Office Not Required</a>&quot;,\n<a href=\"https://world.hey.com/jason\">Jason Fried</a> and\n<a href=\"https://world.hey.com/david\">David Heinemeier Hansson</a> of\n<a href=\"https://en.wikipedia.org/w/index.php?title=Basecamp_(company)&redirect=no\">Basecamp</a>\n(now <a href=\"https://en.wikipedia.org/wiki/37signals\">37signals</a>) present a compelling\ncase for why working remotely isn&#39;t just a viable option - it&#39;s often superior\nto the traditional office-based model.<br>Written in 2013, well before the pandemic forced companies to adopt remote work\nen masse, the book now seems prophetic. Fried and Hansson argue that remote work\nis not a fleeting trend but an inevitable evolution in how businesses operate.\nThe authors, who built their successful software company with team members\nscattered across the globe, offer practical advice for implementing and thriving\nin a remote work environment.<br>Let&#39;s explore the core principles of &quot;Remote&quot; that can help businesses and\nworkers navigate this new reality.</p>\n<h2 id=\"why-remote-work-makes-sense\">Why Remote Work Makes Sense</h2>\n<h3 id=\"the-office-paradox\">The Office Paradox</h3>\n<p>One of the book&#39;s most compelling arguments is that traditional offices often\nhinder productivity rather than enhance it. Fried and Hansson point out that\nwhen people need to get serious work done, they rarely cite the office as their\npreferred location. Instead, they choose early mornings, late evenings, or\nweekends - times when interruptions are minimal.<br>Offices have become &quot;interruption factories&quot; where meaningful work is chopped\ninto small, ineffective chunks. Meetings, impromptu desk visits, and constant\nnoise create an environment where deep, focused work becomes nearly impossible.\nRemote work, by contrast, allows people to create their own distraction-free\nenvironments.</p>\n<h3 id=\"the-end-of-commuting\">The End of Commuting</h3>\n<p>Another significant advantage of remote work is eliminating the commute. Beyond\nthe obvious time savings, research shows that long commutes correlate with\nincreased obesity, stress, neck and back pain, and even higher divorce rates.\nThe authors calculate that an average commute consumes 300-400 hours per year -\ntime that could be redirected toward productive work or personal well-being.</p>\n<h3 id=\"access-to-global-talent\">Access to Global Talent</h3>\n<p>Perhaps most importantly, remote work dramatically expands the talent pool.\nInstead of limiting hiring to a specific geographical area, companies can\nrecruit from anywhere in the world. This not only increases the chances of\nfinding exceptional talent but also naturally leads to a more diverse workforce\nwith varied perspectives.</p>\n<h2 id=\"making-remote-work-work\">Making Remote Work Work</h2>\n<h3 id=\"communication-the-key-to-success\">Communication: The Key to Success</h3>\n<p>Effective remote work hinges on communication. The book advocates for a blend of\nsynchronous and asynchronous communication methods:</p>\n<ol>\n<li><strong>Overlap Time</strong>: Ensure team members have at least 4 hours of overlap in\ntheir workdays to allow for real-time collaboration when needed.</li>\n<li><strong>Screen Sharing</strong>: Use tools like WebEx, GoToMeeting, or Join.me to\ncollaborate visually, making it feel more like sitting side-by-side.</li>\n<li><strong>Transparent Documentation</strong>: Make information accessible to everyone\nregardless of time zone, eliminating bottlenecks. Basecamp, their project\nmanagement tool, was designed specifically with this in mind.</li>\n<li><strong>Virtual Water Cooler</strong>: Create spaces for casual conversation to maintain\ncompany culture and social connections. The authors used Campfire, their\nweb-based chat service, for this purpose. While Campfire was discontinued as\na standalone product, it was recently relaunched in 2024 as part of their\nONCE line - allowing users to purchase and self-host the software on their\nown servers rather than subscribing to a SaaS model.</li>\n</ol>\n<h3 id=\"navigating-legal-and-financial-considerations\">Navigating Legal and Financial Considerations</h3>\n<p>The book doesn&#39;t shy away from the practical challenges of remote work. In the\nchapter &quot;Taxes, accounting, laws, oh my!&quot; the authors tackle the nuts and bolts\nof remote employment:</p>\n<ul>\n<li><strong>Domestic remote work</strong> is relatively straightforward from a legal\nstandpoint, with few complications beyond potential state tax implications if\nemployees work across state lines.</li>\n<li><strong>International remote work</strong> presents more challenges. The authors outline\ntwo main approaches: establishing a local office (expensive but comprehensive)\nor hiring people as contractors (simpler but with limitations on benefits and\nemployment protections).</li>\n<li><strong>For remote workers</strong>, they recommend setting up a personal company and\nbilling as a contractor if working for an international company, though they\nacknowledge this isn&#39;t a perfect solution.</li>\n</ul>\n<p>The authors are refreshingly honest here, acknowledging that running with a\nless-than-perfect legal setup is common practice - though they recommend\nconsulting professionals for complex situations.</p>\n<h3 id=\"overcoming-common-objections\">Overcoming Common Objections</h3>\n<p>Fried and Hansson systematically address the objections typically raised against\nremote work:</p>\n<ul>\n<li><strong>&quot;How do I know people are working?&quot;</strong> If you can&#39;t trust employees to work\nremotely, the issue is hiring, not location.</li>\n<li><strong>&quot;What about security?&quot;</strong> With proper protocols, remote work can be just as\nsecure as office work.</li>\n<li><strong>&quot;We need face-to-face meetings.&quot;</strong> Most meetings can be conducted\neffectively online, and occasional in-person gatherings can satisfy the need\nfor face time.</li>\n<li><strong>&quot;We need to maintain our culture.&quot;</strong> Culture stems from values and actions,\nnot physical proximity.</li>\n</ul>\n<h3 id=\"avoiding-remote-work-pitfalls\">Avoiding Remote Work Pitfalls</h3>\n<p>The book doesn&#39;t gloss over remote work&#39;s challenges:</p>\n<ol>\n<li><strong>Isolation</strong>: Combat loneliness by encouraging employees to work from\nco-working spaces or cafés occasionally.</li>\n<li><strong>Overwork</strong>: Without clear boundaries, remote workers may struggle to\ndisconnect. Managers should focus on results rather than hours worked and\nlook out for signs of burnout.</li>\n<li><strong>Communication Barriers</strong>: When face-to-face interaction is limited,\nmisunderstandings can occur. Clear, thoughtful communication becomes even\nmore crucial.</li>\n</ol>\n<h2 id=\"building-and-managing-a-remote-team\">Building and Managing a Remote Team</h2>\n<h3 id=\"hiring-for-remote-work\">Hiring for Remote Work</h3>\n<p>The authors emphasise that great remote workers possess certain qualities:</p>\n<ul>\n<li><strong>Self-motivation</strong>: They can stay productive without direct supervision.</li>\n<li><strong>Strong writing skills</strong>: Since much of remote communication is written,\nclear writing is essential.</li>\n<li><strong>Results-oriented mindset</strong>: They focus on output rather than hours at a\ndesk.</li>\n</ul>\n<h3 id=\"creating-trust-and-accountability\">Creating Trust and Accountability</h3>\n<p>Rather than micromanaging, successful remote teams are built on trust. The book\nrecommends:</p>\n<ul>\n<li><strong>Focus on outputs</strong>: Judge work by what&#39;s accomplished, not when or how it&#39;s\ndone.</li>\n<li><strong>Regular check-ins</strong>: Brief one-on-ones help maintain connection without\nbecoming burdensome.</li>\n<li><strong>Eliminate roadblocks</strong>: Ensure remote workers have the authority and access\nthey need to be effective.</li>\n</ul>\n<h3 id=\"the-remote-toolbox\">The Remote Toolbox</h3>\n<p>The authors provide a practical &quot;Remote Toolbox&quot; with specific recommendations:</p>\n<ul>\n<li><strong>Basecamp</strong>: Their own project management tool for organising tasks,\ndiscussions, and files in one central location.</li>\n<li><strong>Video conferencing tools</strong>: Google Hangouts (now Google Meet) for group\nvideo calls with up to 10 people.</li>\n<li><strong>Screen sharing</strong>: WebEx, GoToMeeting, and Join.me for collaboration and\ndemonstrations.</li>\n<li><strong>File sharing</strong>: Dropbox for keeping files synchronised across multiple\ndevices and locations.</li>\n<li><strong>Collaborative documents</strong>: Google Docs for real-time collaboration on text\ndocuments and spreadsheets.</li>\n<li><strong>Co-working directories</strong>: Resources like Regus, LiquidSpace, Desktime, and\nthe Coworking Wiki to find workspaces while travelling or to escape home\noffice isolation.</li>\n</ul>\n<p>Many of these tools have evolved since the book&#39;s publication, but the core\nfunctions they serve remain essential to remote work.</p>\n<h3 id=\"the-importance-of-meetups\">The Importance of Meetups</h3>\n<p>Despite advocating for remote work, the authors stress the value of occasional\nin-person gatherings. At their company, they met at least twice yearly for 4-5\ndays[^1]. These meetups strengthen personal bonds, allow for intensive\ncollaboration, and reinforce company culture.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>&quot;Remote: Office Not Required&quot; provides a comprehensive blueprint for\nimplementing successful remote work practices. The authors convincingly argue\nthat remote work offers numerous advantages: increased productivity, access to\nglobal talent, better work-life balance, and reduced overhead costs.<br>What makes this book particularly valuable is its grounding in real-world\nexperience. Fried and Hansson have built their business on these principles and\nhave navigated the challenges they describe.<br>As we continue to redefine what work means in the 21st century, the insights\nfrom &quot;Remote&quot; remain highly relevant. The authors envision a future where work\nis judged by results rather than location, where talent knows no geographical\nboundaries, and where both companies and employees enjoy greater freedom and\nflexibility.<br>For businesses looking to thrive in this new landscape, &quot;Remote&quot; offers not just\nphilosophy but practical strategies for turning the challenges of distributed\nwork into competitive advantages. The future of work is indeed remote - and this\nbook provides an excellent road map for that journey.</p>\n<hr>\n<p>[^1]: Reasonable adjustments must be considered for those who are not able to\n    travel far due to health, family or other reasons beyond their control. It\n    is possible to build and maintain a strong culture that does not necessitate\n    travelling, or at least not travelling often or far, if circumstances don&#39;t\n    allow.</p>\n"
  },
  {
    "title": "😌 It Doesn't Have to Be Crazy at Work",
    "date": "2025-03-23T00:00:00.000Z",
    "tags": [
      "37signals",
      "advantage",
      "best-practices",
      "decision-making",
      "business-value",
      "slow-down",
      "onboarding",
      "remote-work",
      "productivity",
      "company-culture"
    ],
    "url": "/posts/it-doesnt-have-to-be-crazy-at-work-37-signals.html",
    "content": "<p><strong>TL;DR:</strong> Basecamp founders reject the &quot;crazy busy&quot; workplace culture,\nadvocating instead for a &quot;calm company&quot; approach that emphasises reasonable\n40-hour workweeks, focused attention, asynchronous communication, and flexible\nproject scope - proving that sustainable work practices can yield successful\nbusinesses without sacrificing employee wellbeing.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>In today&#39;s hyperactive business environment, &quot;crazy busy&quot; has become a badge of\nhonour. Endless workweeks, constant interruptions, and the expectation of\ninstant responses have created workplaces where stress is the norm and burnout\nis inevitable. But does it have to be this way?\n<a href=\"https://world.hey.com/jason\">Jason Fried</a> and\n<a href=\"https://world.hey.com/david\">David Heinemeier Hansson</a>, the founders of\n<a href=\"https://en.wikipedia.org/w/index.php?title=Basecamp_(company)&redirect=no\">Basecamp</a>\n(renamed to <a href=\"https://en.wikipedia.org/wiki/37signals\">37signals</a> since 2014),\nargue emphatically that it doesn&#39;t. In their book\n&quot;<a href=\"https://basecamp.com/books/calm\">It Doesn&#39;t Have to Be Crazy at Work</a>&quot; they\npresent a compelling case for a calmer, more sustainable approach to work -one\nwhere companies can still be successful without sacrificing the well-being of\ntheir employees.<br>The authors, who have built a profitable business with minimal stress and\nreasonable working hours, dismiss the idea that growth-at-all-costs and\naround-the-clock work schedules are necessary for success. Instead, they\nadvocate for what they call a &quot;calm company&quot; -an organisation that values\nsustainable workloads, reasonable expectations, sufficient rest, and focused\nproductivity. Let&#39;s dive into the key principles that can help transform a\nfrantic workplace into a calm and productive environment.</p>\n<h2 id=\"the-calm-company-philosophy\">The Calm Company Philosophy</h2>\n<h3 id=\"rethinking-time-and-attention\">Rethinking Time and Attention</h3>\n<p>One of the core insights from the book is that modern workplaces have become\n&quot;interruption factories&quot; where meaningful work is nearly impossible. Offices\nchop the workday into tiny fragments -fifteen minutes here, ten minutes there-\nwith meetings, calls, and constant distractions preventing sustained focus.<br>The authors argue that 8-hour workdays and 40-hour workweeks are plenty of time\nto accomplish great work, provided that time is actually protected. Instead of\nmeasuring commitment by hours spent at a desk, a calm company measures results\nand respects boundaries. Basecamp&#39;s philosophy is straightforward: &quot;Work 40\nhours a week, then stop. No all-nighters, no weekends&quot;.<br>To protect time, the book advocates for asynchronous communication whenever\npossible. Not everything requires an immediate response. By promoting a culture\nof eventual response rather than instant reaction, companies give employees the\nspace for deep, focused work. This might mean designating &quot;library rules&quot; in the\noffice -quiet, focused concentration- and setting clear boundaries for when\nreal-time collaboration is truly necessary.</p>\n<h3 id=\"eliminate-excessive-mms-meetings-and-managers\">Eliminate Excessive M&amp;Ms: Meetings and Managers</h3>\n<p>Meetings and micromanagement are two primary culprits behind workplace chaos.\nThe authors are particularly critical of the modern meeting culture, noting that\n&quot;a one-hour meeting with ten people isn&#39;t a one-hour meeting -it&#39;s a ten-hour\nmeeting&quot;. Before calling a meeting, they suggest asking whether it&#39;s truly worth\npulling multiple people away from their focused work.<br>Similarly, the book challenges managers to stop &quot;managing the chairs&quot;\n(monitoring when people arrive and leave) and instead focus on managing the work\nitself. This means setting clear expectations, providing necessary resources,\nremoving obstacles, and then trusting people to execute without constant\nsupervision.<br>At Basecamp, they&#39;ve institutionalised practices like &quot;office hours&quot; for\nexperts, where rather than being constantly available for interruption, they\ndesignate specific times when they&#39;re available for questions. They&#39;ve also\nmoved away from real-time chat for important discussions, recognising that this\nmedium often creates an unhealthy expectation of immediate response.</p>\n<h3 id=\"reasonable-expectations-and-focused-scope\">Reasonable Expectations and Focused Scope</h3>\n<p>Perhaps the most radical departure from conventional business thinking is the\nauthors&#39; approach to goals and expectations. They proudly declare: &quot;We don&#39;t do\ngoals at Basecamp&quot;. Instead of chasing arbitrary targets, they focus on doing\nexcellent work consistently and sustainably.<br>The book introduces the concept of &quot;dreadlines&quot; versus deadlines. A dreadline\nappears when a deadline is paired with an ever-expanding scope. To combat this,\nBasecamp keeps deadlines fixed but makes scope flexible. Projects can only get\nsmaller over time, not larger, ensuring teams can deliver quality work without\nburning out.<br>This means being deliberate about what not to do. As the authors put it: &quot;Having\nless to do isn&#39;t a problem, it&#39;s an advantage&quot;. They suggest developing the\nskill of &quot;narrowing as you go&quot; -starting projects with exploration, then\ngradually focusing in on what&#39;s truly important as you approach the deadline.<br>Basecamp also embraces the &quot;disagree and commit&quot; approach to decision-making.\nRather than requiring consensus, which can lead to endless debate, someone makes\nthe final call after everyone has been heard -and then the whole team commits to\nmaking it work, even if some initially disagreed.</p>\n<h3 id=\"building-a-healthy-remote-work-culture\">Building a Healthy Remote Work Culture</h3>\n<p>Remote work features prominently in Basecamp&#39;s approach to building a calm\ncompany. By removing the expectation that everyone must be in the same physical\nspace, they&#39;ve created more flexibility while maintaining productivity.<br>However, they emphasise that remote work requires intentionality. Teams need\nsufficient overlap in working hours, clear communication practices, and strong\nwriting skills. In fact, the authors consider good writing essential for remote\nteams, as it eliminates ambiguity and creates a clear record of decisions and\nrationales.<br>The authors also address the concern that remote work might lead to isolation or\ndisconnection. They recommend regular in-person meetups[^1] and maintaining a\nstrong company culture based on shared values and respect, not forced\nsocialisation or perks designed to keep people at the office longer.</p>\n<h3 id=\"hiring-and-benefits-that-support-life-outside-work\">Hiring and Benefits That Support Life Outside Work</h3>\n<p>Basecamp&#39;s approach to hiring focuses on finding talented people who value calm\nproductivity over chaotic hustle. Their compensation philosophy is refreshingly\nstraightforward: equal pay for equal work, regardless of location, with no\ncomplex negotiation processes.<br>Their benefits are specifically designed to encourage life beyond work. Rather\nthan offering free meals to keep employees in the office longer, they provide\nbenefits that help people disconnect -like paid sabbaticals, summer hours\n(32-hour workweeks from May through August), and even covering the cost of\nemployees&#39; vacations. This reinforces their belief that the best workers are\nwell-rested ones with rich lives outside the office.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>&quot;It Doesn&#39;t Have to Be Crazy at Work&quot; presents a refreshing alternative to the\nburnout culture that pervades much of today&#39;s business world. The calm company\nmodel isn&#39;t about doing less or lowering standards -it&#39;s about working smarter,\nfocusing on what truly matters, and creating sustainable conditions where people\ncan do their best work without sacrificing their health and happiness.</p>\n<p>The key takeaways from the book include:</p>\n<ol>\n<li>Protect people&#39;s time and attention by eliminating unnecessary interruptions</li>\n<li>Stick to reasonable work hours (40 hours per week is plenty)</li>\n<li>Replace constant meetings with more thoughtful, asynchronous communication</li>\n<li>Focus on the quality of work rather than hours logged</li>\n<li>Keep deadlines fixed but be flexible about scope</li>\n<li>Build a culture of trust where remote work can thrive</li>\n<li>Be intentional about what you say no to</li>\n</ol>\n<p>As the authors suggest, &quot;calm is contagious&quot; -and so is crazy. By choosing calm,\ncompanies can create environments where employees thrive, creativity flourishes,\nand sustainable success becomes possible. The choice, as they say, is yours.</p>\n<hr>\n<p>[^1]: Reasonable adjustments must be considered for those who are not able to\n    travel far due to health, family or other reasons beyond their control. It\n    is possible to build and maintain a strong culture that does not necessitate\n    travelling, or at least not travelling often or far, if circumstances don&#39;t\n    allow.</p>\n"
  },
  {
    "title": "⏪ Making Data Transformations Reversible with fasttransform",
    "date": "2025-03-22T00:00:00.000Z",
    "tags": [
      "machine-learning",
      "data-processing",
      "fast-ai",
      "python",
      "data-science",
      "optimisation",
      "best-practices",
      "interpretability"
    ],
    "url": "/posts/fasttransform-for-reversible-data-transformations.html",
    "content": "<p><strong>TL;DR:</strong> Fast.ai&#39;s fasttransform library makes machine learning data pipelines\nreversible by pairing each transformation with its inverse, enabling\nvisualisation of transformed data for debugging and utilising multiple dispatch\nto handle different data types intelligently - crucial for understanding model\nbehaviour and identifying spurious correlations.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Machine learning practitioners face a common problem: after applying multiple\ntransformations to prepare data for training, it becomes difficult to visualise\nwhat the model actually sees. This visualisation gap makes debugging challenging\nand often leads to missing critical insights about model behaviour.<br>For example, consider a model built to distinguish wolves from huskies that\nperforms poorly on certain images. Without the ability to easily inspect how\ntransformations affect the input data, one might miss that the model is actually\ndetecting snow (common in wolf photos) rather than the animals themselves.<br>Fast.ai&#39;s solution to this problem is\n<a href=\"https://github.com/AnswerDotAI/fasttransform\">fasttransform</a>[^1], a library\nthat ensures any transformation applied to data can be easily reversed. Let&#39;s\nexplore how it works and why it matters.</p>\n<h2 id=\"reversible-pipelines-made-simple\">Reversible Pipelines Made Simple</h2>\n<h3 id=\"the-problem-with-one-way-transforms\">The Problem with One-Way Transforms</h3>\n<p>Traditional data transformation pipelines in libraries like PyTorch are one-way\nstreets. Consider this simple example of normalising an image:</p>\n<pre><code class=\"language-python\">from torchvision import transforms as T\ntransforms_pt = T.Compose([\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize(*imagenet_stats)\n])\n\nimg = Image.open(&quot;husky.jpeg&quot;)\nimg_transformed = transforms_pt(img)\n</code></pre>\n<p>Attempting to visualise <code>img_transformed</code> results in a mess of pixel values\noutside the displayable range. To see what the model sees, one needs to manually\nwrite an inverse transform function:</p>\n<pre><code class=\"language-python\">def decode_pt(tensor, mean, std):\n    out = tensor.clone()\n    for t, m, s in zip(out, mean, std): t.mul_(s).add_(m)\n    out = out.mul(255).clamp(0, 255).byte()\n    return out\n</code></pre>\n<p>This is tedious and error-prone, especially as your transformation pipeline\ngrows more complex.</p>\n<h3 id=\"an-elegant-solution\">An Elegant Solution</h3>\n<p>fasttransform takes a fundamentally different approach by pairing each\ntransformation with its inverse. Here&#39;s the same pipeline using fasttransform:</p>\n<pre><code class=\"language-python\">from fastai.vision.all import *\n\ntransforms_ft = Pipeline([\n   PILImage.create,\n   Resize(256, method=&quot;squish&quot;),\n   Resize(224, method=&quot;crop&quot;),\n   ToTensor(),\n   IntToFloatTensor(),\n   Normalize.from_stats(*imagenet_stats)\n])\n\n# Transform our image\nfpath = Path(&quot;./huskies_vs_wolves/train/husky/husky_0.jpeg&quot;)\nimg_transformed = transforms_ft(fpath)\n# To reverse the transformations:\nimg_decoded = transforms_ft.decode(img_transformed)\n</code></pre>\n<p>The magic lies in how each transform defines both forward and reverse\noperations:</p>\n<pre><code class=\"language-python\">class Normalize(Transform):\n    def __init__(self, mean=None, std=None):\n        self.mean = mean\n        self.std = std\n        \n    def encodes(self, x): return (x-self.mean) / self.std  # forward transform\n    def decodes(self, x): return x*self.std + self.mean    # inverse transform\n</code></pre>\n<p>By defining both <code>encodes</code> and <code>decodes</code> methods, fasttransform automatically\nknows how to reverse your transformations. This is particularly valuable when\nworking with fast.ai v2, where this kind of visualisation capability is built\ndirectly into core functions like <code>show_batch</code> and <code>show_results</code>.</p>\n<h3 id=\"multiple-dispatch-the-secret-sauce\">Multiple Dispatch: The Secret Sauce</h3>\n<p>Another powerful feature of fasttransform is how it handles different types of\ndata. Using a concept called\n<a href=\"https://www.youtube.com/watch?v=kc9HwsxE1OY\">multiple dispatch</a>[^2],\ntransformations can apply differently based on the type of data they receive.</p>\n<p>This becomes particularly valuable when dealing with images and their labels,\nallowing a single pipeline to handle both:</p>\n<pre><code class=\"language-python\"># Function that loads both image and its label\ndef load_img_and_label(fp): return PILImage.create(fp), parent_label(fp)\n\ntransforms_ft = Pipeline([\n   load_img_and_label,  # Loads both image and label as a tuple\n   Resize(256, method=&quot;squish&quot;),\n   Resize(224, method=&quot;crop&quot;),\n   ToTensor(),\n   IntToFloatTensor(),\n   Normalize.from_stats(*imagenet_stats)\n])\n</code></pre>\n<p>The pipeline intelligently applies each transform only to the appropriate data\ntypes, eliminating the need for separate transformation pipelines.</p>\n<h3 id=\"connections-to-julias-multiple-dispatch\">Connections to Julia&#39;s Multiple Dispatch</h3>\n<p>Interestingly, the concept of multiple dispatch that fasttransform leverages is\na core feature of the Julia programming language. In Julia, which method of a\nfunction gets called depends on the types of all arguments, not just the first\none (as in traditional object-oriented programming).<br>As explained in Julia&#39;s documentation: &quot;<em>Using all of a function&#39;s arguments to\nchoose which method should be invoked, rather than just the first, is known as\nmultiple dispatch. Multiple dispatch is particularly useful for mathematical\ncode, where it makes little sense to artificially deem the operations to\n&#39;belong&#39; to one argument more than any of the others</em>&quot;.<br>The connection to Julia is particularly illuminating, as it demonstrates how\nconcepts from one language can inspire powerful design patterns in another. Just\nas Julia&#39;s multiple dispatch enables elegant mathematical code, fasttransform&#39;s\nimplementation of this concept allows for cleaner, more intuitive data pipelines\nin Python.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>fasttransform represents a significant step forward in making machine learning\nworkflows more intuitive and debugging more accessible. By making\ntransformations reversible through paired encode/decode methods and leveraging\nmultiple dispatch to handle different data types intelligently, it solves two\nfundamental problems in data processing pipelines: the inability to easily\nreverse transformations to inspect data, and the need for separate\ntransformation pipelines for different types of data.<br>The ability to easily visualise transformed data isn&#39;t just convenient -it&#39;s\nessential for understanding model behaviour and catching issues like the\nwolf/husky example, where models learn spurious correlations rather than\nintended features.<br>As machine learning systems grow more complex, tools like fasttransform that\nimprove transparency and the ability to debug become increasingly valuable.\nWhether working with images, text, time series, or other data types, being able\nto see what a model sees provides critical insights that might otherwise be\nmissed.<br>Returning to our wolf/husky example, the ability to easily visualise transformed\ndata allows researchers to immediately identify that their model is learning to\ndetect snow backgrounds rather than animal features -a crucial insight for\nbuilding more robust models.<br>Those interested in trying fasttransform can install it with\n<code>pip install fasttransform</code> and check out the\n<a href=\"https://github.com/AnswerDotAI/fasttransform\">official fasttransform documentation</a>\nfor more examples and detailed API references. The library offers these\ncapabilities with minimal performance overhead, as the paired transformation\napproach adds negligible computational cost while providing significant benefits\nfor debugging and understanding model behaviour.</p>\n<hr>\n<p>[^1]: Rens Dimmendaal, Hamel Husain, &amp; Jeremy Howard.\n    &quot;<a href=\"https://www.fast.ai/posts/2025-02-20-fasttransform.html\">fasttransform: Reversible Pipelines Made Simple</a>&quot;\n    fast.ai blog, February 20, 2025.</p>\n<p>[^2]: &quot;<a href=\"https://docs.julialang.org/en/v1/manual/methods/\">Methods · The Julia Language</a>&quot;\n    Julia Documentation, docs.julialang.org.</p>\n"
  },
  {
    "title": "💡 TIL: A Reactive Python Notebook That Might Replace Jupyter",
    "date": "2025-03-22T00:00:00.000Z",
    "tags": [
      "til",
      "data-science",
      "python",
      "best-practices",
      "reproducibility",
      "literate-programming",
      "jupyter-alternative",
      "reactivity"
    ],
    "url": "/posts/git-friendly-literate-programming-with-marimo.html",
    "content": "<p><strong>TL;DR:</strong> Marimo offers a reactive Python notebook alternative to Jupyter that\nsolves hidden state problems by storing notebooks as pure Python files with\ndeterministic execution based on variable dependencies, making them\ngit-friendly, reproducible, and deployable whilst providing modern features like\nVim keybindings and interactive elements.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>As a long-time Vim/Neovim and IPython user, I&#39;m quite particular about my\ndevelopment environment. So when I say a notebook platform caught my attention\nenough to consider switching, that&#39;s significant. Recently, I stumbled upon\n<a href=\"https://marimo.io/\">Marimo</a>, and it might just be the Jupyter alternative I&#39;ve\nbeen searching for.</p>\n<h2 id=\"what-is-marimo\">What is Marimo?</h2>\n<p>Marimo is a reactive Python notebook environment that solves many long-standing\nissues with traditional notebooks. Unlike Jupyter, which stores notebooks as\nJSON with embedded code and outputs, Marimo notebooks are pure Python files that\nare:</p>\n<ul>\n<li><strong>Reactive</strong>: Run a cell, and Marimo automatically runs dependent cells or\nmarks them as stale</li>\n<li><strong>Consistent</strong>: No hidden state problems that plague traditional notebooks</li>\n<li><strong>Executable</strong>: Can run as standard Python scripts from the command line</li>\n<li><strong>Git-friendly</strong>: Since they&#39;re just <code>.py</code> files, they work seamlessly with\nversion control</li>\n<li><strong>Deployable</strong>: Easily share as interactive web apps or slides</li>\n</ul>\n<h2 id=\"why-this-matters-for-literate-programming\">Why This Matters for Literate Programming</h2>\n<p>Literate programming -the approach of writing code as a narrative explanation\ninterleaved with executable components- is incredibly powerful for data science,\nML, and AI work. It helps create self-documenting, reproducible research and\napplications.<br>The problem with Jupyter has always been that while it looks like literate\nprogramming, its execution model (arbitrary cell execution order) and hidden\nstate make it fundamentally unreliable. Marimo solves this by ensuring\ndeterministic execution based on variable dependencies rather than cell\nposition.</p>\n<h2 id=\"key-features-that-won-me-over\">Key Features That Won Me Over</h2>\n<ol>\n<li><strong>Vim keybindings</strong>: As a Neovim user, this is non-negotiable</li>\n<li><strong>Modern editor features</strong>: GitHub Copilot integration, AI completion, and\nvariable explorer</li>\n<li><strong>Reactive runtime</strong>: No more &quot;did I run all the cells in the right order?&quot;\nproblems</li>\n<li><strong>Interactive elements</strong>: Sliders, tables, and plots that automatically\nupdate dependent cells</li>\n<li><strong>SQL integration</strong>: Write SQL against dataframes, databases, or other\nsources right in your notebook</li>\n<li><strong>Package management</strong>: Built-in support for dependency tracking and isolated\nenvironments</li>\n<li><strong>Pure Python storage</strong>: No more JSON files with embedded outputs making git\ndiffs unreadable</li>\n</ol>\n<h2 id=\"comparisons-with-other-literate-programming-tools\">Comparisons with Other Literate Programming Tools</h2>\n<h3 id=\"plutojl-julia\">Pluto.jl (Julia)</h3>\n<p>Pluto.jl pioneered the reactive notebook concept that Marimo implements. Both\nshare:</p>\n<ul>\n<li>Automatic reactivity based on variable dependencies</li>\n<li>Deterministic execution order</li>\n<li>Interactive UI elements</li>\n</ul>\n<p><strong>Differences</strong>:</p>\n<ul>\n<li>Pluto is Julia-specific; Marimo is Python-specific</li>\n<li>Marimo stores notebooks as standard <code>.py</code> files; Pluto uses a custom format</li>\n<li>Marimo has more built-in integrations with Python data science libraries</li>\n<li>Pluto has tighter integration with Julia&#39;s capabilities</li>\n</ul>\n<h3 id=\"livebook-elixir\">Livebook (Elixir)</h3>\n<p>Livebook brings reactive notebooks to the Elixir ecosystem, with:</p>\n<ul>\n<li>Smart cells for common tasks</li>\n<li>Built-in deployment capabilities</li>\n<li>Collaborative editing</li>\n</ul>\n<p><strong>Differences</strong>:</p>\n<ul>\n<li>Livebook embraces Elixir&#39;s concurrency model; Marimo follows Python&#39;s\nexecution model</li>\n<li>Marimo&#39;s Python foundation makes it more accessible for data science work</li>\n<li>Livebook has more built-in tools for building distributed systems</li>\n</ul>\n<h2 id=\"pros-and-cons\">Pros and Cons</h2>\n<h3 id=\"pros\">Pros</h3>\n<ul>\n<li><strong>Reproducibility</strong>: Deterministic execution eliminates the &quot;run cells in\nwrong order&quot; problem</li>\n<li><strong>Git-friendly</strong>: Pure Python files make version control and collaboration\nmuch easier</li>\n<li><strong>No hidden state</strong>: Deleted cell variables are removed from memory</li>\n<li><strong>Deployability</strong>: From notebook to web app with minimal effort</li>\n<li><strong>Testability</strong>: Run standard test suites against your notebooks</li>\n<li><strong>Modern IDE features</strong>: Seems like they&#39;ve thought of everything</li>\n</ul>\n<h3 id=\"cons\">Cons</h3>\n<ul>\n<li><strong>Learning curve</strong>: The reactive model requires a shift in thinking if you&#39;re\nused to Jupyter</li>\n<li><strong>Ecosystem maturity</strong>: Newer than Jupyter, so fewer third-party extensions</li>\n<li><strong>Performance considerations</strong>: Automatic reactivity could cause issues with\nexpensive computations (though there are options to mitigate this[^1])</li>\n<li><strong>Language limitation</strong>: Python-only, unlike Jupyter which supports multiple\nkernels</li>\n</ul>\n<h2 id=\"getting-started\">Getting Started</h2>\n<p>Installation is straightforward:</p>\n<pre><code class=\"language-bash\">uv pip install marimo\n# or with recommended extras\nuv pip install marimo[recommended]\n\n# Try the tutorial\nmarimo tutorial intro\n</code></pre>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>As someone deeply invested in both Vim/Neovim and the Python data ecosystem,\nMarimo strikes an impressive balance. It brings the benefits of reactive\nprogramming to Python notebooks while maintaining the flexibility and\nfamiliarity that Python users expect.<br>What truly sets Marimo apart is how it addresses the fundamental issues of\nreproducibility and hidden state that have plagued notebooks for years. By\ntreating notebooks as actual programs with deterministic execution, it enables\nliterate programming in a way that Jupyter always promised but never fully\ndelivered.<br>Is it perfect? No. But it&#39;s the most compelling alternative I&#39;ve seen so far,\nand I&#39;m seriously considering making the switch for my daily work.</p>\n<hr>\n<p>[^1]: Marimo provides a &quot;lazy&quot; configuration option where cells that would be\n    automatically re-executed are instead marked as &quot;stale&quot;, allowing users to\n    manually control when expensive computations run. Users can also implement\n    caching strategies using Marimo&#39;s built-in caching functionality,\n    compartmentalise heavy computations into separate cells to control their\n    execution flow, or use the @mo.cell decorator with runtime configurations to\n    customise how specific cells behave when dependencies change.</p>\n"
  },
  {
    "title": "🏠 Why Companies and Individuals Are Moving Back from the Cloud",
    "date": "2025-03-20T00:00:00.000Z",
    "tags": [
      "cloud",
      "on-prem",
      "performance",
      "security",
      "mlops",
      "deployment",
      "best-practices",
      "data-science"
    ],
    "url": "/posts/cloud-repatriation-trends-implications.html",
    "content": "<p><strong>TL;DR:</strong> Cloud repatriation is gaining momentum with 86% of CIOs planning to\nmove some workloads back on-premises, driven by unexpected costs, performance\nneeds, security concerns, and desire for greater control - though most\norganisations are adopting hybrid approaches rather than abandoning cloud\nentirely, strategically placing workloads where they function most efficiently.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>The last decade has witnessed the meteoric rise of cloud computing, with\norganisations large and small migrating their data, applications, and\ninfrastructure to public cloud environments. The promises were compelling:\nreduced capital expenditure, unlimited scalability, enhanced flexibility, and\naccess to cutting-edge technologies without the overhead of managing physical\ninfrastructure. However, a notable countertrend has emerged in recent years\n-cloud repatriation. This phenomenon, sometimes referred to as &quot;reverse cloud\nmigration,&quot; involves moving workloads, applications, and data back from public\ncloud environments to on-premises data centres, private clouds, or hybrid setups\n(<a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International, 2023</a>).\nI&#39;ve previously explored this topic in my article [The On-Prem Comeback (aka\nCloud Repatriation)]({{ site.baseurl }}{% link\n_posts/2024-11-14-cloud-repatriation.md %}), where I introduced the basic\nconcepts and early examples of this trend.<br>This article explores the growing cloud repatriation movement, examining why\norganisations and individuals are reconsidering their cloud-first strategies,\nthe key drivers behind these decisions, and how they&#39;re implementing these\ntransitions to achieve more balanced and optimised IT infrastructures.</p>\n<h2 id=\"the-scale-of-the-cloud-repatriation-movement\">The Scale of the Cloud Repatriation Movement</h2>\n<p>The repatriation trend is not isolated but represents a significant shift in how\norganisations approach their IT infrastructure strategy. According to a 2021\nsurvey by IDC cited by\n<a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International</a>,\n80% of organisations reported repatriating workloads or data from public cloud\nenvironments. More recent data from the end of 2024 showed that 86% of CIOs\nplanned to move some public cloud workloads back to private cloud or on-premises\n-the highest on record for the Barclays CIO Survey\n(<a href=\"https://www.puppet.com/blog/cloud-repatriation\">Puppet, 2025</a>).<br>A recent survey by Rackspace found that nearly seven in 10 companies (69%) have\nmoved at least some applications off the cloud and back to on-premise systems or\nprivate clouds\n(<a href=\"https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/\">ZDNet, 2025</a>).<br>It&#39;s important to note that this doesn&#39;t represent a wholesale abandonment of\ncloud computing. Only about 8% of organisations are moving their entire\nworkloads off the cloud, according to an October 2024 IDC survey\n(<a href=\"https://www.puppet.com/blog/cloud-repatriation\">Puppet, 2025</a>). Most are\nselectively repatriating specific workloads while maintaining others in the\ncloud, resulting in more nuanced, hybrid approaches to IT infrastructure.</p>\n<h2 id=\"key-drivers-of-cloud-repatriation\">Key Drivers of Cloud Repatriation</h2>\n<h3 id=\"cost-optimisation\">Cost Optimisation</h3>\n<p>While the cloud initially promised cost savings through reduced capital\nexpenditure and operational flexibility, many organisations have experienced\nwhat industry experts call &quot;bill shock&quot; as their cloud usage scales. According\nto\n<a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International</a>,\n&quot;a Gartner study predicts that through 2024, 60% of infrastructure and\noperations leaders will encounter public cloud cost overruns that negatively\nimpact their on-premises budgets&quot;.<br>This cost concern is particularly relevant for organisations with predictable,\nhigh-volume workloads. According to\n<a href=\"https://www.rsa.com/resources/blog/identity-governance-and-administration/cloud-repatriation-why-enterprise-it-is-returning-from-the-cloud/\">RSA</a>,\nthe company 37Signals announced that its &quot;cloud exit&quot; would save more than $10\nmillion over five years. Similarly, a 2022 report by Andreessen Horowitz found\nthat repatriation of cloud workloads could reduce cloud bills by 50% or more for\nsome companies\n(<a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International, 2023</a>).<br>David Linthicum, a leading consultant and former CTO with Deloitte, attributes\nmuch of this cost issue to technical debt: &quot;<em>They didn&#39;t refactor the\napplications to make them more efficient in running on the public cloud\nproviders. So the public cloud providers, much like if we&#39;re pulling too much\nelectricity off the grid, just hit them with huge bills to support the\ncomputational and storage needs of those under-optimized applications</em>&quot;\n(<a href=\"https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/\">ZDNet, 2025</a>).</p>\n<h3 id=\"performance-and-latency\">Performance and Latency</h3>\n<p>Performance requirements are driving many repatriation decisions, particularly\nfor applications requiring ultra-low latency. According to\n<a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International</a>,\n&quot;a study by the IEEE found that for certain AI workloads, on-premises GPU\nclusters outperformed cloud-based solutions by up to 30% in terms of performance\nper dollar&quot;.<br>This performance concern is especially critical in fields like financial\ntrading, scientific research, and manufacturing where latency can significantly\nimpact outcomes. As\n<a href=\"https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully\">ComputerWeekly</a>\nnotes, &quot;time-sensitive data includes information that users need to access as\nrapidly as possible -think financial trading feeds -or where the application is\nsensitive to latency&quot;.</p>\n<h3 id=\"security-and-compliance\">Security and Compliance</h3>\n<p>Security concerns and regulatory compliance requirements are powerful motivators\nfor cloud repatriation. According to the Rackspace survey cited by\n<a href=\"https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/\">ZDNet</a>,\ndata security and compliance concerns were the most common reason for\nrepatriation, cited by 50% of respondents.<br>The implementation of stringent regulations like GDPR has compelled many\norganisations to keep certain data within specific geographic boundaries. As\n<a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International</a>\nhighlights, &quot;<em>The Data Protection Commission reported a 59% increase in GDPR\ncomplaints in 2022, underscoring the importance of data sovereignty</em>&quot;.<br>Despite cloud providers&#39; significant security investments, many organisations\nprefer to maintain direct control over their most sensitive data. According to\n<a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International</a>,\n&quot;<em>a 2023 Thales Cloud Security Study found that 45% of businesses have\nexperienced a cloud-based data breach or failed audit in the past 12 months,\nhighlighting ongoing security concerns</em>&quot;.</p>\n<h3 id=\"control-and-vendor-lock-in\">Control and Vendor Lock-in</h3>\n<p>The desire for greater control over hardware and software configurations, along\nwith concerns about vendor lock-in, are also driving repatriation efforts.\nOn-premises infrastructure offers more customisation possibilities that may not\nbe available in public cloud environments.<br>Richard Robbins, founder of TheTechnologyVault.com, observes that &quot;<em>enterprises\ndon&#39;t like being dependent upon someone else&#39;s cloud infrastructure</em>&quot;\n(<a href=\"https://www.zdnet.com/article/why-some-companies-are-backing-away-from-the-public-cloud/\">ZDNet, 2025</a>).\nThis concern is particularly acute among regulated industries such as financial\ninstitutions, which are &quot;<em>moving some or all of their web apps from the cloud\nback to on-prem or to hybrid setups</em>&quot; due to &quot;vulnerability and downsides to\ncloud hosting&quot; that make &quot;executives feel nervous about not having more\ncontrol&quot;.</p>\n<h2 id=\"the-emergence-of-balanced-approaches\">The Emergence of Balanced Approaches</h2>\n<p>Rather than a binary choice between cloud and on-premises, organisations are\nincreasingly adopting hybrid and multi-cloud approaches that offer the best of\nboth worlds. This trend allows organisations to:</p>\n<ul>\n<li>Keep sensitive or high-performance workloads on-premises</li>\n<li>Leverage cloud services for scalability and innovation</li>\n<li>Maintain flexibility to adapt to changing business needs</li>\n</ul>\n<p>According to\n<a href=\"https://blog.trginternational.com/cloud-repatriation-business-return-on-premises\">TRG International</a>,\n&quot;<em>The hybrid cloud market is expected to grow from $85.3 billion in 2022 to\n$262.4 billion by 2027, according to MarketsandMarkets research</em>&quot;. Similarly,\n&quot;<em>Flexera&#39;s 2023 State of the Cloud Report revealed that 71% of enterprises are\npursuing a hybrid cloud strategy, combining public cloud, private cloud, and\non-premises infrastructure</em>&quot;.</p>\n<h2 id=\"personal-cloud-repatriation\">Personal Cloud Repatriation</h2>\n<p>The repatriation trend isn&#39;t limited to enterprises. Individuals are also\nexploring self-hosting options for personal data.<br>For example,\n<a href=\"https://hachyderm.io/@Jeffrey04/114175854454606516\">a fediverse user</a> recently\nposted about developing a self-hosted photo album application when faced with\ncloud storage limitations: &quot;<em>Being an enthusiastic photographer, my partner\ncaptured moments of us together. However, the increasing stack of photos is\naccelerating the imminent explosion of my cloud storage</em>&quot;\n(<a href=\"https://kitfucoda.medium.com/a-love-story-in-code-building-my-self-hosted-photo-album-b56a4e89ebdd\">KitFu Coda, 2023</a>).\nThis personal project highlights how individuals with technical skills can\nleverage idle hardware to create cost-effective alternatives to cloud storage\nservices.<br>As\n<a href=\"https://kitfucoda.medium.com/a-love-story-in-code-building-my-self-hosted-photo-album-b56a4e89ebdd\">they note</a>,\n&quot;<em>Self-hosting your own data is becoming a trend these days, and it is really\nnot hard to get started</em>&quot;. This trend parallels the enterprise movement, with\nindividuals seeking greater control, cost savings, and privacy for their\npersonal data.</p>\n<h2 id=\"planning-for-successful-repatriation\">Planning for Successful Repatriation</h2>\n<p>For organisations considering cloud repatriation, careful planning is essential.\nKey considerations include:</p>\n<ol>\n<li><strong>Workload Assessment</strong>: Not all workloads benefit equally from repatriation.\n<a href=\"https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully\">ComputerWeekly</a>\nadvises that &quot;<em>broadly, repatriation might be the best option where data is\nsensitive, time sensitive or expensive to store in the cloud</em>&quot;.</li>\n<li><strong>Infrastructure Preparation</strong>: Organisations must ensure they have the\nphysical capacity, networking, power, and cooling capabilities to support\nrepatriated workloads. According to\n<a href=\"https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully\">ComputerWeekly</a>,\n&quot;<em>a large repatriation project might be a prompt to reorganise the\ndatacentre, perhaps by moving to newer equipment that can pack more storage\ninto a single rack or that consumes less power</em>&quot;.</li>\n<li><strong>Skills Assessment</strong>:\n<a href=\"https://www.computerweekly.com/feature/Cloud-repatriation-How-to-do-it-successfully\">ComputerWeekly</a>\nnotes the importance of having &quot;<em>enough staff to provision and manage a\nlarger system</em>&quot; with the necessary &quot;<em>security and privacy skills needed to\nhandle sensitive data</em>&quot; and &quot;<em>technical know-how to handle mission-critical,\nlatency sensitive applications</em>&quot;.</li>\n<li><strong>Future-Proofing</strong>:\n<a href=\"https://www.rsa.com/resources/blog/identity-governance-and-administration/cloud-repatriation-why-enterprise-it-is-returning-from-the-cloud/\">RSA</a>\nemphasises the importance of maintaining flexibility: &quot;<em>Organizations should\nconsider the long-term implications of repatriation for their overall IT\nstrategy. This includes planning for future scalability, considering how\nrepatriation fits into the broader digital transformation initiatives, and\nensuring that the new infrastructure aligns with long-term business goals</em>&quot;.</li>\n</ol>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Cloud repatriation represents a maturing perspective on IT infrastructure\nstrategy rather than a rejection of cloud computing. As organisations gain\nexperience with cloud environments, they&#39;re becoming more strategic about which\nworkloads belong where, based on factors like cost, performance, security, and\ncontrol.<br>The future likely belongs to balanced, hybrid approaches that leverage the\nstrengths of both cloud and on-premises infrastructure. As\n<a href=\"https://www.puppet.com/blog/cloud-repatriation\">Puppet</a> notes, &quot;<em>Cloud\nrepatriation is not an endpoint, but rather a strategic tool in the ongoing\nevolution of enterprise IT. It empowers organizations to take control of their\ndigital assets, enhance their security posture, and align their technology\ninfrastructure with their business objectives</em>&quot;.<br>For both organisations and individuals, the key is making informed decisions\nabout where and how to deploy IT resources based on specific needs rather than\nfollowing blanket &quot;cloud-first&quot; or &quot;on-premises-first&quot; policies. This nuanced\napproach to infrastructure strategy will likely characterise the next phase of\ndigital transformation as the industry moves beyond the initial hype cycles of\ncloud adoption.</p>\n"
  },
  {
    "title": "⚙️ Turning Data Science into Real-World Value with The Drivetrain Framework",
    "date": "2025-03-20T00:00:00.000Z",
    "tags": [
      "data-science",
      "decision-making",
      "machine-learning",
      "modelling-mindsets",
      "optimisation",
      "fast-ai",
      "advantage",
      "best-practices",
      "design-principles",
      "causal-inference",
      "business-value",
      "predictive-modelling",
      "integration",
      "deliberate-experimentation",
      "real-value"
    ],
    "url": "/posts/the-drivetrain-method.html",
    "content": "<p><strong>TL;DR:</strong> Jeremy Howard&#39;s Drivetrain Framework transforms data science from\nisolated predictions to value-creating systems through four steps: defining\nclear objectives, identifying controllable levers, collecting causal data\nthrough deliberate experimentation, and building integrated systems that combine\npredictive models with optimisation - bridging the gap between analytics and\nmeasurable business results.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Most data science initiatives fail to deliver meaningful impact. Why? Because\nthey focus on prediction rather than action. Organisations spend millions\nbuilding sophisticated prediction models that tell them what <em>might</em> happen, but\nprovide no clear path to influencing outcomes.<br>This gap between prediction and value creation is what Jeremy Howard, data\nscientist and entrepreneur, addressed in his transformative\n&quot;<a href=\"https://www.youtube.com/watch?v=vYrWTDxoeGg\">Drivetrain Framework</a>&quot; back\nin 2012. Having successfully applied this approach to revolutionise insurance\npricing, Howard outlines a systematic method for connecting data science to\ntangible business results.<br>The framework isn&#39;t about building more complex algorithms -it&#39;s about\nconstructing systems that link predictions to decisions that drive value. If\nyou&#39;re struggling to translate advanced analytics into bottom-line results or\nfinding your data science investments yield interesting insights but limited\naction, this framework offers a practical solution to bridge that gap.</p>\n<h2 id=\"the-four-critical-components\">The Four Critical Components</h2>\n<p>The Drivetrain Framework consists of four interconnected steps that bridge the\ngap between data and value:</p>\n<h3 id=\"1-define-your-objective\">1. Define Your Objective</h3>\n<p>Begin with absolute clarity about what you&#39;re trying to achieve. In Howard&#39;s\ninsurance example, the objective was straightforward: maximise profit from each\ncustomer based on price. For Google&#39;s search engine, it was finding the most\nrelevant web page based on a query. For a marketing team, it might be maximising\ncustomer lifetime value.<br>Without a clear objective, data science becomes an academic exercise. With one,\nit becomes a targeted tool for value creation.</p>\n<h3 id=\"2-identify-your-levers\">2. Identify Your Levers</h3>\n<p>Next, determine what variables you can actually control. These are your &quot;levers&quot;\n-the actions you can take to influence outcomes:</p>\n<ul>\n<li>For Google, the key lever was the ordering of search results</li>\n<li>For insurers, it was the price offered to each customer</li>\n<li>For marketers, levers include product recommendations, discount offers, and\ncommunication timing</li>\n</ul>\n<p>The insight here is focusing not on what you can predict, but on what you can\nchange.</p>\n<h3 id=\"3-collect-causal-data\">3. Collect Causal Data</h3>\n<p>Howard emphasises a crucial distinction: most organisations have plenty of\nobservational data showing correlations, but lack causal data showing what\nhappens when you pull different levers.<br>This requires intentional experimentation:</p>\n<ul>\n<li>The insurance company randomly varied prices to understand true price-response\nrelationships</li>\n<li>A marketing team might randomly test diverse recommendations rather than\nshowing what customers already like</li>\n</ul>\n<p>The counterintuitive insight: You must sometimes sacrifice short-term\noptimisation to collect data that enables superior long-term results. Howard\nconvinced insurance executives to randomise pricing for six months -initially\naccepting potentially lower profits -to build models that later significantly\nincreased their profitability and transformed how the entire industry approached\npricing.</p>\n<h3 id=\"4-build-an-integrated-system\">4. Build an Integrated System</h3>\n<p>The final step combines three elements to connect levers to objectives:</p>\n<ul>\n<li><strong>Modeller</strong>: Build predictive models for key relationships (e.g. how price\naffects purchase probability)</li>\n<li><strong>Simulator</strong>: Combine models to predict outcomes of actions (e.g. how price\nchanges affect profit across customer segments)</li>\n<li><strong>Optimizer</strong>: Find the best lever settings to achieve objectives (e.g.\noptimal price for each customer)</li>\n</ul>\n<p>This integrated approach replaces the need for complex &quot;PageRank-like&quot;\nalgorithms with systems that combine simpler models to optimise real-world\noutcomes.</p>\n<h2 id=\"application-revolutionising-marketing\">Application: Revolutionising Marketing</h2>\n<p>Howard suggests marketing analytics remains in the &quot;Dark Ages&quot; and ready for\ntransformation through the Drivetrain approach:<br>Consider Amazon&#39;s recommendation system. Rather than simply suggesting more\nbooks by authors you&#39;ve already read, a Drivetrain-based system would:</p>\n<ol>\n<li>Define the objective as maximising customer lifetime value</li>\n<li>Identify recommendation content as a key lever</li>\n<li>Collect causal data by testing diverse recommendations, including unexpected\nones</li>\n<li>Build an integrated system that models what customers might enjoy but don&#39;t\nyet know about, and optimises for long-term value</li>\n</ol>\n<p>In Howard&#39;s experience, companies implementing this approach have seen\nsubstantial improvements in customer engagement and retention while achieving\nmeaningful reductions in marketing costs.</p>\n<h2 id=\"drawing-from-engineering\">Drawing from Engineering</h2>\n<p>Howard notes that many solutions already exist in engineering disciplines, which\ndata scientists would benefit from studying.<br>Aircraft designers have used integrated models and optimisation for decades,\ncombining aerodynamic models, structural analysis, and optimisation techniques\nto create planes that safely fly millions of passengers daily.<br>Building construction similarly relies on systems that integrate architectural\nmodels, structural engineering, and materials science to optimize for safety,\ncost, and aesthetics.<br>The most advanced example might be Google&#39;s self-driving car, which integrates\nmultiple predictive models (how the car responds to controls, what sensors\ndetect) with optimisation to safely navigate real-world environments,\nsignificantly improving safety in testing environments.<br>These engineering successes demonstrate how combining relatively simple models\ninto integrated systems can solve extraordinarily complex problems.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The Drivetrain Framework represents a fundamental shift in how we should\napproach data science:</p>\n<ol>\n<li>Move beyond building better predictive models in isolation</li>\n<li>Focus on connecting predictions to actions that drive real value</li>\n<li>Invest in collecting causal data through deliberate experimentation</li>\n<li>Integrate modelling, simulation, and optimisation into coherent systems</li>\n</ol>\n<p>By adopting this framework, organisations can bridge the gap between\nsophisticated analytics and meaningful results. The companies that will gain\ncompetitive advantage aren&#39;t those with marginally better algorithms, but those\nthat build integrated systems connecting data to decisions that create value.</p>\n<h2 id=\"getting-started\">Getting Started</h2>\n<p>To begin implementing the Drivetrain approach:</p>\n<ol>\n<li>Identify one high-value business objective with measurable outcomes</li>\n<li>Map the specific levers your team can control that influence this objective</li>\n<li>Design small-scale experiments to collect causal data about these\nrelationships</li>\n<li>Start simple -build basic models for key relationships, then integrate them\nbefore attempting sophisticated optimisation</li>\n</ol>\n<p>The most important step is shifting your thinking from &quot;<em>what can we predict?</em>&quot;\nto &quot;<em>what actions can we take to create value?</em>&quot; -the essence of the Drivetrain\nFramework.</p>\n"
  },
  {
    "title": "📦 From Compilation to Containerisation and Back Again",
    "date": "2025-03-19T00:00:00.000Z",
    "tags": [
      "deno",
      "typescript",
      "deployment",
      "cross-platform",
      "evolution",
      "toolchain",
      "best-practices",
      "code-quality"
    ],
    "url": "/posts/compilation-going-back-full-circle.html",
    "content": "<p><strong>TL;DR:</strong> Programming languages have evolved from compiled executables to\ninterpreted languages and containerisation, but Deno 2.0 brings deployment full\ncircle by enabling TypeScript/JavaScript compilation into standalone\nbinaries-offering simplified cross-platform deployment whilst maintaining\necosystem richness and enabling single-language development across entire\napplication stacks.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Over the years, I&#39;ve experimented with numerous programming languages and\ndeployment strategies. Python has been my domain&#39;s lingua franca -with its vast\necosystem for data science and AI applications. However, its deployment\ncomplexities have consistently been a pain point: managing dependencies,\nconfiguring containers, and setting up build pipelines.<br>This search for a better alternative has led me through statically compiled\nlanguages like Go and Rust; JIT-compiled languages like Julia; and hosted\nlanguages like Clojure and Scala. Yet most failed to provide a good balance\nbetween ecosystem richness and deployment simplicity. Recently, however, Deno\n2.0 has emerged as a compelling solution -particularly with its ability to\ncompile TypeScript (TS) / JavaScript (JS) to standalone executables.</p>\n<h2 id=\"the-circular-evolution-of-programming-languages\">The Circular Evolution of Programming Languages</h2>\n<p>Programming languages have undergone a fascinating evolution. In the beginning\n(the late 1950s and 1960s), languages like Fortran, COBOL, and C were\nahead-of-time compiled -transformed directly into machine code executables that\ncould run without additional dependencies.<br>As computing evolved, the pendulum swung toward higher-level languages\n-interpreted languages like Python and hosted environments like the JVM-\nprioritising readability and developer productivity over raw performance. These\nlanguages abstracted away machine-level concerns, allowing developers to focus\non solving business problems.<br>Yet this shift introduced new challenges. Python applications often require\nmanaging complex dependency trees, virtual environments, and platform-specific\nconfigurations. The infamous &quot;<em>works on my machine</em>&quot; problem became so pervasive\nthat containerisation emerged as a solution.<br>While effective, containerisation introduces its own complexities:\norchestration, image management, and networking configurations. What began as a\nsolution to simplify deployment has become a complex system requiring\nspecialised knowledge.</p>\n<h2 id=\"deno-compilation-makes-a-comeback\">Deno: Compilation Makes a Comeback</h2>\n<p>Deno 2.0 represents a return to first principles. As highlighted in the\n<a href=\"https://youtube.com/watch?v=ZsDqTQs3_G0\">Run JavaScript Anywhere</a> video, its\n<code>compile</code> command enables developers to transform JS and TS programs into\nstandalone binaries that run across major platforms -no runtime installation or\ndependencies required.</p>\n<pre><code class=\"language-typescript\">// sample.ts\nimport { open } from &quot;https://deno.land/x/open/index.ts&quot;;\n\n// Open a URL in the default browser\nawait open(&quot;https://example.com&quot;);\n</code></pre>\n<p>With a simple <code>deno compile sample.ts</code> command, this code becomes a standalone\nexecutable that works on any machine without requiring Deno to be installed.<br>This compilation process isn&#39;t traditional transpilation to machine code -it\nembeds your JS and TS code into a specialized Deno runtime binary (denort). Your\nscript and dependencies are bundled as an EZIP file and injected into the\nruntime binary, creating a self-contained executable that can be code-signed for\ndistribution.</p>\n<p>The key benefits include:</p>\n<ol>\n<li><strong>Cross-platform compatibility</strong> without runtime requirements</li>\n<li><strong>Simplified deployment</strong> with single-binary distribution</li>\n<li><strong>Bundled assets</strong> for complete portability</li>\n<li><strong>Improved startup times</strong> compared to interpreter-based approaches</li>\n</ol>\n<p>Deno 2.0 enhances these capabilities further with support for npm packages, web\nworkers, cross-compilation, smaller binary sizes, and code signing with custom\nicons - making it viable for complete applications, not just scripts.</p>\n<h2 id=\"the-single-language-advantage\">The Single Language Advantage</h2>\n<p>Beyond deployment simplicity, using a single language across an entire project\nstack creates significant organisational benefits. I&#39;ve experienced first-hand\nhow using different languages for front-end, back-end, and data science work can\ncreate silos within teams.<br><a href=\"https://dockyard.com/blog/2024/02/06/5-benefts-amplified-saw-switching-to-elixir\">Amplified&#39;s case study</a>\ndemonstrates this point clearly. After switching from a React/JS front-end and\nPhoenix/Elixir back-end to an all-Elixir approach with LiveView, they reported:</p>\n<ol>\n<li><strong>Halved server costs</strong> through more efficient resource utilisation</li>\n<li><strong>Dramatically increased development speed</strong> by eliminating cross-language\nsilos</li>\n<li><strong>Improved team cohesion</strong> with shared tooling and knowledge</li>\n<li><strong>Enhanced maintainability</strong> through code reuse</li>\n<li><strong>Reduced team size requirements</strong> from 12 developers to just 2</li>\n</ol>\n<p>TS with Deno provides a similar single language opportunity -allowing teams to\nbuild front-end interfaces, back-end services, and data processing workflows\nwith the same toolchain. The JS/TS ecosystem is rapidly maturing for AI, ML, and\ndata science applications, as I noted in my previous article on [Modern Data\nScience and AI Engineering with Deno 2.0]({{ site.baseurl }}{% link\n_posts/2024-09-05-deno.md %}).<br>One often overlooked benefit is the reduced cognitive load when developers don&#39;t\nneed to context-switch between different language paradigms, package managers,\ntesting frameworks, and debugging approaches.</p>\n<h2 id=\"practical-applications\">Practical Applications</h2>\n<p>Deno&#39;s compilation capabilities shine in several real-world scenarios:</p>\n<ol>\n<li><strong>CLI Tools</strong>: Creating self-contained executables that &quot;just work&quot; across\nplatforms without complex installation instructions</li>\n<li><strong>Offline Environments</strong>: Deploying to systems without internet access, where\npackage resolution at runtime isn&#39;t possible</li>\n<li><strong>Cross-Platform Applications</strong>: Building desktop applications that leverage\nweb technologies without requiring a browser runtime</li>\n</ol>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>We&#39;ve come full circle in programming language evolution -from compiled\nlanguages like Fortran in the 1950s, to interpreted languages for improved\ndeveloper experience, to containerisation for managing deployment complexities,\nand now back to compilation with Deno[^1].<br>Deno&#39;s approach represents a compelling blend - combining deployment simplicity\nwith the ecosystem richness of modern TS/JS. For AI engineering, this addresses\nmany pain points of Python deployment while maintaining access to growing\necosystem of data science tools.<br>While Elixir offers similar single language benefits, its distribution story\nremains a work in progress with projects like\n<a href=\"https://github.com/burrito-elixir/burrito\">Burrito</a> showing promise but not yet\nfully mature. Until then, Deno stands out as a viable alternative for simplified\ndeployment without sacrificing ecosystem benefits.<br>The future of deployment may look surprisingly like its past, just with better\nlanguages and tools at our disposal -offering a path toward more cohesive,\nefficient software development that reduces complexity without sacrificing\ncapability.</p>\n<hr>\n<p>[^1]: Go, Zig, Rust, C/C++ D, Nim, Common Lisp are some prominent examples of\n    ahead-of-time compiled languages that -with the exception of Common Lisp-\n    excel in systems programming. However, Deno allows a ubiquitous,\n    higher-level language like JS and its superset TS to join the club of\n    languages that can easily package code to a cross-platform single binary.</p>\n"
  },
  {
    "title": "🧠 RAG vs CAG: Understanding Knowledge Augmentation in LLMs",
    "date": "2025-03-18T00:00:00.000Z",
    "tags": [
      "rag",
      "llm",
      "ai",
      "machine-learning",
      "prompt-engineering",
      "nlp",
      "data-processing",
      "best-practices"
    ],
    "url": "/posts/rag-or-cag.html",
    "content": "<p><strong>TL;DR:</strong> Retrieval Augmented Generation (RAG) and Cache Augmented Generation\n(CAG) represent two distinct approaches to expanding LLM knowledge: RAG\ndynamically retrieves relevant documents for each query, offering scalability\nfor large datasets, whilst CAG preloads all information into the model&#39;s context\nwindow, providing faster responses for smaller, static knowledge bases.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Large Language Models (LLMs) face a fundamental knowledge problem: they&#39;re\nlimited to information present in their training data. This creates challenges\nwhen dealing with recent events that occurred after training or proprietary\ninformation specific to an organization.<br>To address these limitations, two primary augmentation techniques have emerged:\nRetrieval Augmented Generation (RAG) and Cache Augmented Generation (CAG). This\narticle breaks down both approaches based on\n<a href=\"https://www.youtube.com/channel/UCKWaEZ-_VweaEx1j62do_vQ\">IBM Technology</a>&#39;s\ncomprehensive explanation from their\n<a href=\"https://youtube.com/watch?v=HdafI0t3sEY\">video on RAG vs CAG</a>, examining how\nthey work, their capabilities, and when to use each one.</p>\n<h2 id=\"understanding-rag-and-cag\">Understanding RAG and CAG</h2>\n<h3 id=\"retrieval-augmented-generation-rag\">Retrieval Augmented Generation (RAG)</h3>\n<p>RAG operates through a two-phase system:</p>\n<ol>\n<li><strong>Offline Phase (Preparation)</strong><ul>\n<li>Documents are broken into manageable chunks.</li>\n<li>Vector embeddings are created for each chunk using an embedding model.</li>\n<li>These embeddings are stored in a vector database, creating a searchable\nknowledge index.</li>\n</ul>\n</li>\n<li><strong>Online Phase (Query &amp; Response)</strong><ul>\n<li>The user submits a query.</li>\n<li>The RAG retriever converts this query to a vector using the same embedding\nmodel.</li>\n<li>The system performs a similarity search in the vector database.</li>\n<li>It retrieves the most relevant document chunks (typically 3-5 passages).</li>\n<li>These chunks and the user&#39;s query are placed in the LLM&#39;s context window.</li>\n<li>The LLM generates an answer based on both the query and the retrieved\ncontext.</li>\n</ul>\n</li>\n</ol>\n<p>For example, if asked <em>&quot;What film won Best Picture this year?&quot;</em>, the system\nmight retrieve information about <em>&quot;Anora&quot;</em> winning the award, even if this\noccurred after the model&#39;s original training.</p>\n<p>A key advantage of RAG is its modularity - components like the vector database,\nembedding model, or LLM can be swapped independently without rebuilding the\nentire system.</p>\n<h3 id=\"cache-augmented-generation-cag\">Cache Augmented Generation (CAG)</h3>\n<p>CAG takes a fundamentally different approach:</p>\n<ul>\n<li>Instead of retrieving knowledge on demand, CAG preloads all available\ninformation into the model&#39;s context window</li>\n<li>The entire knowledge corpus is formatted into one massive prompt that fits\nwithin the model&#39;s context limits</li>\n<li>The LLM processes this extensive input in a single forward pass</li>\n<li>The model&#39;s internal state is captured in what&#39;s called a &quot;KV cache&quot;\n(key-value cache)</li>\n<li>When a user query arrives, it&#39;s added to this pre-existing KV cache</li>\n<li>The model can access any relevant information from the cache without\nreprocessing the entire knowledge base</li>\n</ul>\n<p>The fundamental distinction: RAG fetches only what it predicts is needed, while\nCAG loads everything upfront and remembers it for later use.</p>\n<h2 id=\"comparing-capabilities\">Comparing Capabilities</h2>\n<h3 id=\"accuracy\">Accuracy</h3>\n<ul>\n<li><strong>RAG</strong>: Accuracy depends heavily on the retriever component. If the retriever\nfails to fetch relevant documents, the LLM won&#39;t have the facts needed to\nanswer correctly.</li>\n<li><strong>CAG</strong>: Guarantees that all information is available (assuming it exists in\nthe knowledge base), but places the burden on the LLM to extract the right\ninformation from a large context.</li>\n</ul>\n<h3 id=\"latency\">Latency</h3>\n<ul>\n<li><strong>RAG</strong>: Higher latency due to additional steps of embedding the query,\nsearching the index, and processing retrieved text.</li>\n<li><strong>CAG</strong>: Lower latency once knowledge is cached, as answering queries requires\nonly one forward pass without retrieval lookup time.</li>\n</ul>\n<h3 id=\"scalability\">Scalability</h3>\n<ul>\n<li><strong>RAG</strong>: Can scale to millions of documents as only a small portion is\nretrieved per query.</li>\n<li><strong>CAG</strong>: Limited by the model&#39;s context window size (typically ~32k-100k\ntokens), restricting it to a few hundred documents at most.</li>\n</ul>\n<h3 id=\"data-freshness\">Data Freshness</h3>\n<ul>\n<li><strong>RAG</strong>: Easy to update incrementally as you add new document embeddings or\nremove outdated ones.</li>\n<li><strong>CAG</strong>: Requires recomputation when data changes, making it less suitable for\nfrequently updated information.</li>\n</ul>\n<h2 id=\"when-to-use-each-approach\">When to Use Each Approach</h2>\n<p>The video presents several scenarios to illustrate when each approach is more\nappropriate:</p>\n<ol>\n<li><strong>IT Help Desk Bot with Static Manual (200 pages, rarely updated)</strong><ul>\n<li><strong>Best Choice</strong>: CAG</li>\n<li><strong>Rationale</strong>: Knowledge base is small enough to fit in most LLM context\nwindows, information is static, and caching enables faster query responses.</li>\n</ul>\n</li>\n<li><strong>Legal Research Assistant (Thousands of constantly updated documents)</strong><ul>\n<li><strong>Best Choice</strong>: RAG</li>\n<li><strong>Rationale</strong>: Knowledge base is massive and dynamic, precise citations are\nrequired, and incremental updates are essential.</li>\n</ul>\n</li>\n<li><strong>Clinical Decision Support System (Patient records, treatment guides, drug\ninteractions)</strong><ul>\n<li><strong>Best Choice</strong>: Hybrid Approach</li>\n<li><strong>Rationale</strong>: Use RAG to retrieve relevant subsets from the massive\nknowledge base, then load that retrieved content into a long-context model\nusing CAG for follow-up questions.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The choice between RAG and CAG ultimately depends on your specific use case.\nConsider RAG when dealing with large or frequently updated knowledge sources,\nwhen citations are necessary, or when resources for running long-context models\nare limited. CAG is preferable when working with a fixed knowledge set that fits\nwithin your model&#39;s context window, when low latency is crucial, or when you\nwant to simplify deployment.<br>As LLM technology evolves with expanding context windows and improved retrieval\nmechanisms, we may see these approaches converge or new hybrid solutions emerge.\nFor now, understanding the strengths and limitations of both RAG and CAG allows\nAI engineers to make informed decisions about knowledge augmentation strategies\nthat best suit their specific applications.</p>\n"
  },
  {
    "title": "🤖 The State of AI Agents in 2025",
    "date": "2025-03-15T00:00:00.000Z",
    "tags": [
      "ai",
      "machine-learning",
      "llm",
      "best-practices",
      "evaluation",
      "prompt-engineering",
      "decision-making"
    ],
    "url": "/posts/navigating-ais-frontier-2025.html",
    "content": "<p><strong>TL;DR:</strong> Despite significant advancements creating a &quot;perfect storm&quot; for AI\nagents in 2025, truly autonomous systems still face five categories of\ncumulative errors that prevent reliable performance; overcoming these challenges\nrequires focused strategies in data curation, robust evaluation frameworks,\nscaffolding systems, distinctive user experiences, and multimodal approaches.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>The AI landscape has evolved at a breathtaking pace over the past few years,\nwith autonomous AI agents being positioned as the next revolutionary frontier.\nAt the 2025 AI Engineer Summit, Grace Isford, a partner at Lux Capital,\ndelivered an <a href=\"https://www.youtube.com/watch?v=HS5a8VIKsvA\">insightful keynote</a>\non &quot;The State of the AI Frontier&quot; that challenged the prevailing narrative about\nAI agents. While many industry players proclaim that 2025 marks the &quot;perfect\nstorm&quot; for AI agents, Isford&#39;s presentation offered a more nuanced view,\nhighlighting both the tremendous progress and the significant challenges that\nremain. This article summarises the key insights from her keynote, examining the\ncurrent state of AI agents and the strategies developers can employ to overcome\npersistent limitations.</p>\n<h2 id=\"the-perfect-storm-for-ai-agents\">The Perfect Storm for AI Agents</h2>\n<p>The speaker began by acknowledging the remarkable progress in AI over the past\ntwo and a half years. The industry has seen exponential advancements since the\nrelease of Stable Diffusion in August 2022, with the pace of innovation only\naccelerating. 2025 has already witnessed several landmark developments:</p>\n<ul>\n<li>The announcement of the $500 billion Stargate project collaboration between\nthe U.S. government, OpenAI, SoftBank, and Oracle</li>\n<li>OpenAI&#39;s o3 model exceeding human performance in the Arc AGI challenge</li>\n<li>DeepSeq&#39;s R1 model launch causing market disruptions and reaching the top of\nthe App Store</li>\n<li>France&#39;s new AI initiative announced at the France AI Summit, bringing Europe\nback into the global AI race</li>\n</ul>\n<p>These developments, alongside other factors, have created what many call the\n&quot;perfect storm&quot; for AI agents:</p>\n<ol>\n<li>Reasoning models (like OpenAI&#39;s o1 and o3, DeepSeq&#39;s R1, and Grok&#39;s latest\noffering) now outperform humans in various benchmarks</li>\n<li>Increased test-time compute (more resources allocated to inference rather\nthan just training)</li>\n<li>Engineering and hardware optimisations driving efficiency</li>\n<li>Cheaper inference and hardware costs</li>\n<li>A narrowing gap between open-source and closed-source models</li>\n<li>Massive infrastructure investments from governments and corporations\nworldwide</li>\n</ol>\n<h2 id=\"the-reality-gap-why-ai-agents-arent-quite-working-yet\">The Reality Gap: Why AI Agents Aren&#39;t Quite Working Yet</h2>\n<p>Despite this promising landscape, Isford argued that truly autonomous AI agents\naren&#39;t functioning as seamlessly as industry hype suggests. To illustrate this\npoint, she shared a real-world example of trying to use OpenAI&#39;s operator to\nbook a flight from New York to San Francisco with specific requirements. Despite\nseemingly straightforward criteria (departure time after 3 PM, avoiding rush\nhour, specific airlines, budget constraints, seat preferences), the agent failed\nto deliver a satisfactory result.</p>\n<p>The presenter identified five categories of cumulative errors that prevent AI\nagents from delivering consistent, reliable results:</p>\n<ol>\n<li><strong>Decision Errors</strong>: Choosing incorrect facts or overthinking/exaggerating\nscenarios</li>\n<li><strong>Implementation Errors</strong>: Encountering access issues or integration failures\n(like CAPTCHA challenges)</li>\n<li><strong>Heuristic Errors</strong>: Applying wrong criteria or missing critical contextual\ninformation</li>\n<li><strong>Taste Errors</strong>: Failing to account for personal preferences not explicitly\nstated</li>\n<li><strong>Perfection Paradox</strong>: User expectations heightened by AI&#39;s capabilities in\nsome areas lead to frustration when agents perform at merely human speed or\nmake basic errors</li>\n</ol>\n<p>These errors compound dramatically in complex multi-agent systems with\nmulti-step tasks. Isford presented a compelling visual example showing how even\nagents with impressive 99% and 95% accuracy rates drop to 60% and 8% reliability\nrespectively after just 50 consecutive steps.</p>\n<h2 id=\"five-strategies-for-building-better-ai-agents\">Five Strategies for Building Better AI Agents</h2>\n<p>The keynote then shifted to offering concrete strategies for mitigating these\nchallenges and building more effective AI agents:</p>\n<h3 id=\"1-data-curation\">1. Data Curation</h3>\n<ul>\n<li>Recognise that data is increasingly diverse (text, images, video, audio,\nsensor data)</li>\n<li>Curate proprietary data, including data generated by the agent itself</li>\n<li>Design &quot;data flywheels&quot; that automatically improve agent performance through\nuser interactions</li>\n<li>Recycle and adapt to user preferences in real-time</li>\n</ul>\n<h3 id=\"2-robust-evaluation-systems\">2. Robust Evaluation Systems</h3>\n<ul>\n<li>Move beyond evaluations for verifiable domains (math, science) to develop\nframeworks for subjective assessments</li>\n<li>Collect signals about human preferences</li>\n<li>Build personalised evaluation systems that reflect actual user needs</li>\n<li>Sometimes the best evaluation is direct human testing rather than relying\nsolely on benchmarks</li>\n</ul>\n<h3 id=\"3-scaffolding-systems\">3. Scaffolding Systems</h3>\n<ul>\n<li>Implement safeguards to prevent cascading failures when errors occur</li>\n<li>Build complex compound systems that can work together harmoniously</li>\n<li>Incorporate human intervention at critical junctures</li>\n<li>Develop self-healing agents that can recognise their own mistakes and correct\ncourse</li>\n</ul>\n<h3 id=\"4-user-experience-as-a-competitive-moat\">4. User Experience as a Competitive Moat</h3>\n<ul>\n<li>Recognise that UX differentiation is crucial when most applications are using\nthe same foundation models</li>\n<li>Deeply understand user workflows to create elegant human-machine collaboration</li>\n<li>Integrate seamlessly with existing systems to deliver tangible ROI</li>\n<li>Focus on industries with proprietary data sources and specialised workflows\n(robotics, manufacturing, life sciences)</li>\n</ul>\n<h3 id=\"5-multimodal-approaches\">5. Multimodal Approaches</h3>\n<ul>\n<li>Move beyond basic chatbot interfaces to create more human-like experiences</li>\n<li>Incorporate multiple sensory capabilities (vision, voice, and potentially\ntouch or smell)</li>\n<li>Build personal memory systems that understand users on a deeper level</li>\n<li>Transform inconsistent but visionary products into experiences that exceed\nexpectations through novel interfaces</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>While 2025 has created what appears to be a perfect storm for AI agents with\nadvanced reasoning models, increased compute efficiency, and massive\ninfrastructure investments, the reality is that autonomous AI agents still face\nsignificant challenges. The cumulative effect of small errors across\ndecision-making, implementation, heuristics, and user preferences creates\nsubstantial reliability issues in complex agent systems.</p>\n<p>However, as this keynote emphasised, these challenges are not insurmountable. By\nfocusing on meticulous data curation, developing sophisticated evaluation\nframeworks, implementing robust scaffolding systems, prioritising distinctive\nuser experiences, and embracing multimodal approaches, developers can build AI\nagents that deliver on their transformative potential. The lightning strike of\ntruly autonomous, reliable AI agents may not have happened yet, but with these\nstrategies, the industry is moving steadily toward that breakthrough moment.</p>\n"
  },
  {
    "title": "🗄️ SQLite: The Minimalist Database for AI Engineering",
    "date": "2025-02-11T00:00:00.000Z",
    "tags": [
      "ai",
      "data-modeling",
      "data-processing",
      "data-science",
      "minimal",
      "production",
      "python",
      "zero-config"
    ],
    "url": "/posts/sqlite-minimalist-choice-for-ai-engineering.html",
    "content": "<p><strong>TL;DR:</strong> SQLite offers a zero-configuration, pre-installed database solution\nideal for AI engineering projects, supporting modern data structures including\nvectors, graphs, and JSON documents whilst providing single-file portability,\nACID compliance, and broad language compatibility - making it an excellent\nminimalist choice when specialised database systems would be overkill.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>In today&#39;s AI engineering landscape, choosing the right database can feel\noverwhelming. While specialised solutions like <a href=\"https://qdrant.tech/\">Qdrant</a>\n(vectors), <a href=\"https://neo4j.com/\">Neo4j</a> (graphs), and\n<a href=\"https://www.mongodb.com/\">MongoDB</a> (documents) excel in their niches, there&#39;s a\ncompelling case for <a href=\"https://www.sqlite.org/index.html\">SQLite</a> as a versatile,\nminimalist solution that comes pre-installed on most systems and supports\nmultiple data structures effectively. Speaking of minimalism,\n<a href=\"https://github.com/tconbeer/harlequin\">Harlequin</a> (named after a\n<a href=\"https://en.wikipedia.org/wiki/Harlequin_duck\">sea 🦆</a>) makes data exploration\nvery enjoyable. Credit for the SQLite idea goes to\n<a href=\"https://bsky.app/profile/simonwillison.net\">Simon Willison</a>, a prolific AI\nresearcher among others, who has been posting\n<a href=\"https://simonwillison.net/tags/sqlite/\">blog articles</a> and\n<a href=\"https://til.simonwillison.net/sqlite\">TILs</a> (Today I Learned) about it since\n2003!</p>\n<h2 id=\"the-power-of-pre-installation\">The Power of Pre-installation</h2>\n<p>SQLite&#39;s ubiquity is remarkable. It comes pre-installed on:</p>\n<ul>\n<li>macOS</li>\n<li>Most Linux distributions (including Ubuntu, as evidenced by its\n<a href=\"https://releases.ubuntu.com/24.10/ubuntu-24.10-desktop-amd64.manifest\">manifest</a>)</li>\n<li>Python&#39;s standard library</li>\n<li>Android devices</li>\n<li>iOS devices</li>\n</ul>\n<p>This universal availability means you can start developing immediately without\nadditional setup or installation steps.</p>\n<h2 id=\"modern-data-structure-support\">Modern Data Structure Support</h2>\n<p>Despite its lightweight nature, SQLite handles modern data structures\nsurprisingly well:</p>\n<ol>\n<li><strong>Vector Storage</strong>[^1]</li>\n</ol>\n<pre><code class=\"language-sql\">CREATE VIRTUAL TABLE vec_items USING vec0(embedding float[4])\n</code></pre>\n<pre><code class=\"language-sql\">-- vectors can be provided as JSON or in a compact binary format\nINSERT INTO vec_items(rowid, embedding)\n  VALUES\n    (1, &#39;[-0.200, 0.250, 0.341, -0.211, 0.645, 0.935, -0.316, -0.924]&#39;),\n    (2, &#39;[0.443, -0.501, 0.355, -0.771, 0.707, -0.708, -0.185, 0.362]&#39;),\n    (3, &#39;[0.716, -0.927, 0.134, 0.052, -0.669, 0.793, -0.634, -0.162]&#39;),\n    (4, &#39;[-0.710, 0.330, 0.656, 0.041, -0.990, 0.726, 0.385, -0.958]&#39;);\n</code></pre>\n<pre><code class=\"language-sql\">-- KNN-style query\nSELECT\n  rowid,\n  distance\nFROM vec_items\nWHERE embedding MATCH &#39;[0.890, 0.544, 0.825, 0.961, 0.358, 0.0196, 0.521, 0.175]&#39;\nORDER BY distance\nLIMIT 3\n</code></pre>\n<ol start=\"2\">\n<li><strong>Graph Relationships</strong>[^2]</li>\n</ol>\n<pre><code class=\"language-sql\">-- Create table `nodes`\nCREATE TABLE IF NOT EXISTS nodes (\n    id TEXT PRIMARY KEY,\n    properties TEXT\n)\n</code></pre>\n<pre><code class=\"language-sql\">-- Create table `edges`\nCREATE TABLE IF NOT EXISTS edges (\n    source TEXT,\n    target TEXT,\n    relationship TEXT,\n    weight REAL,\n    PRIMARY KEY (source, target, relationship),\n    FOREIGN KEY (source) REFERENCES nodes(id),\n    FOREIGN KEY (target) REFERENCES nodes(id)\n)\n</code></pre>\n<pre><code class=\"language-sql\">-- Create indices of the `edges` between `source` and `target`, for improved performance\nCREATE INDEX IF NOT EXISTS source_idx ON edges(source)\nCREATE INDEX IF NOT EXISTS target_idx ON edges(target)\n</code></pre>\n<pre><code class=\"language-sql\">-- Count the no. of incoming and outgoing edges per node, known as &#39;degree centrality&#39;\nSELECT id,\n       (SELECT COUNT(*) FROM edges WHERE source = nodes.id) +\n       (SELECT COUNT(*) FROM edges WHERE target = nodes.id) as degree\nFROM nodes\nORDER BY degree DESC\nLIMIT 10\n</code></pre>\n<ol start=\"3\">\n<li><strong>Document Storage</strong></li>\n</ol>\n<pre><code class=\"language-sql\">CREATE TABLE documents (\n    id INTEGER PRIMARY KEY,\n    content JSON,\n    metadata JSON\n);\n</code></pre>\n<h2 id=\"portability-and-simplicity\">Portability and Simplicity</h2>\n<p>One of SQLite&#39;s strongest features is its\n<a href=\"https://www.sqlite.org/onefile.html\">single-file</a> nature. Your entire database\nexists in one file that can be:</p>\n<ul>\n<li>Backed up with a simple copy operation</li>\n<li>Easily version controlled (for smaller databases)</li>\n<li>Moved between systems effortlessly</li>\n<li>Examined with standard SQLite tools</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>While specialised databases have their place, SQLite offers a compelling\ncombination of features that make it ideal for many AI engineering projects:</p>\n<ul>\n<li>Zero configuration</li>\n<li>Pre-installed availability</li>\n<li>Support for multiple data structures</li>\n<li>Single-file portability</li>\n<li>Wide language support, especially in Python and Go</li>\n<li>ACID[^3] compliance</li>\n</ul>\n<p><strong>TL;DR</strong>: When you need a lightweight, self-contained database that can handle\ndocuments, vectors, and graphs without the complexity of a full database server,\nSQLite is often an excellent choice.</p>\n<hr>\n<p>[^1]: Example from\n    <a href=\"https://alexgarcia.xyz/sqlite-vec/python.html\">sqlite-vec with Python</a></p>\n<p>[^2]: Examples from\n    <a href=\"https://dev.to/stephenc222/how-to-build-lightweight-graphrag-with-sqlite-53le\">How to Build Lightweight GraphRAG with SQLite</a></p>\n<p>[^3]: Atomicity, Consistency, Isolation, Durability\n    (<a href=\"https://en.wikipedia.org/wiki/ACID\">ACID</a>), per Wikipedia, &quot;<em>is a set of\n    properties of database transactions intended to guarantee data validity\n    despite errors, power failures, and other mishaps. In the context of\n    databases, a sequence of database operations that satisfies the ACID\n    properties (which can be perceived as a single logical operation on the\n    data) is called a transaction. For example, a transfer of funds from one\n    bank account to another, even involving multiple changes such as debiting\n    one account and crediting another, is a single transaction.</em>&quot;</p>\n"
  },
  {
    "title": "💡 TIL: To Prepare for AI, Study History's Tech Cycles",
    "date": "2025-02-09T00:00:00.000Z",
    "tags": [
      "til",
      "ai",
      "fast-ai",
      "llm",
      "machine-learning",
      "best-practices",
      "decision-making",
      "evolution"
    ],
    "url": "/posts/TIL-prepare-for-ai.html",
    "content": "<p><strong>TL;DR:</strong> Fast.ai founder Jeremy Howard advocates studying historical\ntechnology cycles rather than attempting to predict AI&#39;s future, recommending a\npractical preparation strategy that combines domain expertise with AI\ncapabilities through self-directed learning, side projects, and community\nengagement - emphasising that success will come from embracing uncertainty\nwhilst pursuing counter-cyclical opportunities.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p><a href=\"https://jeremy.fast.ai/\">Jeremy Howard</a> isn&#39;t just another voice in the AI\nconversation. As the creator of\n<a href=\"https://towardsdatascience.com/understanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664\">ULMFiT</a>\n(the algorithm that modern LLMs like ChatGPT are based on), founding researcher\nat <a href=\"https://course.fast.ai/\">fast.ai</a>, and <a href=\"https://www.answer.ai/\">Answer.AI</a>,\nHoward brings a unique perspective shaped by decades at the forefront of AI\ndevelopment. Recently, when\n<a href=\"https://xcancel.com/chrisbarber/status/1888037803566747942\">asked about preparing for AI</a>,\nhis response wasn&#39;t about futuristic predictions or doomsday scenarios. Instead,\nhe offered something more valuable: practical wisdom drawn from historical\npatterns.</p>\n<h2 id=\"why-this-matters-now\">Why This Matters Now</h2>\n<p>We&#39;re at a critical juncture with AI, similar to where we were with the internet\nin 1990. Just as the internet transformed every aspect of our lives, AI is\npoised to do the same. The difference? We can learn from history this time.\nHoward&#39;s insights are particularly valuable because they come from someone who\nhas not only observed but shaped these technological transitions.</p>\n<h2 id=\"key-insights-on-technology-evolution\">Key Insights on Technology Evolution</h2>\n<p>Howard emphasises a crucial pattern: technology doesn&#39;t just grow linearly. Each\ninnovation follows a &quot;hockey stick&quot; growth curve before flattening into a\nsigmoid.</p>\n<center>\n    <figure>\n        <img src=\"https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/0a6eced3bce4c70b7ba715fe7873d1659ce2e9a9/images/hockey-stick-growth.png\" width=\"80%\" height=\"80%\" />\n    <figcaption>Hockey stick growth</figcaption>\n    </figure>\n</center>\n\n<p>More importantly, new &quot;hockey sticks&quot; emerge unexpectedly in different areas.\nThis pattern repeats &quot;like clockwork&quot; making historical understanding more\nvaluable than future predictions.</p>\n<h2 id=\"practical-preparation-strategy\">Practical Preparation Strategy</h2>\n<p>Rather than trying to predict AI&#39;s future, Howard advocates for:</p>\n<ul>\n<li>Embracing uncertainty while avoiding both dismissive fear and blind hype</li>\n<li>Taking a counter-cyclical approach: pursuing opportunities others overlook</li>\n<li>Investing months in mastering AI tools, accepting initial poor results as part\nof the learning process</li>\n<li>Combining AI capabilities with deep domain expertise</li>\n<li>Building practical knowledge through side projects and community engagement</li>\n</ul>\n<h2 id=\"the-education-perspective\">The Education Perspective</h2>\n<p>Howard challenges traditional educational paths, suggesting alternatives:</p>\n<ul>\n<li>Self-directed learning through resources like\n<a href=\"https://course.fast.ai/\">fast.ai</a></li>\n<li>Multiple side hustles to build practical experience</li>\n<li>Community building with like-minded innovators</li>\n<li>Using AI itself to learn technical skills</li>\n<li>Developing both technical and human skills as a generalist</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The key takeaway isn&#39;t about predicting AI&#39;s future -it&#39;s about preparing for it\nintelligently. Howard&#39;s message is: success in the AI era won&#39;t come from\nperfect predictions or traditional career paths. Instead, it will come from\npractical engagement, continuous learning, and the ability to combine domain\nexpertise with AI capabilities. As he puts it, those who master this combination\nwill have &quot;superpowers&quot; compared to those who don&#39;t adapt.<br>The most valuable insight? Even AI experts can&#39;t predict AI&#39;s future reliably.\nThe best strategy is to engage deeply with the technology while maintaining a\ngrounded, practical approach to learning and application. The future belongs to\nthe tinkerers, the experimenters, and those willing to learn from both past and\npresent.</p>\n"
  },
  {
    "title": "🚀 A Minimal, Pragmatic Approach to Production-Ready AI & ML with Go",
    "date": "2025-01-26T00:00:00.000Z",
    "tags": [
      "ai",
      "go",
      "llm",
      "minimal",
      "machine-learning",
      "toolchain",
      "zero-config",
      "code-quality",
      "cross-platform",
      "production"
    ],
    "url": "/posts/go-pragmatic-modern-development.html",
    "content": "<p><strong>TL;DR:</strong> Go offers a refreshingly minimal approach to AI and ML development\nwith its concise 47-page specification, zero-configuration toolchain, and\nfunctional equivalents to key Python ML libraries - providing explicit error\nhandling, enforced code consistency, and cross-platform capabilities whilst\nreducing cognitive overhead and team friction in production environments.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Modern software development often involves navigating complex toolchains,\nopinionated frameworks, and resource-heavy development environments. Many\nlanguages require extensive configuration, multiple runtime dependencies, and\nintroduce significant cognitive overhead through their vast feature sets and\nmultiple approaches to solving the same problem. Node.js, JVM languages, and\neven Python with its extensive ecosystem can lead to analysis paralysis, code\ninhomogeneity and team disagreements over tooling and style.<br>Go offers a refreshing alternative. With a language specification under 50\npages, a consolidated toolchain, and a &quot;batteries included&quot; approach, it\nprovides a low-cognitive-overhead solution for developers seeking simplicity and\nproductivity. Its zero-config philosophy, coupled with built-in formatting\n(<code>go fmt</code>), linting[^1] (<code>go vet</code>), and testing tools, promotes code uniformity\nand reduces team friction over stylistic choices. The sizeable Go community is\ncentralised, using Slack in this case, which serves as a focal point for\ncommunication, support, networking, and staying informed about the latest\ndevelopments.<br>While Go may lack a REPL as sophisticated as IPython or the Julia interactive\nenvironment, this limitation encourages proper Test-Driven Development practices\nrather than the post-implementation testing often seen in REPL-heavy\nenvironments. Tools like <a href=\"https://github.com/fatih/vim-go\">vim-go</a>&#39;s <code>:GoRun</code>\nand Go Playground provide sufficient interactive development capabilities for\nmost use cases.<br>Below I&#39;m collecting some thoughts on attractive aspects of Go I&#39;ve discerned so\nfar and how they compare with other languages I&#39;ve considered. The list of Go&#39;s\nfeatures is far from complete, for example I&#39;ve not mentioned goroutines among\nothers.</p>\n<h2 id=\"python-vs-go-libraries-comparison\">Python vs Go Libraries Comparison</h2>\n<table>\n<thead>\n<tr>\n<th>Domain</th>\n<th>Python Library</th>\n<th>Go Equivalent</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Numerical Computing</td>\n<td><a href=\"https://github.com/numpy/numpy\">NumPy</a></td>\n<td><a href=\"https://github.com/gonum/gonum\">gonum</a></td>\n</tr>\n<tr>\n<td>Data Processing</td>\n<td><a href=\"https://github.com/pandas-dev/pandas\">Pandas</a></td>\n<td><a href=\"https://github.com/go-gota/gota\">gota</a></td>\n</tr>\n<tr>\n<td>Visualisation</td>\n<td><a href=\"https://github.com/plotly/plotly.py\">Plotly</a></td>\n<td><a href=\"https://github.com/MetalBlueberry/go-plotly\">go-plotly</a></td>\n</tr>\n<tr>\n<td>Gradient Boosting</td>\n<td><a href=\"https://github.com/dmlc/xgboost\">XGBoost</a></td>\n<td><a href=\"https://github.com/Unity-Technologies/go-xgboost\">go-xgboost</a></td>\n</tr>\n<tr>\n<td>Machine Learning</td>\n<td><a href=\"https://github.com/scikit-learn/scikit-learn\">Scikit-Learn</a></td>\n<td><a href=\"https://github.com/sjwhitworth/golearn\">golearn</a></td>\n</tr>\n<tr>\n<td>Deep Learning</td>\n<td><a href=\"https://github.com/tensorflow/tensorflow\">TensorFlow</a><br><a href=\"https://github.com/pytorch/pytorch\">PyTorch</a></td>\n<td><a href=\"https://github.com/galeone/tfgo\">tfgo</a><br><a href=\"https://github.com/sugarme/gotch\">gotch</a></td>\n</tr>\n<tr>\n<td>LLM Development</td>\n<td><a href=\"https://github.com/langchain-ai/langchain\">LangChain</a></td>\n<td><a href=\"https://github.com/tmc/langchaingo\">langchaingo</a></td>\n</tr>\n<tr>\n<td>Vector Search</td>\n<td><a href=\"https://github.com/weaviate/weaviate-python-client\">Weaviate Client</a></td>\n<td><a href=\"https://github.com/weaviate/weaviate-python-client\">Weaviate Go Client</a></td>\n</tr>\n</tbody></table>\n<p><em>Update: <a href=\"https://github.com/Promacanthus/awesome-golang-ai\">Awesome Golang.ai</a>\nis a very nice curated list of AI-related Go libraries worth checking.</em></p>\n<h2 id=\"development-experience\">Development Experience</h2>\n<p>Go&#39;s tooling is exceptional. With <a href=\"https://github.com/fatih/vim-go\">vim-go</a> in\n<a href=\"https://neovim.io/\">Neovim</a>, you get immediate access to formatting, linting,\nand code navigation. Unlike JVM languages or JavaScript frameworks that may\nrequire more complex build configurations, Go projects maintain a simple,\npredictable structure thanks to <code>go mod</code>. The <code>go fmt</code> command -triggered on\nsave by default- enforces consistent code style eliminating debates over\nformatting and best practices, while <code>go vet</code> catches common mistakes early.</p>\n<h2 id=\"error-handling-done-right\">Error Handling Done Right</h2>\n<p>Go&#39;s approach to error handling initially feels verbose:</p>\n<pre><code class=\"language-go\">result, err := someFunction()\nif err != nil {\n    return err\n}\n</code></pre>\n<p>But this explicitness pays dividends. By treating errors as values that must be\nhandled, Go forces developers to think about failure cases upfront. The <code>defer</code>\nkeyword complements this by ensuring clean-up code runs regardless of errors:</p>\n<pre><code class=\"language-go\">file, err := os.Open(&quot;data.txt&quot;)\nif err != nil {\n    return err\n}\ndefer file.Close()\n</code></pre>\n<h2 id=\"mlai-capabilities\">ML/AI Capabilities</h2>\n<p>While Go isn&#39;t the primary choice for ML/AI experimentation, its simplicity and\nperformance make it excellent for production deployments. Its standard library\nand growing ecosystem provide solid foundations for numerical computing\n(<a href=\"https://github.com/gonum/gonum\">gonum</a>), data processing\n(<a href=\"https://github.com/go-gota/gota\">gota</a>), and ML/AI applications\n(<a href=\"https://github.com/gorgonia/gorgonia\">Gorgonia</a>,\n<a href=\"https://github.com/galeone/tfgo\">tfgo</a>,\n<a href=\"https://github.com/sugarme/gotch\">gotch</a>). The language&#39;s focus on simplicity\nand performance makes it particularly suitable for model serving and inference\nworkloads.</p>\n<h2 id=\"language-design\">Language Design</h2>\n<p>Go&#39;s refreshingly concise specification (under 50 pages) contrasts sharply with\nother languages. Even the highly promising Zig, a younger language half of Go&#39;s\nage, has a 74-page specification despite being positioned as a simpler low-level\nlanguage.</p>\n<figure>\n    <img src=\"https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/refs/heads/main/images/Zig%20language%20spec.png\" width=\"80%\" height=\"80%\"/>\n    <figcaption>Zig's language spec</figcaption>\n</figure>\n\n<p>Go&#39;s intentionally limited feature set and single way of solving problems\npromote maintainable, uniform code that&#39;s easier to reason about and review, as\nreflected in its compact language spec.</p>\n<figure>\n    <img src=\"https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/refs/heads/main/images/Go%20language%20spec.png\" width=\"80%\" height=\"80%\"/>\n    <figcaption>Go's language spec</figcaption>\n</figure>\n\n<p>For ML engineers and developers seeking a reliable, low-overhead language that\nexcels at building robust, production-ready applications, Go offers a compelling\nchoice. While it won&#39;t replace Python for rapid prototyping and research, its\nsimplicity, performance, and consolidated toolchain make it an very compelling\naddition to any developer&#39;s toolkit.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>To me, Go stands out as a pragmatic choice for modern development through its\nkey strengths:</p>\n<ul>\n<li>Minimal cognitive overhead with a 47-page specification</li>\n<li>Zero-config toolchain including formatting, testing, and package management</li>\n<li>Centralised community, providing a single-source of truth</li>\n<li>Enforced error handling and clean resource management via <code>defer</code></li>\n<li>Growing ML/AI ecosystem comparable to Python&#39;s established libraries</li>\n<li>Cross-platform compilation and efficient garbage collection</li>\n<li>Single, clear way to solve problems, reducing team friction</li>\n<li>Lightweight development environment compared to JVM, .NET, BEAM or Node.js</li>\n</ul>\n<p>While Python remains dominant for ML/AI research, prototyping and -frequently-\nproduction, Go excels in production environments where code maintainability,\nperformance, and team collaboration are crucial. Its intentionally limited\nfeature set, combined with a comprehensive standard library and maturing ML\necosystem, makes it a very attractive choice for developers seeking simplicity\nwithout sacrificing capability.<br>The language&#39;s design philosophy strongly aligns with my needs as a Data\nprofessional looking to reduce tooling complexity and maintain consistent,\nreliable codebases. Go&#39;s lightweight yet rich toolchain allows writing safe,\nefficient AI and data-oriented code based on simplicity and reliability. This\nrefreshing alternative in today&#39;s complex development landscape has strongly\ntempted me to start moving my practice to Go&#39;s more principled approach.</p>\n<hr>\n<p>[^1]: Vet is -in essence- a linter, since it helps improve code quality. Quoting\n    Go&#39;s <a href=\"https://go.dev/src/cmd/vet/doc.go\">vet doc</a> <em>&quot;Vet examines Go source\n    code and reports suspicious constructs, such as Printf calls whose arguments\n    do not align with the format string. Vet uses heuristics that do not\n    guarantee all reports are genuine problems, but it can find errors not\n    caught by the compilers.&quot;</em></p>\n"
  },
  {
    "title": "🔧 A 5-Minute Guide to Engineering Machine Learning Systems",
    "date": "2025-01-21T00:00:00.000Z",
    "tags": [
      "machine-learning",
      "best-practices",
      "mlops",
      "monitoring",
      "production",
      "quality-assurance",
      "data-science",
      "decision-making"
    ],
    "url": "/posts/ml-best-practices.html",
    "content": "<p><strong>TL;DR:</strong> This concise guide distils Google&#39;s 43 machine learning best\npractices into essential principles across four phases: starting with simple\nheuristics before ML, building robust data pipelines, prioritising feature\nengineering over complex algorithms, and gradually introducing complexity only\nafter monitoring systems are established - emphasising engineering excellence\nover ML expertise.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>This is a concise reference guide distilling Martin Zinkevich&#39;s\n<a href=\"https://developers.google.com/machine-learning/guides/rules-of-ml\">influential Google article on machine learning best practices</a>.\nWhile the original spans 43 detailed rules, this 10-minute summary captures the\nessential principles for building production ML systems. Whether you&#39;re starting\na new project or reviewing an existing one, this summary can be used as a\npractical checklist for engineering-focused machine learning.</p>\n<h2 id=\"core-philosophy\">Core Philosophy</h2>\n<blockquote>\n<p>Do machine learning like the great engineer you are, not like the great\nmachine learning expert you aren&#39;t.</p>\n</blockquote>\n<p>Most ML gains come from great features, not algorithms. The basic approach\nshould be:</p>\n<ol>\n<li>Ensure solid end-to-end pipeline</li>\n<li>Start with reasonable objective</li>\n<li>Add common-sense features simply</li>\n<li>Maintain pipeline integrity</li>\n</ol>\n<h2 id=\"phase-i-before-machine-learning-rules-1-3\">Phase I: Before Machine Learning (Rules #1-3)</h2>\n<ol>\n<li><p><strong>Don&#39;t be afraid to launch without ML</strong></p>\n<ul>\n<li>Simple heuristics get you 50% of the way</li>\n<li>Launch with heuristics when data is insufficient</li>\n<li>Example: Use install rate for app ranking</li>\n</ul>\n</li>\n<li><p><strong>First, design and implement metrics</strong></p>\n<ul>\n<li>Track everything possible in current system</li>\n<li>Get early permission from users</li>\n<li>Design systems with metric instrumentation</li>\n<li>Implement experiment framework</li>\n</ul>\n</li>\n<li><p><strong>Choose ML over complex heuristics</strong></p>\n<ul>\n<li>Simple heuristics for launching</li>\n<li>Complex heuristics become unmaintainable</li>\n<li>ML models are easier to maintain long-term</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"phase-ii-first-pipeline-rules-4-11\">Phase II: First Pipeline (Rules #4-11)</h2>\n<ol>\n<li><p><strong>Keep first model simple, get infrastructure right</strong></p>\n<ul>\n<li>Focus on data pipeline integrity</li>\n<li>Define clear evaluation metrics</li>\n<li>Plan model integration carefully</li>\n</ul>\n</li>\n<li><p><strong>Pipeline Health is Critical</strong></p>\n<ul>\n<li>Test infrastructure independently</li>\n<li>Monitor freshness requirements</li>\n<li>Watch for silent failures</li>\n<li>Give feature columns owners</li>\n<li>Document feature expectations</li>\n</ul>\n</li>\n<li><p><strong>Starting Your ML System</strong></p>\n<ul>\n<li>Test getting data into algorithm</li>\n<li>Test getting models out correctly</li>\n<li>Monitor data statistics continuously</li>\n<li>Build alerting system</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"your-first-objective-rules-12-15\">Your First Objective (Rules #12-15)</h2>\n<ol>\n<li><p><strong>Choose Objectives Wisely</strong></p>\n<ul>\n<li>Don&#39;t overthink initial objective choice</li>\n<li>Start with simple, observable metrics</li>\n<li>Use directly observed user behaviours</li>\n<li>Example: clicks, downloads, shares</li>\n</ul>\n</li>\n<li><p><strong>Model Selection Guidelines</strong></p>\n<ul>\n<li>Start with interpretable models</li>\n<li>Separate spam filtering from quality ranking</li>\n<li>Use simple linear models initially</li>\n<li>Make debugging easier</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"phase-iii-feature-engineering-rules-16-22\">Phase III: Feature Engineering (Rules #16-22)</h2>\n<ol>\n<li><p><strong>Plan to launch and iterate</strong></p>\n<ul>\n<li>Expect regular model updates</li>\n<li>Design for feature flexibility</li>\n<li>Keep infrastructure clean</li>\n</ul>\n</li>\n<li><p><strong>Feature Engineering Principles</strong></p>\n<ul>\n<li>Start with directly observed features</li>\n<li>Use cross-product features wisely</li>\n<li>Clean up unused features</li>\n<li>Scale feature complexity with data</li>\n</ul>\n</li>\n<li><p><strong>Feature Coverage and Quality</strong></p>\n<ul>\n<li>Features that generalise across contexts</li>\n<li>Monitor feature coverage</li>\n<li>Document feature ownership</li>\n<li>Regular feature clean-up</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"human-analysis-rules-23-28\">Human Analysis (Rules #23-28)</h2>\n<ol>\n<li><p><strong>Testing and Validation</strong></p>\n<ul>\n<li>Use crowdsourcing or live experiments</li>\n<li>Measure model deltas explicitly</li>\n<li>Look for error patterns</li>\n<li>Consider long-term effects</li>\n</ul>\n</li>\n<li><p><strong>Common Pitfalls</strong></p>\n<ul>\n<li>Engineers aren&#39;t typical users</li>\n<li>Beware of confirmation bias</li>\n<li>Quantify undesirable behaviours</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"training-serving-skew-rules-29-37\">Training-Serving Skew (Rules #29-37)</h2>\n<ol>\n<li><p><strong>Prevent Skew</strong></p>\n<ul>\n<li>Save serving-time features</li>\n<li>Weight sampled data properly</li>\n<li>Reuse code between training/serving</li>\n<li>Test on future data</li>\n</ul>\n</li>\n<li><p><strong>Monitor Everything</strong></p>\n<ul>\n<li>Track performance metrics</li>\n<li>Watch data distributions</li>\n<li>Monitor feature coverage</li>\n<li>Check prediction bias</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"phase-iv-optimisation-and-complex-models-rules-38-43\">Phase IV: Optimisation and Complex Models (Rules #38-43)</h2>\n<ol>\n<li><p><strong>When to Add Complexity</strong></p>\n<ul>\n<li>After simple approaches plateau</li>\n<li>When objectives are well-aligned</li>\n<li>If maintenance cost justifies gains</li>\n</ul>\n</li>\n<li><p><strong>Advanced Techniques</strong></p>\n<ul>\n<li>Keep ensembles simple</li>\n<li>Look for new information sources</li>\n<li>Balance complexity vs. benefits</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"final-recommendations\">Final Recommendations</h2>\n<ol>\n<li><p><strong>Launch Decisions</strong></p>\n<ul>\n<li>Consider multiple metrics</li>\n<li>Use proxies for long-term goals</li>\n<li>Balance simple vs. complex</li>\n</ul>\n</li>\n<li><p><strong>System Evolution</strong></p>\n<ul>\n<li>Start simple, add complexity gradually</li>\n<li>Monitor consistently</li>\n<li>Keep infrastructure clean</li>\n<li>Document everything</li>\n</ul>\n</li>\n</ol>\n"
  },
  {
    "title": "🤖 Understanding AI Agents: Tools, Planning, and Evaluation",
    "date": "2025-01-14T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "prompt-engineering",
      "system-prompts",
      "evaluation",
      "best-practices",
      "toolchain",
      "machine-learning"
    ],
    "url": "/posts/agents-chip-huyen.html",
    "content": "<p><strong>TL;DR:</strong> Chip Huyen&#39;s analysis of AI agents explores how they combine\nfoundation models with specialised tools (knowledge augmentation, capability\nextension, and write actions), planning mechanisms (ReAct, Reflexion), and\nevaluation frameworks to accomplish complex tasks whilst highlighting challenges\nin tool selection, planning efficiency, and error management.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>This article summarises Chip Huyen&#39;s comprehensive blog post\n&quot;<a href=\"https://huyenchip.com//2025/01/07/agents.html\">Agents</a>&quot; adapted from her\nupcoming book AI Engineering (2025). The original piece provides an in-depth\nexamination of intelligent agents, which represent a fundamental concept in AI,\ndefined by Russell and Norvig in their seminal 1995 book\n<a href=\"https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach\">Artificial Intelligence: A Modern Approach</a>\nas anything that can perceive its environment through sensors and act upon it\nthrough actuators. Huyen explores how the unprecedented capabilities of\nfoundational models have transformed theoretical possibilities into practical\napplications, enabling agents to operate in diverse environments -from digital\nworkspaces for coding to physical settings for robotics. These agents can now\nassist with tasks ranging from website creation to complex negotiations.</p>\n<h2 id=\"understanding-agents-and-their-tools\">Understanding Agents and Their Tools</h2>\n<p>An agent&#39;s effectiveness is determined by two key factors: its environment and\nits tool inventory. The environment defines the scope of possible actions, while\ntools enable the agent to perceive and act within this environment. Modern\nagents leverage three distinct categories of tools.<br>Knowledge augmentation tools, including text retrievers and web browsing\ncapabilities, prevent model staleness by enabling access to current information.\nHowever, web browsing tools require careful API selection to protect against\nunreliable or harmful content. Capability extension tools address inherent model\nlimitations -for instance, providing calculators for precise arithmetic or code\ninterpreters for programming tasks. These interpreters demand robust security\nmeasures to prevent code injection attacks.<br>Write actions represent the most powerful and potentially risky category,\nenabling agents to modify databases or send emails. These tools are\ndistinguished from read-only actions by their ability to affect the environment\ndirectly. The <a href=\"https://arxiv.org/abs/2304.09842\">Chameleon</a> system demonstrated\nthe power of tool augmentation, achieving an 11.37% improvement on ScienceQA (a\nscience question answering task) and 17% on TabMWP (a tabular math\nproblem-solving task) through strategic tool combination.</p>\n<center>\n    <figure>\n           <a href=\"https://huyenchip.com//2025/01/07/agents.html\"><img src=\"https://huyenchip.com/assets/pics/agents/8-tool-transition.png\" width=\"80%\" height=\"80%\"/></a>\n        <figcaption>A tool transition tree by Chameleon</figcaption>\n    </figure>\n</center>\n\n<h2 id=\"planning-and-execution-strategies\">Planning and Execution Strategies</h2>\n<p>Effective planning requires balancing granularity and flexibility. While\n<a href=\"https://arxiv.org/abs/2302.04761\">Toolformer</a> managed with 5 tools and\n<a href=\"https://arxiv.org/abs/2304.09842\">Chameleon</a> with 13,\n<a href=\"https://arxiv.org/abs/2305.15334\">Gorilla</a> attempted to handle 1,645 APIs,\nillustrating the complexity of tool selection. Plans can be expressed either in\nnatural language or specific function calls, each approach offering different\nadvantages in maintainability and precision.<br>Foundational Model planners require minimal training but need careful prompting,\nwhile Reinforcement Learning planners demand extensive training for robustness.\nModern planning systems support multiple control flows: sequential, parallel,\nconditional, and iterative patterns. The\n<a href=\"https://arxiv.org/abs/2210.03629\">ReAct</a> framework successfully combines\nreasoning with action,</p>\n<center>\n    <figure>\n        <a href=\"https://huyenchip.com//2025/01/07/agents.html\"><img src=\"https://huyenchip.com/assets/pics/agents/5-ReAct.png\" width=\"80%\" height=\"80%\"/></a>\n        <figcaption>ReAct agent</figcaption>\n    </figure>\n</center>\n\n<p>while <a href=\"https://arxiv.org/abs/2303.11366\">Reflexion</a> separates evaluation and\nself-reflection for improved performance.</p>\n<center>\n    <figure>\n        <a href=\"https://huyenchip.com//2025/01/07/agents.html\"><img src=\"https://huyenchip.com/assets/pics/agents/6-reflexion.png\" width=\"80%\" height=\"80%\"/></a>\n        <figcaption>Reflexion agent</figcaption>\n    </figure>\n</center>\n\n<h2 id=\"reflection-and-error-management\">Reflection and Error Management</h2>\n<p>Continuous reflection and error correction form the backbone of reliable agent\nsystems. The process begins with query validation, continues through plan\nassessment, and extends to execution monitoring. Chameleon&#39;s tool transition\nanalysis shows how tools are commonly used together, while Voyager&#39;s skill\nmanager builds on this by tracking and reusing successful tool combinations.</p>\n<h2 id=\"evaluation-framework\">Evaluation Framework</h2>\n<p>Agent evaluation requires a comprehensive approach to failure mode analysis.\nPlanning failures might involve invalid tools or incorrect parameters, while\ntool-specific failures demand targeted analysis. Efficiency metrics must\nconsider not just step count and costs, but also completion time constraints.\nWhen comparing AI and human agents, it&#39;s essential to recognise their different\noperational patterns -what&#39;s efficient for one may be inefficient for the other.\nWorking with domain experts helps identify missing tools and validate\nperformance metrics.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Huyen&#39;s analysis demonstrates that successful AI agents emerge from the careful\norchestration of three key elements: strategic tool selection, sophisticated\nplanning mechanisms, and robust evaluation frameworks. While tools dramatically\nenhance agent capabilities -as evidenced by Chameleon&#39;s significant performance\nimprovements- their effectiveness depends on thoughtful curation, balancing\nbetween Toolformer&#39;s minimal approach and Gorilla&#39;s extensive API integration.\nThe integration of planning frameworks like ReAct and Reflexion shows how\ncombining reasoning with action and incorporating systematic reflection can\nenhance agent performance. However, as an emerging field without established\ntheoretical frameworks, significant challenges remain in tool selection,\nplanning efficiency, and error management. Future developments will focus on\nagent framework evaluation and memory systems for handling information beyond\ncontext limits, while maintaining the delicate balance between capability and\ncontrol that Huyen emphasises throughout her analysis.</p>\n"
  },
  {
    "title": "💡 TIL: A Simple Yet Effective Ensemble Technique called Model Soup 🍲",
    "date": "2025-01-10T00:00:00.000Z",
    "tags": [
      "neural-network",
      "machine-learning",
      "performance",
      "mlops",
      "production",
      "evaluation"
    ],
    "url": "/posts/TIL-model-soups.html",
    "content": "<p><strong>TL;DR:</strong> Model soups provide a computationally efficient ensemble technique by\naveraging the weights of similarly trained neural networks, outperforming both\nindividual models and traditional prediction-averaging ensembles while\nmaintaining single-model inference speed.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>While most ensemble methods in machine learning combine model predictions,\nthanks to\n<a href=\"https://bsky.app/profile/chrisalbon.com/post/3lfbbixka7c25\">Chris Albon</a> I\nrecently learned about an alternative approach called &quot;<em>model soups</em>&quot; that works\ndirectly with model parameters. Instead of aggregating outputs, model soups\nblend the actual weights and biases of neural networks, showing promising\nresults in computer vision and language tasks.</p>\n<center>\n   <a href=\"https://bsky.app/profile/chrisalbon.com/post/3lfbbixka7c25\"><img src=\"https://cdn.bsky.app/img/feed_fullsize/plain/did:plc:umpsiyampiq3bpgce7kigydz/bafkreihvr4b4gid7v6y7karhiusawtqfdbhoen2bt6q55pmugyioj3q3gq@jpeg\" width=\"80%\" height=\"80%\"/></a>\n</center>\n\n<h2 id=\"main-concept\">Main Concept</h2>\n<p>Model soups are created by averaging the parameters (weights and biases) of\nmultiple independently trained neural networks that share the same architecture\nand training setup. For example, if we have three models with weights 2.32,\n4.21, and 1.23 for a particular parameter, the &quot;souped&quot; model would use (2.32 +\n4.21 + 1.23) / 3 = 2.587 for that parameter. This process is repeated across all\nparameters in the network. However, not all parameter combinations lead to\nimprovements -models typically need similar training datasets, optimisation\nmethods, and hyperparameters (like learning rate and batch size) to blend\neffectively. When done right, parameter-averaged models can outperform both\nindividual networks and traditional prediction-averaging ensembles, while\nmaintaining the inference speed of a single model.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Model soups challenge our intuitions about neural networks by showing that\ndirectly averaging weights can produce better results than averaging\npredictions. While the technique requires careful consideration of training\nconditions, it provides a computationally efficient way to combine multiple\nmodels into a single network, making it particularly valuable for\nresource-constrained production environments where running multiple models in\nparallel isn&#39;t feasible.</p>\n"
  },
  {
    "title": "🔍 Understanding LLM Interpretability",
    "date": "2025-01-09T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "machine-learning",
      "neural-network",
      "model-governance",
      "interpretability"
    ],
    "url": "/posts/interpreting-llms.html",
    "content": "<p><strong>TL;DR:</strong> LLMs present unique interpretability challenges due to neurons\nexhibiting polysemanticity - responding to multiple unrelated concepts through\nsuperposition - which sparse autoencoders help address by mapping neuron\ncombinations to specific concepts, enhancing our ability to understand, control,\nand improve these increasingly influential AI systems.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Large Language Models (LLMs) have become increasingly sophisticated, yet\nunderstanding their inner workings remains a critical challenge for AI safety\nand development. This blog post summarises concepts and research presented in\n<a href=\"https://www.youtube.com/watch?v=UGO_Ehywuxc\">Welch Labs&#39; video on mechanistic interpretability</a>,\nexamining how LLMs process information and recent advances in making their\ndecision-making processes more transparent.</p>\n<h2 id=\"how-llms-think\">How LLMs Think</h2>\n<p>LLMs process text through a sophisticated pipeline:</p>\n<ol>\n<li>Text is converted into tokens and mapped to vectors</li>\n<li>These vectors flow through multiple layers via &quot;<em>residual streams</em>&quot;</li>\n<li>Each layer transforms the information through attention mechanisms</li>\n<li>Final outputs emerge from probability distributions across possible tokens</li>\n</ol>\n<p>This process, while mathematically precise, creates a black box of neural\nconnections that resist simple interpretation.</p>\n<h2 id=\"the-challenge-of-model-transparency\">The Challenge of Model Transparency</h2>\n<p><a href=\"https://ai.google.dev/gemma\">Google Gemma</a> models&#39; analysis of the sentence\n&quot;<em>the reliability of Wikipedia is very</em>&quot; demonstrates this complexity. The model\nassigns varying probabilities to different completions:</p>\n<ul>\n<li>&quot;<em>important</em>&quot; (20.21%)</li>\n<li>&quot;<em>high</em>&quot; (11.16%)</li>\n<li>&quot;<em>questionable</em>&quot; (9.48%)</li>\n</ul>\n<p>These probabilities emerge from intricate interactions between neurons, leading\nto a phenomenon called <em>superposition</em>[^1].</p>\n<h2 id=\"superposition-and-its-solution\">Superposition and Its Solution</h2>\n<p>Unlike vision models where neurons correspond to specific concepts, LLMs exhibit\n<a href=\"https://arxiv.org/abs/2210.01892\">polysemanticity</a> -individual neurons respond\nto multiple, unrelated concepts. This occurs because LLMs encode more concepts\nthan available neurons by using specific neuron combinations.</p>\n<p>This complexity necessitated the development of [sparse autoencoders]({{\nsite.baseurl }}{% link _posts/2025-01-09-sparse-autoencoders.md %}), which:</p>\n<ol>\n<li>Map complex neuron combinations to specific concepts</li>\n<li>Extract interpretable features from LLMs</li>\n<li>Enable direct manipulation of model behaviour</li>\n</ol>\n<h2 id=\"practical-implications\">Practical Implications</h2>\n<p>Understanding LLM internals has crucial implications:</p>\n<ul>\n<li><strong>AI Safety</strong>: Better control over model behaviours and outputs</li>\n<li><strong>Development</strong>: More targeted improvements in model capabilities</li>\n<li><strong>Deployment</strong>: Enhanced ability to predict and prevent unwanted behaviours</li>\n<li><strong>Trust</strong>: Greater transparency in AI decision-making processes</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>While tools like sparse autoencoders have provided unprecedented insights into\nmodel behaviour, they&#39;ve also revealed the vast complexity of LLM internal\nmechanisms -the &quot;dark matter&quot; of AI. As these models become more integral to\nsociety, advancing our ability to interpret and control them becomes\nincreasingly critical for responsible AI development.<br>This improved understanding represents not just academic progress, but a crucial\nstep toward safer, more reliable AI systems.</p>\n<hr>\n<p>[^1]: superposition in the context of neural networks is the ability of a single\n    neuron to represent multiple features simultaneously.\n    <a href=\"https://hdl.handle.net/1721.1/157073\">https://hdl.handle.net/1721.1/157073</a></p>\n"
  },
  {
    "title": "📐 Sparse Autoencoders: A Technical Overview",
    "date": "2025-01-09T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "neural-network",
      "machine-learning",
      "data-science",
      "linear-algebra",
      "statistics",
      "evaluation",
      "interpretability",
      "modelling-mindsets",
      "design-principles",
      "best-practices",
      "data-processing"
    ],
    "url": "/posts/sparse-autoencoders.html",
    "content": "<p><strong>TL;DR:</strong> Sparse autoencoders are neural networks that learn efficient data\nrepresentations by reconstructing their input while enforcing neuron inactivity\nconstraints, combining reconstruction error, weight decay, and KL-divergence\nsparsity penalties to automatically extract interpretable features without\nmanual engineering.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Supervised learning has achieved remarkable successes in areas ranging from\ncomputer vision to genomics. However, as Andrew Ng points out in his\n<a href=\"https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf\">CS294A lecture notes</a>,\nit faces a fundamental limitation: the need for manually engineered features.\nWhile researchers have spent years crafting specialised features for vision,\naudio, and text processing, this approach neither scales nor generalises well.\nSparse autoencoders offer an elegant solution to this challenge by automatically\nlearning features from unlabelled data. These neural networks are distinguished\nby two key characteristics:</p>\n<ol>\n<li>They attempt to reconstruct their input, forcing them to capture essential\ndata patterns</li>\n<li>They employ a sparsity constraint that mimics biological neural systems,\nwhere neurons fire infrequently and selectively</li>\n</ol>\n<p>While simple implementations may not outperform hand-engineered features in\nspecific domains like computer vision, their strength lies in their generality\nand biological plausibility. The sparse coding principle has proven effective\nacross diverse domains including audio, text, and visual processing.<br>The mathematical framework combines reconstruction error, regularisation, and\nsparsity penalties to learn efficient, interpretable representations. This\napproach not only advances machine learning capabilities but also provides\ninsights into how biological neural networks might learn and process\ninformation. This overview examines the mathematical foundations, practical\nimplementation, and emergent properties of sparse autoencoders, following the\nframework presented in Stanford&#39;s CS294A course notes.</p>\n<h2 id=\"sparse-autoencoders\">Sparse Autoencoders</h2>\n<p>An autoencoder is a neural network that learns to reconstruct its input. In a\nsparse autoencoder, we add a critical biological constraint: neurons should be\n&quot;inactive&quot; most of the time, mimicking how biological neurons exhibit low\naverage firing rates.<br>The basic architecture is:</p>\n<pre><code>Input (x) -&gt; Hidden Layer (sparse activation) -&gt; Output (x̂)\n</code></pre>\n<p>Where:</p>\n<ul>\n<li>Input and output dimensions are equal $(x, \\hat{x} \\in \\R^n)$</li>\n<li>Hidden layer learns a sparse representation</li>\n<li>Network uses sigmoid activation: $f(z) = \\frac{1}{1+e^{-z}}$</li>\n</ul>\n<h2 id=\"mathematical-framework\">Mathematical Framework</h2>\n<ol>\n<li><p><strong>Base Cost Function</strong> (single training example):</p>\n<p>$$\n J(W,b; x,y) = \\frac{1}{2}||h_{W,b}(x) - y||^2\n $$</p>\n<p>For a single training example:\\</p>\n<ul>\n<li>Measures reconstruction error between network output $h_{W,b}(x)$ and\ntarget $y$\\</li>\n<li>For autoencoders: $y = x$ (we reconstruct the input)\\</li>\n<li>$\\frac{1}{2}$ factor simplifies gradient computations\\</li>\n<li>Squared L2 norm penalises larger reconstruction errors quadratically</li>\n</ul>\n</li>\n<li><p><strong>Full Cost Function with Weight Decay</strong>:</p>\n<p>The cost function $J(W,b)$ combines the average reconstruction error<br>$\\frac{1}{m}\\sum_{i=1}^m \\frac{1}{2}||h_{W,b}(x^{(i)}) - x^{(i)}||^2$</p>\n<p>with the weight decay regularisation, to prevent overfitting by penalising\nlarge weights:<br>$\\frac{\\lambda}{2}\\sum_{l=1}^{n_l-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(W_{ji}^{(l)})^2$</p>\n<p>$$\n J(W,b) = \\left[\\frac{1}{m}\\sum_{i=1}^m \\frac{1}{2}||h_{W,b}(x^{(i)}) - y^{(i)}||^2\\right] + \\frac{\\lambda}{2}\\sum_{l=1}^{n_l-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(W_{ji}^{(l)})^2\n $$</p>\n<p>Key points:</p>\n<ul>\n<li>For autoencoders, output $y^{(i)}$ equals input $x^{(i)}$</li>\n<li>Weight decay applies only to weights $W$, not biases $b$</li>\n<li>$\\lambda$ balances reconstruction accuracy vs. weight magnitude</li>\n<li>The $\\frac{1}{2}$ factor simplifies derivative calculations in\nbackpropagation</li>\n<li>This regularisation is distinct from the sparsity constraint (KL divergence\nterm)</li>\n</ul>\n</li>\n<li><p><strong>Sparsity Measurement</strong>:</p>\n<p>The average activation $\\hat{\\rho}_j$ measures how frequently hidden unit $j$\nfires across the training set:</p>\n<p>$$\n \\hat{\\rho}<em>j = \\frac{1}{m}\\sum</em>{i=1}^m[a_j^{(2)}(x^{(i)})]\n $$</p>\n<p>Key points:</p>\n<ul>\n<li>$a_j^{(2)}(x^{(i)})$ is hidden unit $j$&#39;s activation for input $x^{(i)}$</li>\n<li>With sigmoid activation:<ul>\n<li>Values near 1 mean &quot;active&quot; or &quot;firing&quot;</li>\n<li>Values near 0 mean &quot;inactive&quot;</li>\n</ul>\n</li>\n<li>We constrain $\\hat{\\rho}_j \\approx \\rho$ where $\\rho$ is small (typically\n0.05)</li>\n<li>This enforces selective firing: each neuron responds strongly to specific\ninput patterns</li>\n</ul>\n</li>\n<li><p><strong>Sparsity Penalty</strong> (using\n<a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\">KL divergence</a>):</p>\n<p>The sparsity penalty uses KL divergence to enforce\n$$\\hat{\\rho}_j \\approx \\rho$$\n:</p>\n<p>$$\n \\sum_{j=1}^{s_2}\\rho\\log\\frac{\\rho}{\\hat{\\rho}_j} + (1-\\rho)\\log\\frac{1-\\rho}{1-\\hat{\\rho}_j}\n $$</p>\n<p>Properties of this penalty:</p>\n<ul>\n<li>Minimised (zero) when $\\hat{\\rho}_j = \\rho$</li>\n<li>Monotonically increases as $\\hat{\\rho}_j$ deviates from $\\rho$</li>\n<li>Becomes infinite as $\\hat{\\rho}_j$ approaches 0 or 1</li>\n</ul>\n</li>\n<li><p><strong>Final Cost Function</strong>:</p>\n<p>$$\n J_{sparse}(W,b) = J(W,b) + \\beta\\sum_{j=1}^{s_2}KL(\\rho||\\hat{\\rho}_j)\n $$</p>\n<p>Components:</p>\n<ul>\n<li>$J(W,b)$: Standard autoencoder cost (reconstruction error + weight decay)</li>\n<li>Sparsity term: KL divergence penalty summed over $s_2$ hidden units</li>\n</ul>\n<p>$\\beta$ controls:</p>\n<ul>\n<li>Balance between accurate reconstruction and sparse representation</li>\n<li>Strength of sparsity enforcement</li>\n<li>Higher $\\beta$ → stronger sparsity constraint</li>\n</ul>\n<p>This formulation naturally penalises both over- and under-activation of\nhidden units relative to target sparsity $\\rho$.</p>\n</li>\n</ol>\n<h2 id=\"training-process\">Training Process</h2>\n<p>The key modification to standard backpropagation occurs in the hidden layer:</p>\n<p>$$\n\\delta_i^{(2)} = \\left(\\sum_{j=1}^{s_3}W_{ji}^{(3)}\\delta_j^{(3)}\\right)f&#39;(s_i^{(2)}) + \\beta\\left(-\\frac{\\rho}{\\hat{\\rho}_i} + \\frac{1-\\rho}{1-\\hat{\\rho}_i}\\right)\n$$</p>\n<p>Where:</p>\n<ul>\n<li>First term: Standard backpropagation gradient through the network</li>\n<li>Second term: Gradient of KL-divergence sparsity penalty</li>\n<li>$s_i^{(2)}$ is weighted input sum to hidden unit $i$</li>\n<li>$\\hat{\\rho}_i$ must be pre-computed using full training set</li>\n</ul>\n<p>This modification ensures gradient descent optimises both reconstruction\naccuracy and sparsity.</p>\n<h2 id=\"practical-guidelines\">Practical Guidelines</h2>\n<ul>\n<li>$\\rho$ ≈ 0.05 (5% target activation rate)</li>\n<li>$\\beta$ controls sparsity penalty strength</li>\n<li>Initialise weights randomly near zero</li>\n<li>Must compute forward pass on all examples first to calculate $\\hat{\\rho}$</li>\n</ul>\n<h2 id=\"results\">Results</h2>\n<p>When trained on images, the network naturally learns edge detectors at different\norientations, similar to what is found in the visual cortex. This emergence of\nbiologically plausible features validates the sparsity approach.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Sparse autoencoders represent a mathematically principled approach to\nunsupervised feature learning, combining biological inspiration with rigorous\noptimisation techniques. Their key innovation lies in the sparsity constraint,\nimplemented through KL divergence, which forces hidden units to develop\nspecialised, interpretable features.</p>\n<p>The mathematical framework achieves this through three key components:</p>\n<ol>\n<li>A reconstruction cost that ensures faithful data representation</li>\n<li>A weight decay term that prevents overfitting</li>\n<li>A sparsity penalty that enforces selective neural activation</li>\n</ol>\n<p>This formulation has proven successful in practice, typically leading to:</p>\n<ul>\n<li>Edge and feature detectors emerging naturally from visual data</li>\n<li>Interpretable representations comparable to biological neural coding</li>\n<li>Robust feature learning even with\n<a href=\"https://en.wikipedia.org/wiki/Overcompleteness\">overcomplete</a> hidden layers</li>\n</ul>\n<p>The practical value of sparse autoencoders extends beyond their theoretical\nelegance -they provide a foundation for understanding how neural networks can\nlearn meaningful data representations without supervision. Their success in\nlearning biologically plausible features validates both their design principles\nand their potential for advanced machine learning applications. Their main\nlimitation lies in hyperparameter sensitivity, particularly to the sparsity\ntarget ρ and weight β, requiring careful tuning for optimal performance.</p>\n"
  },
  {
    "title": "💡 TIL: How Different Societies View and Value Choice",
    "date": "2025-01-08T00:00:00.000Z",
    "tags": [
      "til",
      "decision-making",
      "best-practices",
      "evaluation",
      "statistics",
      "design-principles",
      "modelling-mindsets"
    ],
    "url": "/posts/TIL-the-art-of-choice.html",
    "content": "<p><strong>TL;DR:</strong> Sheena Iyengar&#39;s cross-cultural research reveals that how we perceive\nand respond to choice varies dramatically between societies - with evidence\nshowing that sometimes having fewer choices or allowing others to choose for us\ncan lead to better outcomes, challenging the widely-held Western belief that\nmore individual choice is always beneficial.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Today I revisited a talk on\n<a href=\"https://www.youtube.com/watch?v=lDq9-QxvsNU\">the art of choosing</a> by Sheena\nIyengar. A humourous and informative presentation, it reminded me that our\nassumptions about choice –as studied by Prof. Iyengar through research spanning\nAmerican, European and Asian populations– reveals fascinating cultural\ndifferences in how we perceive and respond to choice. Her research reveals some\neye-opening insights that I&#39;ll briefly summarise below.</p>\n<h2 id=\"perceiving-choice\">Perceiving Choice</h2>\n<p>First, while Americans believe individual choice is sacred (think &quot;have it your\nway&quot;), research shows this isn&#39;t universal. When studying children solving\npuzzles, Asian-American children actually performed better when their mothers\nchose for them, while Anglo-American children did better choosing for\nthemselves. This reveals how deeply cultural context shapes not just our\npreferences, but the actual effectiveness of our choices.</p>\n<p>Second, remember how overwhelming it feels staring at 50 different breakfast\ncereals? Turns out, people from post-communist countries often saw seven\ndifferent sodas as just one choice: &quot;soda or no soda.&quot; This isn&#39;t because\nthey&#39;re less sophisticated, it&#39;s because the ability to spot tiny differences\nbetween products is a learned skill -not a natural one.</p>\n<p>Most striking was the research on medical decisions. When comparing American and\nFrench parents making end-of-life decisions for infants, American parents had\nmore negative emotions and guilt despite insisting on having the choice, while\nFrench parents, whose doctors made the decisions, coped better. This challenges\nthe core American belief that having choice is always better.</p>\n<p>Concluding with a personal story, Prof. Iyengar -who is blind- shared how she\nonce brought two &quot;clearly different&quot; shades of pink nail polish to her lab. When\nshe removed the labels, half the participants couldn&#39;t tell them apart. Those\nwho could, chose differently when the labels were present versus absent, showing\nhow marketing narratives shape what we think we&#39;re choosing.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The TL;DR is: Through cross-cultural research, Prof. Iyengar shows that how we\nunderstand and value choice varies dramatically across cultures. Sometimes,\nhaving fewer choices or letting others choose for us might actually lead to\nbetter outcomes.<br>As a technologist, inundated with a very wide choice of tools that often offer\nsimilar results, I have made the conscious decision to reduce my tooling\nfootprint to the minimum viable toolstack possible. I&#39;m happy to let more\nknowledgeable professionals choose, with <em>adequate justification</em>, tools for my\nline of work but I do disagree with the zealotry that&#39;s occasionally observed in\ntech and complemented by big egos.</p>\n"
  },
  {
    "title": "💡 TIL: The Matrix Equation That Makes Linear Regression Work",
    "date": "2025-01-08T00:00:00.000Z",
    "tags": [
      "data-science",
      "machine-learning",
      "statistics",
      "ai",
      "linear-algebra",
      "til",
      "modelling-mindsets",
      "data-modeling"
    ],
    "url": "/posts/TIL-lin-alg-applied-to-stats.html",
    "content": "<p><strong>TL;DR:</strong> Linear regression can be elegantly solved using the matrix equation β\n= (X^TX)^(-1)X^Ty, which mathematically guarantees minimum squared error by\naccounting for feature correlations - though real-world applications often\nfavour gradient descent due to the direct solution&#39;s computational complexity,\nnumerical instability with correlated features, and memory constraints.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>This morning\n<a href=\"https://xcancel.com/andrew_n_carr/status/1876855682529480844\">an interesting interview question</a>\nmotivated me to remind myself how it&#39;s possible to solve linear regression\nthrough matrix algebra. Below is what I learned:</p>\n<h2 id=\"the-theory-an-elegant-mathematical-solution\">The Theory: An Elegant Mathematical Solution</h2>\n<p>Linear regression finds the best-fit line through data points by finding optimal\ncoefficients ($\\beta$) that minimise squared errors. The equation\n$\\beta = (X^TX)^{-1}X^Ty$ elegantly solves this optimisation problem using\nmatrix algebra.</p>\n<p>The solution involves these key components:</p>\n<ol>\n<li>$X$ is our feature matrix (n samples × p features)</li>\n<li>$y$ is our target values (n × 1)</li>\n<li>$X^T$ is the transpose of X</li>\n<li>$\\beta$ is our solution vector (p × 1) of coefficients</li>\n</ol>\n<p>Here&#39;s how this elegant solution works:</p>\n<ol>\n<li><p>$X^TX$ creates a $(p \\times p)$ matrix of feature products:</p>\n<ul>\n<li>Each element $(i,j)$ contains the dot product between features $i$ and $j$</li>\n<li>When features are centred, these products are proportional to covariances[^1]</li>\n<li>When features are also standardised, it yields correlations scaled by $n$</li>\n</ul>\n</li>\n<li><p>$(X^TX)^{-1}$ computes the inverse of this matrix:</p>\n<ul>\n<li>Compensates for feature correlations in coefficient calculations[^2]</li>\n<li>Required for solving the normal equations $X^TX\\beta = X^Ty$</li>\n<li>Exists only when no feature is a linear combination of others</li>\n</ul>\n</li>\n<li><p>$X^Ty$ creates a $(p \\times 1)$ vector of feature-target products:</p>\n<ul>\n<li>Each element $i$ contains the dot product of feature $i$ with target $y$</li>\n<li>Represents raw feature-target relationships before adjustment</li>\n<li>When centred, proportional to feature-target covariances[^3]</li>\n</ul>\n</li>\n<li><p>Final multiplication $(X^TX)^{-1}X^Ty$:</p>\n<ul>\n<li>Solves the normal equations $X^TX\\beta = X^Ty$</li>\n<li>Accounts for inter-feature correlations in determining coefficients</li>\n<li>Mathematically guarantees minimum squared error</li>\n</ul>\n</li>\n</ol>\n<p>For more information, check Hastie, Tibshirani &amp; Friedman&#39;s\n&quot;<a href=\"https://archive.org/details/elementsofstatis0000hast\">Elements of Statistical Learning</a>&quot;\nseminal book.</p>\n<h2 id=\"the-real-world-catch\">The Real-World Catch</h2>\n<p>While mathematically elegant, this direct solution has practical limitations in\nreal-world applications:</p>\n<ol>\n<li><em>Computational Complexity</em>: Computing $(X^TX)^{-1}$ requires $\\Omicron(n^3)$\noperations, becoming prohibitively expensive for large feature sets. This is\nwhy gradient descent, with its $\\Omicron(n^2)$ per-iteration complexity,\noften proves more practical.</li>\n<li><em>Numerical Instability</em>: When features are highly correlated (like monthly\nand annual income), $X^TX$ becomes nearly singular[^3]. Even small rounding\nerrors in the computation of its inverse can lead to large errors in $\\beta$.\nIn extreme cases, when features are perfectly correlated, the inverse doesn&#39;t\nexist at all. Gradient descent avoids this matrix inversion entirely.</li>\n<li><em>Memory Constraints</em>: Large datasets require holding the entire $X^TX$ matrix\nin memory, while gradient descent can work with mini-batches, making it more\nmemory-efficient.</li>\n</ol>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>While this equation brilliantly demonstrates the power of linear algebra in\nstatistics, real-world machine learning often favours gradient descent&#39;s\niterative approach. Think of it as choosing between a perfect GPS route through\nheavy traffic (direct solution) versus taking smaller, adaptable steps through\nclear side streets (gradient descent). Both reach the same destination, but the\npractical path often wins in real-world conditions.</p>\n<hr>\n<p>[^1]: When features are centred (mean = 0), each product becomes $n$ times the\n    covariance. This means $X^TX$ captures how features vary together, which is\n    crucial because correlated features can lead to unstable coefficients if not\n    accounted for. The relationship between $X^TX$ and covariance comes from the\n    definition of sample covariance:\n    $cov(X_i, X_j) = \\frac{1}{n-1}\\sum_{k=1}^n (x_{ki} - \\bar{x_i})(x_{kj} - \\bar{x_j})$.\n    When data is centred, this simplifies to $\\frac{1}{n-1}(X^TX)_{ij}$.\n    $\\frac{X^TX}{n-1}$ returns the sample covariance matrix. This matters\n    because a) when features are uncentred, $(X^TX)$ gives the sum of products,\n    b) when centred $\\frac{X^TX}{n-1}$ gives covariances, c) when also\n    standardised (std = 1), $\\frac{X^TX}{n-1}$ gives correlations.</p>\n<p>[^2]: Adjusts coefficient estimates to account for shared information between\n    features. For example, if height and weight are correlated, we need to\n    determine each variable&#39;s unique contribution to the prediction, not their\n    overlapping effect.</p>\n<p>[^3]: When centred, each element becomes $n$ times the covariance between a\n    feature and the target. This reveals how each feature individually relates\n    to $y$ before accounting for other features&#39; effects, providing a starting\n    point for determining final coefficients.</p>\n<p>[^3]: A matrix is singular (or non-invertible) when its determinant is zero. In\n    practical terms, this means one or more columns can be expressed as linear\n    combinations of other columns.</p>\n"
  },
  {
    "title": "💊 Lessons for Modern Drug Development from the Golden Age of Antibiotics",
    "date": "2025-01-07T00:00:00.000Z",
    "tags": [
      "iterative-refinement",
      "evolution",
      "data-science",
      "evaluation",
      "decision-making",
      "best-practices",
      "modelling-mindsets",
      "production"
    ],
    "url": "/posts/golden-age-of-antibiotics.html",
    "content": "<p><strong>TL;DR:</strong> Despite our vastly superior modern technology, antibiotic development\nhas dramatically declined since the post-WWII &quot;Golden Age&quot; (1940s-1960s) that\nproduced most antibiotic classes we still use today-highlighting how scientific\ncapability alone cannot drive progress without three key elements working\ntogether: economic incentives that correct market failures, institutional\ncoordination, and systematic application of technological tools.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>In a thought-provoking analysis[^1], Our World in Data reveals a striking\nparadox in medical progress: the most productive period in antibiotic\ndevelopment occurred in the two decades following World War II, with scientific\ncapabilities far more limited than today. This &quot;Golden Age of Antibiotics&quot;\n(1940s-1960s) produced nearly two-thirds of the antibiotic drug classes we still\nrely on[^2].<br>Even more surprisingly, since 1970 -despite exponential advances in computing\npower and biotechnology- only eight new classes of antibiotics have been\napproved[^2]. This indicates a stark decline that threatens the foundation of\nmodern medicine. Traditional screening methods now rediscover existing compounds\nmost of the time rather than finding new ones[^3].<br>Modern tools like genome sequencing and systematic screening methods offer\nunprecedented capabilities. We&#39;ve only identified a small fraction of bacterial\nspecies, many of which could harbour new antibiotic compounds[^2]. Yet despite\nthese capabilities, development has stagnated due to fundamental market failures\nand fragmented research efforts.<br>This article examines this paradoxical inverse relationship between\ntechnological capability and antibiotic development: How did the Golden Age\nachieve such remarkable success with limited tools? Why has progress slowed as\nour capabilities have grown? Most importantly, what combinations of economic\nincentives and modern technology could spark a new era of antibiotic discovery?</p>\n<h2 id=\"when-urgency-met-innovation\">When Urgency Met Innovation</h2>\n<p>The Golden Age of Antibiotics stands as medicine&#39;s most productive period in\nantimicrobial discovery, yielding over 20 new antibiotic classes -more than\ndouble what we&#39;ve developed in the 50 years since[^2]. Three pivotal\nbreakthroughs, coupled with unprecedented coordination, drove this remarkable\nsuccess.<br>The foundation was laid by Paul Ehrlich&#39;s systematic approach to drug discovery.\nBy methodically testing hundreds of compounds, he discovered\n<a href=\"https://en.wikipedia.org/wiki/Arsphenamine\">salvarsan</a> in 1910 -the first\nsynthetic antibiotic that effectively treated syphilis[^2]. A second milestone\nemerged when Alexander Fleming discovered penicillin in 1928. However, the real\ninnovation came through coordinated wartime effort. With infections being the\nsecond-most common cause of hospital admissions in the US Army, the U.S. Office\nof Scientific Research and Development (OSRD) launched a global search for more\nproductive penicillin strains, ultimately finding a high-yielding strain on a\ncantaloupe[^4].<br>The third breakthrough came from Selman Waksman&#39;s insight into soil bacteria.\nHis discovery that soil-dwelling\n<a href=\"https://en.wikipedia.org/wiki/Actinomycetales\">actinomycetes</a> bacteria\nnaturally produce antibiotics led to\n<a href=\"https://en.wikipedia.org/wiki/Streptomycin\">streptomycin</a>&#39;s development and\nopened an entirely new avenue for antibiotic discovery[^5].<br>What transformed these breakthroughs into a &quot;golden age&quot; was unprecedented\ncoordination. The U.S. War Production Board orchestrated collaboration between\ngovernment, academia, and industry -removing patent restrictions, sharing data,\nand streamlining clinical trials[^6]. The results were remarkable: some\nantibiotics, like\n<a href=\"https://en.wikipedia.org/wiki/Tetracycline_antibiotics\">tetracyclines</a> and\n<a href=\"https://en.wikipedia.org/wiki/Macrolide\">macrolides</a>, went from discovery to\nclinical use within the same year.</p>\n<h2 id=\"scientific-progress-and-market-failure\">Scientific Progress and Market Failure</h2>\n<p>The contrast between the Golden Age and our current era reflects a fundamental\nmisalignment between public health needs and market incentives[^2]. The market\nstructure fundamentally disfavours antibiotics in two ways:</p>\n<ol>\n<li>Revenue Structure: While chronic disease medications can generate billions in\nannual revenue over decades, new antibiotics typically generate only tens of\nmillions annually[^7], by comparison. This revenue gap has driven many large\npharmaceutical companies away from antibiotic development[^8].</li>\n<li>Conservation Requirements: New antibiotics must be reserved for severe\ndrug-resistant infections, reaching less than 1% of hospitalised\npatients[^7]. This necessary conservation practice severely limits market\npotential.</li>\n</ol>\n<p>Meanwhile, our technological capabilities offer three particularly promising\napproaches:</p>\n<ol>\n<li>Genome mining: a breakthrough technique that identifies hidden antibiotic\ngenes in microbes that remain dormant under standard laboratory conditions.\nThis computational approach has already yielded promising candidates like\nhumimycins[^9].</li>\n<li>Advanced bacterial exploration: research into extreme environments like deep\noceans and deserts, where previously &quot;unculturable&quot; bacteria might harbour\nentirely new antibiotic classes[^3].</li>\n<li>Smart combination strategies: exploiting the observation that bacterial\nresistance to one antibiotic can increase vulnerability to others, opening\nnew therapeutic possibilities[^10].</li>\n</ol>\n<p>Yet these powerful tools remain underutilised due to insufficient investment and\ncoordination. The challenge isn&#39;t scientific capability -it&#39;s the failure to\ncreate systems that effectively deploy these technologies within sustainable\neconomic frameworks.</p>\n<h2 id=\"integrating-economics-and-technology\">Integrating Economics and Technology</h2>\n<p>Drawing from evidence in antibiotic development research, several promising\napproaches could help overcome current market failures while leveraging modern\ntechnological capabilities[^7].</p>\n<h3 id=\"economic-solutions-to-market-failures\">Economic Solutions to Market Failures</h3>\n<ol>\n<li>Subscription Models: The UK has pioneered a system where healthcare systems\npay annual fees for antibiotic access rather than per-volume pricing. This\naddresses both the revenue challenge and conservation requirements by\nproviding stable income while supporting appropriate antibiotic use[^7].</li>\n<li>Advance Market Commitments: These provide guaranteed payments to companies\nthat successfully develop new antibiotics, similar to successful vaccine\ndevelopment programs. This directly addresses the revenue uncertainty that\nhas driven companies away from antibiotic development[^11].</li>\n<li>Collaborative Funding Initiatives: Organisations like CARB-X and GARDP help\nsmaller companies navigate costly clinical trials, distributing development\nrisks that large pharmaceutical companies are unwilling to bear[^7].</li>\n</ol>\n<h3 id=\"leveraging-modern-technology\">Leveraging Modern Technology</h3>\n<p>To maximise the impact of these economic incentives, three technological\napproaches show particular promise:</p>\n<ol>\n<li>Systematic Genome Mining: Using computational power to identify promising\nantibiotic-producing genes in bacterial genomes, revealing compounds that\ntraditional screening would miss[^9].</li>\n<li>Environmental Exploration: Research into extreme environments could unlock\nentirely new antibiotic classes, enabled by modern sequencing\ntechnologies[^3].</li>\n<li>Smart Combination Strategies: Systematic exploration of how resistance to one\nantibiotic can increase vulnerability to others, offering new therapeutic\npossibilities[^10].</li>\n</ol>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The story of antibiotic development demonstrates that scientific capability\nalone cannot drive progress. The Golden Age succeeded through a powerful\ncombination of systematic approaches, unprecedented collaboration, and removal\nof institutional barriers -even with limited technological tools[^2].<br>Today&#39;s challenge is fundamentally different. We possess sophisticated tools\n-from genome mining to advanced screening methods- yet development has stalled.\nThis paradox reveals that progress requires three key elements working in\nconcert: economic incentives, institutional coordination, and technological\napplication[^7].<br>The evidence-based solutions presented in the original Our World in Data article[^1]\noffer a path forward. Market reforms like subscription models and advance market\ncommitments could help correct the fundamental economic misalignment in\nantibiotic development[^8]. Meanwhile, systematic application of computational\ntools, genomic analysis, and bacterial exploration could help unlock new classes\nof antibiotics that traditional methods miss[^9].<br>The urgency is clear. Antimicrobial resistance threatens to undermine many\nadvances in modern medicine[^12]. However, by combining proven coordination\napproaches from the Golden Age with modern capabilities and sustainable economic\nframeworks, we can revitalise antibiotic development for the challenges ahead.</p>\n<hr>\n<p>[^1]: Our World in Data (2024). &quot;What was the Golden Age of Antibiotics, and how\n    can we spark a new one?&quot;\n    <a href=\"https://ourworldindata.org/golden-age-antibiotics\">https://ourworldindata.org/golden-age-antibiotics</a></p>\n<p>[^2]: Hutchings, M. I., Truman, A. W., &amp; Wilkinson, B. (2019). Antibiotics:\n    Past, present and future. Current Opinion in Microbiology, 51, 72–80.\n    <a href=\"https://doi.org/10.1016/j.mib.2019.10.008\">https://doi.org/10.1016/j.mib.2019.10.008</a></p>\n<p>[^3]: Kolter, R., &amp; Van Wezel, G. P. (2016). Goodbye to brute force in\n    antibiotic discovery? Nature Microbiology, 1(2), 15020.\n    <a href=\"https://doi.org/10.1038/nmicrobiol.2015.20\">https://doi.org/10.1038/nmicrobiol.2015.20</a></p>\n<p>[^4]: Gaynes, R. (2017). The Discovery of Penicillin -New Insights After More\n    Than 75 Years of Clinical Use. Emerging Infectious Diseases, 23(5), 849–853.\n    <a href=\"https://doi.org/10.3201/eid2305.161556\">https://doi.org/10.3201/eid2305.161556</a></p>\n<p>[^5]: Waksman, S. A., &amp; Schatz, A. (1945). Streptomycin–Origin, Nature, and\n    Properties. Journal of the American Pharmaceutical Association, 34(11),\n    273–291.\n    <a href=\"https://doi.org/10.1002/jps.3030341102\">https://doi.org/10.1002/jps.3030341102</a></p>\n<p>[^6]: Sampat, B. N. (2023). Second World War and the Direction of Medical\n    Innovation. SSRN Electronic Journal.\n    <a href=\"https://doi.org/10.2139/ssrn.4422261\">https://doi.org/10.2139/ssrn.4422261</a></p>\n<p>[^7]: Årdal, C., et al. (2020). Antibiotic development -economic, regulatory and\n    societal challenges. Nature Reviews Microbiology, 18(5), 267-274.\n    <a href=\"https://doi.org/10.1038/s41579-019-0293-3\">https://doi.org/10.1038/s41579-019-0293-3</a></p>\n<p>[^8]: Renwick, M. J., Brogan, D. M., &amp; Mossialos, E. (2016). A systematic review\n    and critical assessment of incentive strategies for discovery and\n    development of novel antibiotics. The Journal of Antibiotics, 69(2), 73-88.\n    <a href=\"https://doi.org/10.1038/ja.2015.98\">https://doi.org/10.1038/ja.2015.98</a></p>\n<p>[^9]: Chu, J., et al. (2016). Discovery of MRSA active antibiotics using primary\n    sequence from the human microbiome. Nature Chemical Biology, 12(12),\n    1004-1006.\n    <a href=\"https://doi.org/10.1038/nchembio.2207\">https://doi.org/10.1038/nchembio.2207</a></p>\n<p>[^10]: Baym, M., Stone, L. K., &amp; Kishony, R. (2016). Multidrug evolutionary\n    strategies to reverse antibiotic resistance. Science, 351(6268), aad3292.\n    <a href=\"https://doi.org/10.1126/science.aad3292\">https://doi.org/10.1126/science.aad3292</a></p>\n<p>[^11]: Kremer, M., Levin, J., &amp; Snyder, C. M. (2020). Advance Market\n    Commitments: Insights from Theory and Experience. AEA Papers and\n    Proceedings, 110, 269-273.\n    <a href=\"https://www.aeaweb.org/articles?id=10.1257/pandp.20201017\">https://www.aeaweb.org/articles?id=10.1257/pandp.20201017</a></p>\n<p>[^12]: World Health Organization (2024). Antimicrobial Resistance Fact Sheet.\n    <a href=\"https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance\">https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance</a></p>\n"
  },
  {
    "title": "💡 TIL: Test-Driven Development Is Key to Better LLM System Prompts",
    "date": "2025-01-02T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "til",
      "prompt-engineering",
      "testing",
      "best-practices",
      "evaluation",
      "machine-learning",
      "system-prompts"
    ],
    "url": "/posts/TIL-tdd-good-system-prompts.html",
    "content": "<p><strong>TL;DR:</strong> Anthropic&#39;s approach to system prompt development parallels\ntest-driven development-first creating test cases where default model behaviour\nfails, then developing prompts that pass these tests, followed by iterative\nrefinement-highlighting how robust automated evaluation is not merely a quality\ncheck but the foundation for building reliable LLM applications.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>2024 has made clear that writing good automated evaluations for LLM-powered\nsystems is the most critical skill for building useful applications. This\ninsight parallels Anthropic&#39;s internal approach to system prompt development. As\nusual, Simon Willison&#39;s\n<a href=\"https://simonwillison.net/2024/Dec/31/llms-in-2024/#evals-really-matter\">recent insightful 2024 LLM overview</a>\nwas a treasure trove. One item I picked up on was evaluating system prompts\nusing a test-driven approach.</p>\n<h2 id=\"the-evaluation-first-approach\">The Evaluation-First Approach</h2>\n<p><a href=\"https://askell.io/\">Amanda Askell</a>, leading fine-tuning at Anthropic,\n<a href=\"https://xcancel.com/amandaaskell/status/1866207266761760812\">outlines a test-driven process</a>\nfor system prompts:</p>\n<ol>\n<li>Create a test set of messages where the model&#39;s default behaviour fails to\nmeet requirements</li>\n<li>Develop a system prompt that passes these tests</li>\n<li>Identify cases where the system prompt is misapplied and refine it</li>\n<li>Expand the test set and repeat</li>\n</ol>\n<p>This methodology&#39;s importance extends beyond prompt engineering. Companies with\nstrong evaluation suites can adopt new models faster and build more reliable\nfeatures than competitors. As\n<a href=\"https://xcancel.com/cramforce/status/1860436022347075667\">Vercel&#39;s experience demonstrates</a>,\nmoving from complex prompt protection to robust testing enables rapid iteration\nand development.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>While everyone acknowledges evals&#39; importance, implementing them effectively\nremains challenging. The key insight is clear: robust automated evaluation isn&#39;t\njust a quality check, it&#39;s the foundation for building reliable LLM-powered\nsystems.</p>\n"
  },
  {
    "title": "📝 From Vim to VSCode to Neovim",
    "date": "2024-12-24T00:00:00.000Z",
    "tags": [
      "minimal",
      "cross-platform",
      "toolchain",
      "best-practices",
      "design-principles",
      "python",
      "deno",
      "zero-config"
    ],
    "url": "/posts/vscode-to-neovim.html",
    "content": "<p><strong>TL;DR:</strong> For me, Neovim strikes the perfect balance between Vim&#39;s simplicity\nand VSCode&#39;s features. After wrestling with VSCode&#39;s keyboard input failures on\nFedora and its resource demands, I found that Neovim&#39;s single configuration\nfile, robust plugins, and cross-platform reliability make it ideal for my\nPython, Deno, and Clojure development needs. Sometimes stepping back to move\nforward is exactly what we need!</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Vim&#39;s portable <code>.vimrc</code> embodies software minimalism at its best. One file, one\nminute to setup, resulting in a complete development environment. This\nsimplicity served me well until Azure development motivated the use of VSCode.<br>While VSCode worked reasonably well on macOS, Fedora revealed its constraints:\nkeyboard input failures, heavy resource usage, and\n<a href=\"https://stackoverflow.com/questions/35368889/how-can-i-export-settings\">complex environment portability</a>\ncompared to Vim&#39;s <code>vim +PlugInstall</code>. These limitations drove my search for\ntools that could maintain simplicity while meeting my development requirements\nwith simplicity and portability in mind.</p>\n<h2 id=\"vim---vscode---neovim\">Vim -&gt; VSCode -&gt; Neovim</h2>\n<p>Azure development initially pulled me into VSCode&#39;s ecosystem. While stable on\nmacOS, Fedora revealed deal-breakers: random keyboard input failures that only\nresponded to command palette (Ctrl+Shift+P). No amount of configuration resets\nor reinstalls resolved these issues.</p>\n<p>This instability, coupled with VSCode&#39;s resource footprint, led me to Neovim.\nThe timing aligned with my exploration of Clojure, where Neovim&#39;s Conjure plugin\noffered a compelling Lisp development experience that rivaled Emacs.</p>\n<p>My requirements were specific:</p>\n<ul>\n<li>A lightweight Python IDE</li>\n<li>A lightweight Deno IDE</li>\n<li>A lightweight Clojure IDE</li>\n</ul>\n<p>Through [Dialogue Engineering]({{ site.baseurl }}{% link\n_posts/2024-11-15-dialogue-engineering.md %}), I crafted a complete IDE using a\n<a href=\"https://github.com/ai-mindset/init.vim\">single configuration file</a>. Neovim&#39;s\nmixed ecosystem of package managers and dual Vimscript/Lua support presents a\nlearning curve, but the resulting environment is fast, stable, and precisely\ntailored to my needs. One minor drawback is the complexity of adding colour to\nConjure&#39;s output, especially when compared to the rich REPL experiences offered\nby <a href=\"https://ipython.org/\">IPython</a>, <a href=\"https://deno.com/\">Deno</a>, and Clojure with\n<a href=\"https://github.com/bhauman/rebel-readline\">rebel-readline</a>.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The journey from Vim to VSCode and finally to Neovim reflects a common pattern\nin software development: sometimes we need to step backward to move forward.\nWhile VSCode offered modern IDE features, its stability and resource issues on\nLinux highlighted the enduring value of minimal, portable tools.<br>Neovim strikes an elegant balance: it preserves Vim&#39;s philosophy of simplicity\nand portability while providing modern IDE capabilities. Despite minor\nchallenges with REPL colourisation, its single configuration file approach and\nrobust plugin ecosystem make it a powerful choice for polyglot development. For\ndevelopers who value both minimal tooling and modern features, Neovim proves\nthat we don&#39;t always have to choose between the two.</p>\n"
  },
  {
    "title": "💡 TIL: Exploring OpenAI's API with Swagger",
    "date": "2024-12-23T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "openai",
      "openapi",
      "spec"
    ],
    "url": "/posts/TIL-openai-openapi.html",
    "content": "<p><strong>TL;DR:</strong> You can easily explore OpenAI&#39;s complete API documentation by loading\ntheir GitHub-hosted OpenAPI YAML file directly into Swagger&#39;s web interface.\nThis approach lets you interactively examine all endpoints, request/response\nschemas, and test functionality—a valuable reference for anyone building\nservices that need to maintain compatibility with OpenAI&#39;s API structure.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>OpenAI maintains a comprehensive\n<a href=\"https://github.com/openai/openai-openapi/\">OpenAPI specification</a> that\ndocuments their entire API surface. While browsing through their GitHub\nrepository, <a href=\"https://simonwillison.net/\">Simon Willison</a>[^1] discovered you can\neasily explore this spec using Swagger&#39;s web interface.</p>\n<h2 id=\"the-discovery\">The Discovery</h2>\n<p>Willison recently highlighted a neat trick: you can browse OpenAI&#39;s full API\ndocumentation by loading their\n<a href=\"https://github.com/openai/openai-openapi/blob/master/openapi.yaml\">OpenAPI YAML file</a>\ndirectly into\n<a href=\"https://petstore.swagger.io/?url=https://raw.githubusercontent.com/openai/openai-openapi/refs/heads/master/openapi.yaml#/\">Swagger&#39;s web UI</a>.</p>\n<h2 id=\"why-this-matters\">Why This Matters</h2>\n<p>This approach offers several advantages:</p>\n<ul>\n<li>Interactive exploration of all API endpoints</li>\n<li>Complete request/response schemas</li>\n<li>Built-in testing capability</li>\n<li>Detailed parameter documentation</li>\n</ul>\n<p>For developers working with AI APIs, this provides a valuable reference point -\nespecially when building services that need to maintain compatibility with\nOpenAI&#39;s API structure.</p>\n<h2 id=\"try-it-yourself\">Try It Yourself</h2>\n<p>Visit the <a href=\"https://petstore.swagger.io/\">Swagger UI</a> and paste this URL:<br><code>https://raw.githubusercontent.com/openai/openai-openapi/refs/heads/master/openapi.yaml</code></p>\n<hr>\n<p>[^1]: Co-founder of\n    <a href=\"https://blog.natbat.net/post/61658401806/lanyrd-from-idea-to-exit\">Lanyrd</a>,\n    co-creator of <a href=\"https://simonwillison.net/2005/Jul/17/django/\">Django</a> and\n    <a href=\"https://datasette.io/\">Datasette</a> and a prolific independent AI researcher</p>\n"
  },
  {
    "title": "🎛️ A Practical Guide to Fine-tuning LLMs with InstructLab",
    "date": "2024-12-19T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "model-governance",
      "production",
      "quantisation",
      "python",
      "mlops",
      "best-practices",
      "data-science"
    ],
    "url": "/posts/instructlab-and-rag.html",
    "content": "<p><strong>TL;DR:</strong> InstructLab democratises LLM fine-tuning through its structured LAB\nmethodology, offering three hardware-adaptive QLoRA-based training pipelines\n(Simple, Full, and Accelerated) that enable organisations to create\ndomain-specific models without massive computing resources whilst maintaining\ncomprehensive evaluation frameworks.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>The explosion of Large Language Models (LLMs) has created a pressing need for\ndomain-specific adaptations. While base models like GPT-4, Claude, and Llama\ndemonstrate impressive general capabilities, organisations often need models\nthat excel in specific domains or exhibit particular behavioural traits. This\ncustomisation typically requires fine-tuning, a process that has historically\ndemanded significant expertise, computational resources, and sophisticated\ninfrastructure.</p>\n<h3 id=\"the-fine-tuning-challenge\">The Fine-tuning Challenge</h3>\n<p>Traditional LLM fine-tuning presents a complex web of interconnected challenges\nthat organisations must navigate. At its core lies the need for sophisticated\ninfrastructure, often requiring specialised hardware and carefully orchestrated\nsoftware stacks. This infrastructure challenge is compounded by substantial\ncomputational costs, making experimentation and iteration expensive.<br>The data challenge is equally significant. Fine-tuning demands large,\nhigh-quality datasets that are both rare and expensive to create. Even when such\ndatasets exist, organisations face the risk of catastrophic forgetting, where\nmodels lose their general capabilities while acquiring new ones. Moreover,\nvalidating improvements remains a complex task, requiring careful benchmarking\nand evaluation frameworks.<br>These challenges have historically restricted fine-tuning to well-resourced\norganisations, creating a significant barrier to entry for smaller teams and\norganisations seeking to adapt LLMs to their specific needs.</p>\n<h3 id=\"real-world-challenges\">Real-world Challenges</h3>\n<p>The adaptation of LLMs to specific domains presents organisations with a\nmultifaceted set of practical challenges. In healthcare, medical institutions\ngrapple with the need for models that can accurately process and generate\ncontent using complex medical terminology while maintaining strict clinical\nprotocols. This domain expertise challenge extends beyond mere vocabulary; it\nencompasses understanding of medical procedures, drug interactions, and\ndiagnostic reasoning.<br>The financial sector faces equally demanding requirements, particularly around\ncompliance and regulation. Banks and financial institutions must ensure their\nmodels operate within specific regulatory frameworks, making decisions that are\nnot only accurate but also auditable and explainable to regulatory bodies.<br>Data quality emerges as a persistent challenge across sectors. Organisations\ntypically struggle with historical datasets that exhibit inconsistent\nformatting, missing values, and inherent biases. The challenge extends to\nmaintaining proper version control and data lineage tracking, crucial for both\ncompliance and model improvement cycles.<br>Regulatory constraints add another layer of complexity. Healthcare organisations\nmust ensure strict HIPAA compliance in their model development and deployment\nprocesses. Similarly, any organisation handling European data must adhere to\nGDPR requirements, while specific industries often face additional certification\nneeds. These regulatory requirements must be considered not just in the final\ndeployment but throughout the entire fine-tuning process.</p>\n<h3 id=\"the-role-of-instructlab\">The Role of InstructLab</h3>\n<p><a href=\"https://instructlab.ai/\">InstructLab</a> emerges as a systematic solution to these\nchallenges, offering a novel approach to LLM fine-tuning that combines:</p>\n<ul>\n<li>Synthetic data generation for high-quality training examples</li>\n<li>Efficient <a href=\"https://arxiv.org/abs/2305.14314\">QLoRA</a>-based training pipelines</li>\n<li>Comprehensive evaluation frameworks</li>\n<li>Hardware-adaptive processing</li>\n</ul>\n<p>The rest of this article will elaborate on\n<a href=\"https://instructlab.ai/\">InstructLab</a>&#39;s architecture, workflow, and practical\nconsiderations, demonstrating how it makes LLM fine-tuning accessible while\nmaintaining rigorous quality standards. It will explore how organisations can\nleverage this tool to enhance their AI capabilities efficiently and\nsystematically.</p>\n<h2 id=\"from-principles-to-practice\">From Principles to Practice</h2>\n<p><a href=\"https://instructlab.ai/\">InstructLab</a> is built around the LAB (Large-Scale\nAlignment for ChatBots) methodology, leveraging\n[QLoRA(<a href=\"https://arxiv.org/abs/2305.14314\">https://arxiv.org/abs/2305.14314</a>) (Quantized Low-Rank Adaptation) for\nefficient fine-tuning. The system requires Python 3.10/3.11 and approximately\n500GB of disc space for full operation.</p>\n<h3 id=\"architectural-components\">Architectural Components</h3>\n<p>The system operates through three primary components:</p>\n<ul>\n<li><strong>Taxonomy Repository</strong>: A structured collection of knowledge and skills,\norganised in YAML files (max 2300 words per Q&amp;A pair)</li>\n<li><strong>Synthetic Data Generator</strong>: Uses a teacher model (default: Mixtral/Mistral\ninstruct for full pipeline, Merlinite 7b for simple) to transform taxonomy\nentries into diverse training examples</li>\n<li><strong>Training Pipeline System</strong>: <a href=\"https://arxiv.org/abs/2305.14314\">QLoRA</a>-based\ntraining options optimised for different hardware configurations</li>\n</ul>\n<h3 id=\"training-pipelines\">Training Pipelines</h3>\n<p><a href=\"https://instructlab.ai/\">InstructLab</a> offers three specialised training\npipelines:</p>\n<ol>\n<li><p><strong>Simple Pipeline</strong></p>\n<ul>\n<li>Fast training (~1 hour)</li>\n<li>Uses SFT Trainer (Linux) or MLX (MacOS)</li>\n<li>Great for initial experiments and validation</li>\n</ul>\n</li>\n<li><p><strong>Full Pipeline</strong></p>\n<ul>\n<li>Custom <a href=\"https://arxiv.org/abs/2305.14314\">QLoRA</a> training loop optimised\nfor CPU/MPS</li>\n<li>Enhanced data processing functions</li>\n<li>Memory requirement: 32GB RAM</li>\n<li>Balanced performance and accessibility</li>\n</ul>\n</li>\n<li><p><strong>Accelerated Pipeline</strong></p>\n<ul>\n<li>GPU-accelerated distributed <a href=\"https://arxiv.org/abs/2305.14314\">QLoRA</a>\ntraining</li>\n<li>Supports NVIDIA CUDA and AMD ROCm</li>\n<li>Requires 18GB+ GPU memory</li>\n<li>Ideal for production-grade fine-tuning</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"hardware-support-and-quantisation\">Hardware Support and Quantisation</h3>\n<p><a href=\"https://instructlab.ai/\">InstructLab</a> supports various hardware configurations\nwith automatic quantisation:</p>\n<ul>\n<li>Apple M-series chips: MLX optimisation, MPS acceleration</li>\n<li>NVIDIA GPUs: CUDA support, 4-bit quantisation available</li>\n<li>AMD GPUs: ROCm support, similar quantisation options</li>\n<li>Standard CPUs: Optimised quantisation for memory efficiency</li>\n</ul>\n<h2 id=\"practical-workflow\">Practical Workflow</h2>\n<p>With the architectural foundation established,\n<a href=\"https://instructlab.ai/\">InstructLab</a> provides a systematic approach to\nimplementing these components through a straightforward command-line interface.\nThe following sections detail the practical steps to leverage this architecture\neffectively.</p>\n<h3 id=\"setup-and-installation\">Setup and Installation</h3>\n<pre><code class=\"language-bash\">pip install instructlab\nilab config init\n</code></pre>\n<p>Key requirements:</p>\n<ul>\n<li>Python 3.10 or 3.11 (&gt;=3.12 not supported[^1])</li>\n<li>500GB recommended disc space</li>\n<li>16GB RAM minimum, 32GB recommended</li>\n</ul>\n<h3 id=\"core-workflow-steps\">Core Workflow Steps</h3>\n<ol>\n<li><p><strong>Model Acquisition</strong></p>\n<pre><code class=\"language-bash\">ilab model download\n</code></pre>\n<ul>\n<li>Downloads pre-trained base models</li>\n<li>Supports GGUF (4-bit to 16-bit) and Safetensors formats</li>\n<li>Automatic quantisation with configurable parameters</li>\n</ul>\n</li>\n<li><p><strong>Synthetic Data Generation</strong></p>\n<pre><code class=\"language-bash\">ilab model serve\nilab data generate --pipeline [simple|full]\n</code></pre>\n<p>Common issues and solutions:</p>\n<ul>\n<li>Server conflicts: Use different ports with <code>--port</code></li>\n<li>Memory errors: Reduce batch size or use <code>--pipeline simple</code></li>\n<li>Teacher model issues: Verify model checksum and try re-downloading</li>\n</ul>\n</li>\n<li><p><strong>Training</strong></p>\n<pre><code class=\"language-bash\">ilab model train\n</code></pre>\n<p>Hyperparameters (configurable in config.yaml):</p>\n<ul>\n<li>Max epochs: 10</li>\n</ul>\n</li>\n<li><p><strong>Evaluation</strong></p>\n<pre><code class=\"language-bash\">ilab model evaluate\n</code></pre>\n<p>Benchmarks and typical scores:</p>\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/MMLU\">MMLU</a>: Knowledge (0.0-1.0 scale)</li>\n<li>MMLUBranch: Delta improvements</li>\n<li>MTBench: Skills (0.0-10.0 scale)</li>\n<li>MTBenchBranch: Skill improvements</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"model-deployment\">Model Deployment</h3>\n<pre><code class=\"language-bash\">ilab model serve --model-path &lt;new-model-path&gt;\nilab model chat -m &lt;new-model-path&gt; # Optionally, chat with the model\n</code></pre>\n<p>Deployment considerations:</p>\n<ul>\n<li>Verify quantisation level matches hardware capabilities</li>\n<li>Monitor memory usage during serving</li>\n<li>Consider temperature settings for inference (default: 1.0)</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p><a href=\"https://instructlab.ai/\">InstructLab</a> represents a significant advancement in\ndemocratising LLM fine-tuning, bridging the gap between research capabilities\nand practical deployment. Through its innovative LAB methodology and\n<a href=\"https://arxiv.org/abs/2305.14314\">QLoRA</a>-based implementation, it makes\nsophisticated model adaptation accessible to practitioners across different\nhardware configurations.</p>\n<h3 id=\"key-advantages\">Key Advantages</h3>\n<ul>\n<li><strong>Accessibility</strong>: From laptops to data centres,\n<a href=\"https://instructlab.ai/\">InstructLab</a> scales with available resources</li>\n<li><strong>Flexibility</strong>: Multiple training pipelines accommodate different needs and\nconstraints</li>\n<li><strong>Systematic</strong>: Structured approach to knowledge and skill injection through\ntaxonomy</li>\n<li><strong>Verifiable</strong>: Comprehensive evaluation suite ensures quality of fine-tuned\nmodels</li>\n</ul>\n<h3 id=\"practical-impact\">Practical Impact</h3>\n<p><a href=\"https://instructlab.ai/\">InstructLab</a> enables organisations to:</p>\n<ul>\n<li>Create domain-specialised models without massive compute resources</li>\n<li>Systematically inject new capabilities through structured knowledge\nrepresentation</li>\n<li>Validate improvements through quantitative benchmarks</li>\n<li>Deploy fine-tuned models with minimal operational overhead</li>\n</ul>\n<h3 id=\"limitations-and-considerations\">Limitations and Considerations</h3>\n<ul>\n<li><p><strong>Model Constraints</strong>: Currently supports models up to 7B parameters\neffectively</p>\n</li>\n<li><p><strong>Resource Timeline</strong>: Typical deployment cycle from setup to production:</p>\n<ul>\n<li>Initial setup: a few hours</li>\n<li>Synthetic Data generation: 15 minutes to 1+ hours depending on computing\nresources</li>\n<li>Training: several hours on consumer hardware</li>\n<li>Evaluation and deployment: a few hours</li>\n</ul>\n</li>\n<li><p><strong>Maintenance Requirements</strong>:</p>\n<ul>\n<li>Regular model evaluations against new benchmarks</li>\n<li>Periodic retraining with updated taxonomy</li>\n<li>System updates and dependency management</li>\n<li>Storage management for checkpoints and datasets</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"rag-vs-fine-tuning\">RAG vs Fine-tuning</h3>\n<p>It&#39;s important to recognise that fine-tuning isn&#39;t always the optimal solution.\nFor dynamic, frequently changing knowledge bases, Retrieval-Augmented Generation\n(RAG) often provides a more practical and maintainable solution. Fine-tuning\nthrough <a href=\"https://instructlab.ai/\">InstructLab</a> is most valuable for:</p>\n<ul>\n<li>Stable knowledge domains (e.g., natural sciences, engineering)</li>\n<li>Consistent skill enhancement needs</li>\n<li>Cases where inference latency is critical</li>\n</ul>\n<p>The system&#39;s architecture strikes a careful balance between computational\nefficiency and training effectiveness, making it a practical tool for both\nexperimentation and production use. While not eliminating the complexity of LLM\nfine-tuning entirely, <a href=\"https://instructlab.ai/\">InstructLab</a> significantly\nreduces the technical barriers to entry in this crucial domain.</p>\n<hr>\n<p>[^1]: Python version compatibility remains a significant consideration in the ML\n    ecosystem. While newer versions (≥3.12) offer improved performance, they\n    often lack compatibility with essential ML frameworks. This constraint\n    informs <a href=\"https://instructlab.ai/\">InstructLab</a>&#39;s current version\n    requirements.</p>\n"
  },
  {
    "title": "💡 TIL: Understanding GGUF Model Quantisation",
    "date": "2024-12-07T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "energy-reduction",
      "performance",
      "quantisation"
    ],
    "url": "/posts/TIL-llm-quantisation.html",
    "content": "<p><strong>TL;DR:</strong> GGUF quantisation converts LLM weights from 16-bit to lower precision\nformats (2-bit to 6-bit) to run large models on consumer hardware. Each format\noffers different tradeoffs between size, speed, and quality, with Q4_K_S (4-bit)\nrepresenting the sweet spot for most users—providing 3.7x size reduction while\nmaintaining good quality. Mixed precision strategies (_S/_M/_L variants) further\noptimize performance by targeting attention and feed-forward layers with higher\nprecision bits.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>When experimenting with larger language models (12B, 30B, 70B etc.), choosing\nthe right quantisation format becomes crucial for striking a good balance i.e.\nrunning them on consumer hardware while maintaining reasonably good performance.\nI wrote this guide after spending time looking up different GGUF quantisation\ntypes to optimise model selection for my machine&#39;s constraints. This guide\nexplains quantisation methods and their practical tradeoffs to help the reader\nselect the optimal format for their setup.<br>The quantisation formats discussed here are implemented in popular frameworks\nlike <a href=\"https://github.com/ggerganov/llama.cpp\">llama.cpp</a>. Q4_K_S is typically\nthe default format due to its good balance of size, speed, and quality, while\nQ2_K and Q3_K variants are offered for more constrained systems.</p>\n<h2 id=\"what-is-quantisation\">What is Quantisation?</h2>\n<p>Quantisation converts model weights from 16-bit floating point (F16) to lower\nprecision formats using fixed-size blocks. Each block contains multiple weights\nthat share scaling parameters.<br>Perplexity is the key metric used to measure model quality after quantisation.\nIt indicates how well the model predicts text, the lower the perplexity the\nbetter the predictions. For example, a change from 5.91 to 6.78 perplexity\nrepresents a noticeable but often acceptable drop in prediction quality. A model\nwith perplexity 6.78 is slightly less certain about its predictions than one\nwith perplexity 5.91.</p>\n<h2 id=\"basic-quantisation-types-and-k-quantisation\">Basic Quantisation Types and K-Quantisation</h2>\n<p>K-quantisation is a way to make AI models smaller using two methods to store\nweights (the model&#39;s numbers):</p>\n<ol>\n<li>Type-0 (simpler): reconstructs weight as <code>weight = scale × quant</code></li>\n<li>Type-1 (more precise): reconstructs weight as\n<code>weight = scale × quant + minimum</code></li>\n</ol>\n<p>The &quot;block minimum&quot; <code>minimum</code> is the smallest value found in a group of weights.\nBy tracking this minimum, we can represent the other values more precisely\nrelative to it, rather than having to represent their full absolute values.</p>\n<p>Each format groups weights into &quot;super-blocks&quot; to save space. Specifically:</p>\n<p>Q2_K (2-bit):</p>\n<ul>\n<li>Uses Type-1 formula</li>\n<li>Organises weights in groups of 256 (16 blocks × 16 weights)</li>\n<li>Uses 4 bits to store both scales and minimums</li>\n<li>Takes exactly 2.5625 bits per weight</li>\n<li>Result: Shrinks a 13GB model to 2.67GB, but quality drops (perplexity\nincreases from 5.91 to 6.78)</li>\n</ul>\n<p>Q3_K (3-bit):</p>\n<ul>\n<li>Uses Type-0 formula (simpler one)</li>\n<li>Same organisation: 16 blocks × 16 weights</li>\n<li>Uses 6 bits to store scales</li>\n<li>Takes exactly 3.4375 bits per weight</li>\n<li>Better quality than Q2_K but bigger file size</li>\n</ul>\n<p>Q4_K (4-bit):</p>\n<ul>\n<li>Uses Type-1 formula</li>\n<li>Different organisation: 8 blocks × 32 weights = 256 total</li>\n<li>Uses 6 bits for both scales and minimums</li>\n<li>Takes exactly 4.5 bits per weight</li>\n<li>Much better quality, file size around 3.56GB</li>\n</ul>\n<p>Q5_K (5-bit):</p>\n<ul>\n<li>Uses Type-1 formula</li>\n<li>Same organisation as Q4_K</li>\n<li>Also uses 6 bits for scales and minimums</li>\n<li>Takes exactly 5.5 bits per weight</li>\n<li>Quality getting very close to original</li>\n</ul>\n<p>Q6_K (6-bit):</p>\n<ul>\n<li>Uses Type-0 formula</li>\n<li>Back to 16 blocks × 16 weights</li>\n<li>Uses 8 bits for scales</li>\n<li>Takes exactly 6.5625 bits per weight</li>\n<li>Almost perfect quality, file size 5.15GB</li>\n</ul>\n<p>The main tradeoff: Fewer bits means smaller files but lower quality. More bits\nmeans better quality but larger files. This lets users choose what works best\nfor their needs.<br>When compressing numbers in Type-1 quantisation, each block keeps track of its\nsmallest value (the minimum). When reconstructing the weights, this minimum is\nadded back after multiplication. This helps preserve the range of values more\naccurately than just using scaling alone.</p>\n<p>A simple way to think of this concept is:</p>\n<ul>\n<li>Type-0 just stretches/shrinks values using a scale</li>\n<li>Type-1 first shifts all numbers by subtracting the minimum (making them\nsmaller), then scales them for storage, and when reconstructing adds the\nminimum back</li>\n</ul>\n<p>This is why Type-1 generally gives better quality results but needs more storage\nspace. It has to keep track of both the scale and minimum for each block.</p>\n<h2 id=\"mixed-precision-strategies\">Mixed Precision Strategies</h2>\n<p>K-quantisations use different precision levels for different model components.\nFrom <a href=\"https://github.com/ggerganov/llama.cpp\">llama.cpp</a> documentation, there\nare three variants:</p>\n<ul>\n<li><p>S (Small): Uses single quantisation throughout Example using Q3_K_S:</p>\n<blockquote>\n<p>All model tensors → Q3_K (3-bit)<br>Result: 2.75GB size, 6.46 perplexity (7B model)</p>\n</blockquote>\n</li>\n<li><p>M (Medium): Strategic mixed precision Example using Q3_K_M:</p>\n<blockquote>\n<p>attention.wv<a href=\"In\">^1</a>, attention.wo[^2], feed_forward.w2[^3] → Q4_K (4-bit)<br>All other tensors → Q3_K (3-bit)<br>Result: 3.06GB size, 6.15 perplexity (7B model)</p>\n</blockquote>\n</li>\n<li><p>L (Large): Higher precision mix Example using Q3_K_L:</p>\n<blockquote>\n<p>attention.wv<a href=\"In\">^1</a>, attention.wo[^2], feed_forward.w2[^3] → Q5_K (5-bit)<br>All other tensors → Q3_K (3-bit)<br>Result: 3.35GB size, 6.09 perplexity (7B model)</p>\n</blockquote>\n</li>\n</ul>\n<p>These strategies target attention and feed-forward layers with higher precision\nbecause they directly impact text processing quality, as demonstrated by the\nperplexity improvements in benchmarks: Q3_K_S (6.46) → Q3_K_M (6.15) → Q3_K_L\n(6.09).<br>The improvement in perplexity scores demonstrates why mixed precision strategies\nare effective, though they require more storage space.</p>\n<h2 id=\"performance-comparison-7b-model\">Performance Comparison (7B model)</h2>\n<pre><code>Format | Size(GB) | Reduction | BPW  | Perplexity | RTX4080  | M2Max   \nF16    | 13.0     | 1.0x      | 16.0 | 5.91       | 60.0ms   | 116ms\nQ2_K   | 2.67     | 4.9x      | 2.56 | 6.78       | 15.5ms   | 56ms\nQ3_K_S | 2.75     | 4.7x      | 3.44 | 6.46       | 18.6ms   | 81ms\nQ4_K_S | 3.56     | 3.7x      | 4.50 | 6.02       | 15.5ms   | 50ms\nQ6_K   | 5.15     | 2.5x      | 6.56 | 5.91       | 18.3ms   | 75ms\n</code></pre>\n<p>*BPW = Bits Per Weight, Speed in milliseconds per token</p>\n<p>Practical Recommendations:</p>\n<ul>\n<li>Balanced Performance: Q4_K_S</li>\n<li>Maximum Compression: Q2_K</li>\n<li>Best Quality: Q6_K (matches F16)</li>\n<li>Limited RAM: Q2_K or Q3_K</li>\n<li>GPU Inference: Q4_K (optimal speed/quality)</li>\n</ul>\n<p>All data are from recent\n<a href=\"https://github.com/ggerganov/llama.cpp/pull/1684\">llama.cpp</a> performance\nbenchmarks and <a href=\"https://github.com/ggerganov/ggml\">GGML</a> implementation details.</p>\n<h2 id=\"memory-requirements-for-inference\">Memory Requirements for Inference</h2>\n<p>When running quantised models, more RAM is required than the model size alone\nfor inference overhead. Memory requirements depend on several factors:</p>\n<ul>\n<li>Model architecture and size</li>\n<li>Batch size for inference</li>\n<li>Number of layers loaded at once</li>\n<li>Operating system and framework overhead</li>\n</ul>\n<p>For 7B models (verified from benchmarks):</p>\n<pre><code>Format | Model Size | Note\nF16    | 13.0GB    | Base format\nQ4_K_S | 3.56GB    | Common choice\nQ3_K_S | 2.75GB    | Minimum size\nQ6_K   | 5.15GB    | Highest quality\n</code></pre>\n<p>For larger models scale the memory requirements proportionally and ensure\nadditional overhead memory is available for inference. Test with smaller models\nfirst to gauge the system&#39;s capabilities.<br>Actual RAM/VRAM requirements will be higher than the model size. Consider\nmonitoring memory usage during inference to determine exact requirements for a\nspecific setup.<br>Here is an example memory usage scenario for a Q4_K_S 7B model:</p>\n<ul>\n<li>Model size: 3.56GB</li>\n<li>Inference overhead: ~2GB for standard settings</li>\n<li>Operating system buffer: ~1GB recommended</li>\n<li>Total recommended free memory: ~7GB</li>\n</ul>\n<p>This explains why a model that&#39;s &quot;3.56GB&quot; might need 6-7GB of free RAM/VRAM to\nrun smoothly. The exact overhead varies based on your settings and system.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Modern quantisation techniques offer multiple ways to run large language models\non consumer hardware. Here&#39;s what we need to remember:</p>\n<ul>\n<li>K-quantisation provides the best balance through super-blocks and mixed\nprecision strategies</li>\n<li>Q4_K_S (4-bit) represents the current sweet spot for most users, offering:<ul>\n<li>3.7x size reduction</li>\n<li>Good perplexity (6.02)</li>\n<li>Excellent inference speed on both GPU and CPU</li>\n</ul>\n</li>\n<li>For more constrained setups, Q2_K/Q3_K variants can run larger models with\nacceptable quality loss</li>\n<li>Higher bits (Q5_K, Q6_K) approach F16 quality but require more memory</li>\n<li>The _S/_M/_L variants let the user fine-tune the quality-size tradeoff by\nadjusting precision where it matters most</li>\n</ul>\n<p>Before downloading a quantised model, check the system&#39;s available RAM and\nchoose a format that leaves enough memory for comfortable operation. For most\nusers with modern GPUs, Q4_K variants will provide the best experience.</p>\n<hr>\n<pre><code>[llama.cpp](https://github.com/ggerganov/llama.cpp/tree/master/examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp),\n`attention.wv` refers to a tensor that holds the weights for the value\nvectors in the self-attention mechanism of the model. This tensor is crucial\nfor determining how much focus the model places on different parts of the\ninput when generating responses.\n</code></pre>\n<p>[^2]: <code>attention.wo</code> refers to the weight matrix used in the output layer of the\n    attention mechanism within a transformer model. It plays a crucial role in\n    transforming the attention output into the final representation that is used\n    for generating predictions.</p>\n<p>[^3]: <code>feed_forward.w1</code> projects input to a higher-dimensional space, enabling\n    the capture of complex features. <code>feed_forward.w2</code> projects transformed\n    input back to the original dimension with a non-linear activation function,\n    whereas <code>feed_forward.w3</code> applies an additional transformation to enhance\n    the learning of complex patterns. These matrices collectively enable the\n    feed-forward network to transform and learn from the input effectively,\n    contributing to the overall performance of the transformer model.</p>\n"
  },
  {
    "title": "💡 TIL: LLM Evaluation using Critique Shadowing",
    "date": "2024-12-05T00:00:00.000Z",
    "tags": [
      "til",
      "llm",
      "ai",
      "machine-learning",
      "mlops",
      "best-practices",
      "production",
      "model-governance",
      "evaluation",
      "observability",
      "monitoring",
      "quality-assurance",
      "iterative-refinement"
    ],
    "url": "/posts/TIL-llm-eval-critique-shadowing.html",
    "content": "<p><strong>TL;DR:</strong> Critique Shadowing offers an expert-centered approach to LLM\nevaluation by starting with binary pass/fail judgments and detailed critiques\nbefore building automated systems. This iterative methodology—reminiscent of\n1970s knowledge engineering—prioritizes domain expertise over complex metrics,\nrevealing valuable insights about products and users while developing reliable\nevaluation systems that capture nuanced quality standards.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>As LLMs increasingly drive critical business decisions, ensuring their\nreliability becomes paramount. Many teams struggle with complex metrics and\nscoring systems that lead to confusion rather than clarity.\n<a href=\"https://hamel.dev/\">Hamel Husain</a>&#39;s Critique Shadowing methodology[^1] offers a\nsystematic path from drowning in metrics to developing reliable evaluation\nsystems.</p>\n<h2 id=\"the-critique-shadowing-method\">The Critique Shadowing Method</h2>\n<p>The key insight behind Critique Shadowing is deceptively simple: start with\nbinary (pass/fail) expert judgements and detailed critiques before building\nautomated evaluation systems. This approach solves two critical challenges:\ncapturing domain expertise and scaling evaluation processes.</p>\n<p>This expert-centric approach echoes\n<a href=\"https://en.wikipedia.org/wiki/Knowledge_engineering\">knowledge engineering</a>\npractices from the 1970-80s, when AI researchers first recognised the necessity\nof systematically capturing domain expertise. Just as\n<a href=\"https://en.wikipedia.org/wiki/Mycin\">MYCIN</a>&#39;s creators worked closely with\nmedical doctors to encode diagnostic knowledge, Critique Shadowing similarly\nstructures the process of extracting expert judgement for LLM evaluation. While\nthe technology has evolved from rule-based systems to large language models, the\nfundamental challenge of effectively capturing and operationalising expert\nknowledge remains central.</p>\n<h3 id=\"implementation-process\">Implementation Process</h3>\n<p>The methodology follows a structured, iterative process:</p>\n<center>\n    <img src=\"https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/refs/heads/master/images/Critique%20Framework%20Hamel%20Husain.png\" width=\"80%\" height=\"80%\"/>\n</center>\n\n<ol>\n<li>Identify a principal domain expert as the arbiter of quality</li>\n<li>Create a diverse dataset covering different scenarios and user types</li>\n<li>Expert conducts binary pass/fail judgements with detailed critiques</li>\n<li>Address discovered issues and verify fixes</li>\n<li>Develop LLM-based judges using expert critiques as few-shot examples</li>\n<li>Analyse error patterns and root causes</li>\n<li>Create specialised judges for persistent issues</li>\n</ol>\n<p>The process is continuous, repeating periodically or when material changes\noccur. For simpler applications or when manual review is feasible, teams can\nadapt or streamline these steps while maintaining the core principle of\nsystematic data examination.</p>\n<h2 id=\"beyond-automation\">Beyond Automation</h2>\n<p>Husain&#39;s most striking observation is that the process of developing evaluation\nsystems often provides more value than the resulting automated judges. The\nsystematic collection of expert feedback reveals product insights, user needs,\nand failure modes that might otherwise remain hidden. This understanding drives\nimprovements in the core system, not just its evaluation.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The Critique Shadowing methodology succeeds by prioritising expert knowledge and\nsystematic data collection over premature automation. For teams building LLM\napplications, this approach offers a clear path to reliable evaluation systems\nwhile simultaneously deepening their understanding of their product and users.<br>LLM evaluation is an active area of interest and research both in academia and\nindustry. Here is a short list of resources to look into:</p>\n<ul>\n<li><a href=\"https://www.ibm.com/think/topics/llm-evaluation\">IBM LLM Evaluation</a></li>\n<li><a href=\"https://docs.mistral.ai/guides/evaluation/\">Mistral AI - Evaluation</a></li>\n<li><a href=\"https://github.com/mistralai/mistral-evals\">Mistral Evals</a></li>\n<li><a href=\"https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool\">Anthropic - Using the Evaluation Tool</a></li>\n<li><a href=\"https://dev.to/guybuildingai/-top-5-open-source-llm-evaluation-frameworks-in-2024-98m\">Top 5 Open-Source LLM Evaluation Frameworks in 2024</a></li>\n</ul>\n<hr>\n<p>[^1]: Husain, H. (2024). &quot;Creating a LLM-as-a-Judge That Drives Business\n    Results&quot; <a href=\"https://hamel.dev/blog/posts/llm-judge/\">https://hamel.dev/blog/posts/llm-judge/</a></p>\n"
  },
  {
    "title": "✍ A Path to Maintainable AI Systems using Norman's Design Principles",
    "date": "2024-12-03T00:00:00.000Z",
    "tags": [
      "ai",
      "data-science",
      "design-principles",
      "code-quality",
      "mlops",
      "monitoring",
      "observability",
      "production",
      "model-governance",
      "minimal"
    ],
    "url": "/posts/design-principles-ds-ai.html",
    "content": "<p><strong>TL;DR:</strong> Don Norman&#39;s timeless design principles - visibility, feedback,\nconstraints, mappings, and error prevention - apply powerfully to AI systems,\nwhere abstract interfaces and complex workflows often become overwhelming. By\nimplementing these principles with a carefully selected, minimal toolset, we can\ncreate maintainable, observable AI systems that reduce complexity while\nproviding comprehensive functionality - just as Norman observed in physical\nobjects, good design in AI leads to fewer errors and greater user satisfaction.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Don Norman&#39;s principles of good design, outlined in\n<a href=\"https://archive.org/details/thedesignofeverydaythingsbydonnorman\">The Design of Everyday Things</a>,\nare particularly relevant to Data Science and AI Engineering, where systems\noften suffer from unnecessary complexity. This article presents a minimalist\napproach to implementing these principles using a carefully selected set of\ntools that maximise impact while reducing operational overhead. Norman&#39;s\ninsights about visibility, feedback, constraints, and mappings translate\npowerfully to AI system design, where abstract interfaces and complex workflows\ncan easily become overwhelming. Just as Norman observed that poorly designed\nphysical objects lead to user frustration and errors, poorly architected AI\nsystems can result in maintenance nightmares, hidden failure modes, and costly\ndebugging cycles. By applying his principles - making system states visible,\nproviding clear feedback, implementing appropriate constraints, and creating\nnatural mappings between components, we can build AI systems that are not only\nmore intuitive to use but also easier to maintain, debug, and evolve over time.</p>\n<h2 id=\"design-principles-implementation\">Design Principles Implementation</h2>\n<h3 id=\"1-visibility\">1. Visibility</h3>\n<p>Implement comprehensive system observability using <a href=\"https://mlflow.org/\">MLflow</a>\nas your central platform:</p>\n<ul>\n<li>Track experiments, parameters, and metrics</li>\n<li>Version models and artefacts</li>\n<li>Log production predictions and outcomes</li>\n<li>Monitor model performance metrics</li>\n</ul>\n<p>For system-level metrics, use\n<a href=\"https://prometheus.io/docs/visualization/grafana/\">Prometheus/Grafana</a> to:</p>\n<ul>\n<li>Track resource utilisation (CPU, memory, latency)</li>\n<li>Monitor prediction throughput</li>\n<li>Create dashboards for key performance indicators</li>\n</ul>\n<p>Implement adaptive sampling for high-volume systems:</p>\n<pre><code class=\"language-python\">def should_log(request_id, sampling_rate=0.1):\n    return hash(request_id) % 100 &lt; (sampling_rate * 100)\n</code></pre>\n<h3 id=\"2-feedback\">2. Feedback</h3>\n<p>Use <a href=\"https://prometheus.io/docs/visualization/grafana/\">Prometheus/Grafana</a> for\nreal-time monitoring and alerting:</p>\n<ul>\n<li>Set up alerts for model performance degradation</li>\n<li>Monitor data distribution shifts</li>\n<li>Track system health metrics</li>\n<li>Configure tiered alerting based on severity</li>\n</ul>\n<p>Example metric collection:</p>\n<pre><code class=\"language-python\">from prometheus_client import Counter, Histogram\n\nPREDICTIONS = Counter(&#39;model_predictions_total&#39;, &#39;Total predictions made&#39;)\nLATENCY = Histogram(&#39;prediction_latency_seconds&#39;, &#39;Time spent processing prediction&#39;)\n\ndef predict(features):\n    with LATENCY.time():\n        prediction = model.predict(features)\n        PREDICTIONS.inc()\n        return prediction\n</code></pre>\n<h3 id=\"3-constraints\">3. Constraints</h3>\n<p>Implement data and model guardrails using\n<a href=\"https://greatexpectations.io/\">Great Expectations</a>:</p>\n<ul>\n<li>Define data quality expectations</li>\n<li>Set distribution bounds for features</li>\n<li>Monitor for data drift</li>\n<li>Generate validation reports</li>\n</ul>\n<p>Example constraint implementation:</p>\n<pre><code class=\"language-python\">from great_expectations.dataset import Dataset\n\ndef validate_features(df):\n    dataset = Dataset(df)\n    dataset.expect_column_values_to_be_between(&quot;age&quot;, 0, 120)\n    dataset.expect_column_values_to_not_be_null(&quot;critical_feature&quot;)\n    validation_result = dataset.validate()\n    return validation_result.success\n</code></pre>\n<h3 id=\"4-mappings\">4. Mappings</h3>\n<p>Use <a href=\"https://mlflow.org/\">MLflow</a> to maintain clear relationships between:</p>\n<ul>\n<li>Experiments and business objectives</li>\n<li>Models and their training data</li>\n<li>Predictions and outcomes</li>\n<li>Performance metrics and business KPIs</li>\n</ul>\n<p>Example mapping structure:</p>\n<pre><code class=\"language-python\">with mlflow.start_run(run_name=&quot;production_model_v1&quot;):\n    mlflow.log_param(&quot;business_objective&quot;, &quot;customer_churn&quot;)\n    mlflow.log_param(&quot;data_version&quot;, data_hash)\n    mlflow.log_metric(&quot;business_impact&quot;, revenue_improvement)\n    mlflow.log_artifact(&quot;feature_importance.json&quot;)\n</code></pre>\n<h3 id=\"5-error-prevention-and-recovery\">5. Error Prevention and Recovery</h3>\n<p>Integrate safeguards using your core toolset:</p>\n<p><a href=\"https://mlflow.org/\">MLflow</a>:</p>\n<ul>\n<li>Version control for models and artefacts</li>\n<li>Rollback capabilities</li>\n<li>Experiment tracking for reproducibility</li>\n</ul>\n<p><a href=\"https://prometheus.io/docs/visualization/grafana/\">Prometheus/Grafana</a>:</p>\n<ul>\n<li>Early warning system for issues</li>\n<li>Performance degradation detection</li>\n<li>Resource exhaustion prevention</li>\n</ul>\n<p><a href=\"https://greatexpectations.io/\">Great Expectations</a>:</p>\n<ul>\n<li>Data quality validation</li>\n<li>Schema enforcement</li>\n<li>Distribution monitoring</li>\n</ul>\n<p>Example error prevention:</p>\n<pre><code class=\"language-python\">def safe_predict(features):\n    if not validate_features(features):\n        return fallback_prediction()\n    \n    try:\n        with LATENCY.time():\n            prediction = model.predict(features)\n            PREDICTIONS.inc()\n            return prediction\n    except Exception as e:\n        ERROR_COUNTER.inc()\n        return fallback_prediction()\n</code></pre>\n<h2 id=\"implementation-strategy\">Implementation Strategy</h2>\n<ol>\n<li><p>Start with <a href=\"https://mlflow.org/\">MLflow</a></p>\n<ul>\n<li>Set up experiment tracking</li>\n<li>Implement model versioning</li>\n<li>Configure basic logging</li>\n</ul>\n</li>\n<li><p>Add <a href=\"https://prometheus.io/docs/visualization/grafana/\">Prometheus/Grafana</a></p>\n<ul>\n<li>Deploy basic monitoring</li>\n<li>Set up key alerts</li>\n<li>Create essential dashboards</li>\n</ul>\n</li>\n<li><p>Integrate <a href=\"https://greatexpectations.io/\">Great Expectations</a></p>\n<ul>\n<li>Define core data quality rules</li>\n<li>Implement validation pipelines</li>\n<li>Monitor data distributions</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>By focusing on a minimal set of powerful tools (<a href=\"https://mlflow.org/\">MLflow</a>,\n<a href=\"https://prometheus.io/docs/visualization/grafana/\">Prometheus/Grafana</a>, and\n<a href=\"https://greatexpectations.io/\">Great Expectations</a>), you can implement Norman&#39;s\ndesign principles effectively while maintaining system simplicity. This approach\nprovides:</p>\n<ul>\n<li>Comprehensive visibility through unified logging and monitoring</li>\n<li>Immediate feedback via real-time alerts</li>\n<li>Strong constraints through data validation</li>\n<li>Clear mappings between components</li>\n<li>Robust error prevention and recovery</li>\n</ul>\n<p>The key is to fully utilise these core tools rather than adding complexity with\nadditional solutions. This creates maintainable, observable, and reliable AI\nsystems that can scale with your needs.</p>\n"
  },
  {
    "title": "🐼 Pandas or 🐻‍❄️ Polars?",
    "date": "2024-12-02T00:00:00.000Z",
    "tags": [
      "python",
      "pandas",
      "polars",
      "data-processing",
      "code-quality",
      "toolchain",
      "data-science"
    ],
    "url": "/posts/pandas-polars.html",
    "content": "<p><strong>TL;DR:</strong> While Pandas excels at interactive exploration and smaller datasets\nwith its Python-centric ecosystem, Polars leverages Rust&#39;s performance for\nparallel processing and memory efficiency in large-scale data operations. Choose\nPandas for rapid prototyping and datasets under 1GB, and Polars for production\nenvironments with demanding performance requirements or cross-language\ndevelopment needs in Python, Node.js, and Rust.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>The world of Python data processing has long revolved around the well\nestablished <a href=\"https://pandas.pydata.org/\">Pandas</a> library, but in recent years, a\nnew contender has emerged in the form of <a href=\"https://pola.rs/\">Polars</a>. This post\naims to provide a comparison of these two powerful data processing tools, that\nempowers the reader to make an informed choice on a case-by-case basis.</p>\n<h2 id=\"architecture-and-design-comparison\">Architecture and Design Comparison</h2>\n<p>At the core, Pandas and Polars differ in their underlying implementation and\ndesign philosophies.</p>\n<h3 id=\"implementation-and-performance\">Implementation and Performance</h3>\n<p>The Pandas library is written in Python/Cython, with a focus on single-threaded\noperations. In contrast, Polars is built upon the Rust programming language,\nleveraging its performance and concurrency capabilities to enable parallel\nprocessing by default.<br>This distinction in implementation has significant implications for memory\nmanagement and query optimisation. Pandas typically works with multiple copies\nof data, while Polars utilises the Arrow data format, which allows for more\nefficient memory usage. Additionally, Polars offers automatic query\noptimisation, whereas Pandas users must rely on a more sequential, manual\napproach to optimising their data processing pipelines.</p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Pandas</th>\n<th>Polars</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Implementation</td>\n<td>Python/Cython</td>\n<td>Rust</td>\n</tr>\n<tr>\n<td>Processing</td>\n<td>Single-threaded</td>\n<td>Parallel by default</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Multiple copies</td>\n<td>Arrow format</td>\n</tr>\n<tr>\n<td>Query Optimisation</td>\n<td>Sequential</td>\n<td>Automatic</td>\n</tr>\n</tbody></table>\n<h3 id=\"api-and-language-support\">API and Language Support</h3>\n<p>The API and language support differences between Pandas and Polars are quite\nnotable. Pandas -being a Python-only library- offers a mix of method chaining\nand attribute access approaches. In contrast, Polars takes a more expansive\napproach, providing implementations in Python, Node.js, and the Rust programming\nlanguage itself.<br>This language versatility of Polars enables seamless JavaScript and TypeScript\nintegration, allowing data scientists and developers to leverage the same\nperformance benefits regardless of their preferred language. Additionally,\nPolars maintains a consistent method chaining syntax across these different\nlanguage environments, simplifying the learning curve for users who may work\nwith the library in multiple contexts.</p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Pandas</th>\n<th>Polars</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Language Support</td>\n<td>Python-only</td>\n<td>Python, Node.js, Rust</td>\n</tr>\n<tr>\n<td>API Style</td>\n<td>Mixed method chaining and attribute access</td>\n<td>Consistent method chaining</td>\n</tr>\n<tr>\n<td>Language Integration</td>\n<td>N/A</td>\n<td>JavaScript/TypeScript</td>\n</tr>\n</tbody></table>\n<h2 id=\"use-cases-and-trade-offs\">Use Cases and Trade-offs</h2>\n<p>While both Pandas and Polars excel in the realm of data processing, each library\nhas distinct strengths and weaknesses that make them better suited for different\nuse cases and scenarios.</p>\n<h3 id=\"when-to-choose-pandas\">When to Choose Pandas</h3>\n<p>Pandas shines when it comes to interactive data exploration and working with\nsmaller datasets, typically under 1GB in size. The library&#39;s deep integration\nwith the broader scientific computing ecosystem in Python, along with its\nintuitive syntax and extensive documentation, make it an excellent choice for\nrapid prototyping, educational contexts, and projects that require seamless\ncompatibility with the Python-centric data science toolchain.</p>\n<h3 id=\"when-to-choose-polars\">When to Choose Polars</h3>\n<p>On the other hand, Polars emerges as the preferred choice for large-scale data\nprocessing, particularly for datasets exceeding 1GB. The library&#39;s Rust-based\nimplementation and parallel processing capabilities make it a more suitable\noption for production environments with demanding performance requirements.\nPolars also excels in memory-constrained systems, thanks to its efficient use of\nthe Arrow data format, and it is an attractive choice for cross-language\ndevelopment teams due to its implementations in Python, Node.js, and Rust.<br>Furthermore, Polars demonstrates strengths in handling complex data\ntransformations and time series processing at scale, areas where its optimised\nquery engine and parallel processing features can truly shine.</p>\n<p>To summarise the key differences:</p>\n<table>\n<thead>\n<tr>\n<th>Consideration</th>\n<th>Pandas</th>\n<th>Polars</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Dataset Size</td>\n<td>Small to medium (&lt;1GB)</td>\n<td>Scales to larger datasets</td>\n</tr>\n<tr>\n<td>Performance</td>\n<td>Suitable for interactive exploration</td>\n<td>Excels at large-scale processing</td>\n</tr>\n<tr>\n<td>Memory Efficiency</td>\n<td>Works with multiple data copies</td>\n<td>Utilises Arrow format for efficiency</td>\n</tr>\n<tr>\n<td>Query Optimisation</td>\n<td>Sequential, manual approach</td>\n<td>Automatic optimisation</td>\n</tr>\n<tr>\n<td>Language Support</td>\n<td>Python-only</td>\n<td>Python, Node.js, Rust</td>\n</tr>\n<tr>\n<td>Ecosystem Integration</td>\n<td>Strong in Python scientific computing</td>\n<td>Limited cross-language integration</td>\n</tr>\n<tr>\n<td>Learning Resources</td>\n<td>Extensive documentation and community support</td>\n<td>Younger ecosystem, less comprehensive resources</td>\n</tr>\n</tbody></table>\n<p>Ultimately, the choice between Pandas and Polars should be guided by the\nspecific requirements of your project, such as data volume, performance needs,\nlanguage preferences, and ecosystem integration requirements. Both libraries\noffer powerful data processing capabilities, and selecting the right one can\nsignificantly impact the success and efficiency of your data-driven initiatives.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>After carefully evaluating the key differences between Pandas and Polars, the\nchoice between the two data processing libraries ultimately comes down to the\nspecific requirements of your project and use case.<br>For projects focused on interactive data exploration and working with smaller\ndatasets (under 1GB), Pandas remains the go-to choice. Its deep integration with\nthe broader Python scientific computing ecosystem, extensive documentation, and\nlarge community make it a reliable and familiar option for many data scientists\nand developers.<br>However, for large-scale data processing, production environments, and\ncross-language teams, Polars presents a compelling alternative. Its performance\nadvantages, memory efficiency, and multi-language support (Python, Node.js,\nRust) make it an increasingly attractive choice for modern data-intensive\napplications.<br>When deciding between Pandas and Polars, consider factors such as dataset size,\nperformance requirements, memory constraints, language preferences, and the\nlevel of ecosystem integration needed. Pandas may be the better fit for projects\nfocused on rapid prototyping and educational use, while Polars can shine in\nmission-critical, large-scale data processing tasks.<br>Ultimately, both Pandas and Polars are powerful data processing tools, and the\nchoice between them should be guided by the specific needs and constraints of\nyour project. As the data processing landscape continues to evolve, it&#39;s\nvaluable to stay informed about the trade-offs and emerging alternatives to\nensure you make the most informed decision for your team and organisation.</p>\n"
  },
  {
    "title": "📊 Ten Ways to Model Data",
    "date": "2024-11-27T00:00:00.000Z",
    "tags": [
      "modelling-mindsets",
      "data-science",
      "ai",
      "data-modeling",
      "neural-network",
      "best-practices",
      "statistics",
      "machine-learning",
      "decision-making"
    ],
    "url": "/posts/modelling-mindsets.html",
    "content": "<p><strong>TL;DR:</strong> This comprehensive guide explores ten distinct modelling approaches\nacross statistics, machine learning, and causal inference-advocating for\n&quot;T-shaped&quot; expertise where practitioners develop deep knowledge in one or two\nmindsets aligned with their domain needs whilst maintaining sufficient breadth\nto recognise when different approaches are required, with specific\nrecommendations for research, business, and product development contexts.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>As a practitioner looking to work effectively with real-world data and generate\nmeaningful insights, I face a crucial decision: which modelling approaches\nshould I invest my time and energy in learning? After discovering Christoph\nMolnar&#39;s\n<a href=\"https://christophmolnar.com/books/modeling-mindsets/\">Modeling Mindsets</a>, I\nrealised this isn&#39;t about picking the &quot;best&quot; approach. It&#39;s about becoming what\nhe calls a &quot;T-shaped modeller&quot;.<br>The concept is elegantly simple: rather than trying to master every possible\napproach (impossible) or limiting myself to just one (ineffective), I should aim\nto develop:</p>\n<ul>\n<li>Deep expertise in one or two mindsets that align with my goals and problems</li>\n<li>Working knowledge of other approaches to recognise when my primary tools\naren&#39;t optimal</li>\n</ul>\n<p>This systematic exploration serves two purposes:</p>\n<ol>\n<li>To understand the landscape: What are the main modelling mindsets available\ntoday? What are their core premises, strengths, and limitations?</li>\n<li>To make an informed choice: Which mindset(s) should I focus on mastering,\ngiven my goals and constraints?</li>\n</ol>\n<p>Each mindset represents a different way of approaching problems through data.\nFrom the probability-focused world of statistical modelling to the interactive\nrealm of reinforcement learning, from the causality-oriented approach to the\npattern-finding nature of unsupervised learning, each offers unique tools and\nperspectives.<br>By examining these mindsets systematically, I aim to make an informed decision\nabout where to focus my learning efforts while maintaining enough breadth to\nrecognise when I should switch approaches. This isn&#39;t just about theoretical\nunderstanding, it&#39;s about practical effectiveness in solving real-world\nproblems.</p>\n<p>Let&#39;s explore each mindset in turn, focusing on their fundamental premises, key\nstrengths, and limitations to guide this decision.</p>\n<h2 id=\"statistical-modelling-the-foundation-of-data-driven-inference\">Statistical Modelling: The Foundation of Data-Driven Inference</h2>\n<p><em>This mindset sees the world through probability distributions. At its core,\nit&#39;s about modelling how data is generated and making inferences under\nuncertainty.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Everything has a distribution, from dice rolls to customer behaviours</li>\n<li>Models encode assumptions about how data is generated</li>\n<li>Models are evaluated by both checking if their assumptions make sense and\nmeasuring how well they match the data</li>\n<li>Uses same data for fitting and evaluation, unlike machine learning approaches</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Provides rigorous mathematical framework for handling uncertainty</li>\n<li>Strong theoretical foundation spanning decades of research</li>\n<li>Forces explicit consideration of data-generating processes</li>\n<li>Versatile for decisions, predictions, and understanding</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Manual and often tedious modelling process</li>\n<li>Struggles with complex data types like images and text</li>\n<li>Good model fit doesn&#39;t guarantee good predictions</li>\n<li>Less automatable than modern machine learning approaches</li>\n</ol>\n<p>This mindset serves as the foundation for three important sub-approaches:\nFrequentism, Bayesianism, and Likelihoodism, each with its own interpretation of\nprobability and evidence. For someone starting in data science, understanding\nstatistical modelling provides crucial groundwork for understanding both\ntraditional statistics and modern machine learning approaches.</p>\n<h2 id=\"frequentism-making-decisions-through-repeated-experiments\">Frequentism: Making Decisions Through Repeated Experiments</h2>\n<p><em>Frequentism views probability as long-run frequency and assumes that parameters\nin the world are fixed but unknown. It&#39;s the dominant approach in many\nscientific fields, particularly in medicine and psychology.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Interprets probability as frequency in infinite repetitions</li>\n<li>Makes decisions through hypothesis tests and confidence intervals</li>\n<li>Relies on &quot;imagined experiments&quot; to draw conclusions</li>\n<li>Focuses on estimating fixed, true parameters</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Enables clear, binary decisions</li>\n<li>Computationally fast compared to other approaches</li>\n<li>No need for prior information</li>\n<li>Widely accepted in scientific research</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Often oversimplifies complex questions into yes/no decisions</li>\n<li>Vulnerable to p-hacking (searching for significant results)</li>\n<li>Interpretation can be counterintuitive, especially for confidence intervals</li>\n<li>Results depend on the experimental design, not just the data</li>\n</ol>\n<p>For practitioners, Frequentism offers a well-established framework with clear\ndecision rules and strong scientific acceptance. However, its limitations in\nhandling uncertainty and tendency toward oversimplification have led to growing\ninterest in alternative approaches like Bayesian inference.</p>\n<h2 id=\"bayesianism-learning-through-updated-beliefs\">Bayesianism: Learning Through Updated Beliefs</h2>\n<p><em>Bayesianism stands out by treating parameters themselves as random variables\nwith distributions, fundamentally different from Frequentism&#39;s fixed-parameter\nview. It focuses on updating beliefs about parameters as new data arrives.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Requires prior distributions before seeing data</li>\n<li>Updates beliefs through Bayes&#39; theorem</li>\n<li>Produces complete posterior distributions, not just point estimates</li>\n<li>Naturally propagates uncertainty through all calculations[^1]</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Can incorporate prior knowledge and expert opinions</li>\n<li>Provides complete probability distributions for parameters</li>\n<li>More intuitive interpretation of uncertainty</li>\n<li>Cleanly separates inference (getting posteriors) from decisions (using them)</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Choosing priors can be difficult and controversial</li>\n<li>Computationally intensive, especially for complex models</li>\n<li>Mathematically more demanding than frequentist approaches</li>\n<li>Can seem like overkill for simple decisions</li>\n</ol>\n<p>Bayesianism offers a more complete and intuitive framework for handling\nuncertainty, but requires more computational resources and mathematical\nsophistication. It&#39;s particularly valuable when prior knowledge is important or\nwhen understanding full uncertainty is crucial.</p>\n<h2 id=\"likelihoodism-pure-evidence-through-likelihood\">Likelihoodism: Pure Evidence Through Likelihood</h2>\n<p><em>Likelihoodism attempts to reform statistical inference by focusing solely on\nlikelihood as evidence, avoiding both Frequentism&#39;s imagined experiments and\nBayesianism&#39;s subjective priors.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Uses likelihood ratios to compare hypotheses</li>\n<li>Adheres strictly to the likelihood principle</li>\n<li>Rejects both prior probabilities and sampling distributions</li>\n<li>Compares models based on their relative evidence</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>More coherent than Frequentism&#39;s mixed toolkit</li>\n<li>Avoids subjective elements of Bayesianism</li>\n<li>Ideas work well within other statistical mindsets</li>\n<li>Adheres to likelihood principle (evidence depends only on observed data)</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Cannot make absolute statements, only relative comparisons</li>\n<li>No clear mechanism for making final decisions</li>\n<li>Lacks tools for expressing beliefs or uncertainty</li>\n<li>Less practical than other statistical approaches</li>\n</ol>\n<p>Likelihoodism offers interesting theoretical insights but may be less\nimmediately useful than Frequentist or Bayesian approaches. It&#39;s more valuable\nfor understanding the foundations of statistical inference than for day-to-day\ndata analysis.</p>\n<h2 id=\"causal-inference-from-association-to-causation\">Causal Inference: From Association to Causation</h2>\n<p><em>Causal inference moves beyond correlation to understand what actually causes\nobserved effects, providing a framework for analysing interventions and their\nimpacts.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Uses Directed Acyclic Graphs (DAGs) to visualise relationships</li>\n<li>Distinguishes between association and causation</li>\n<li>Requires explicit encoding of causal assumptions</li>\n<li>Can work with both statistical models and machine learning</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Addresses fundamental questions about cause and effect</li>\n<li>Makes assumptions explicit through DAGs</li>\n<li>Models tend to be more robust than pure association-based approaches</li>\n<li>Provides clear framework for analysing interventions</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Requires identifying all relevant confounders</li>\n<li>Cannot verify all causal assumptions from data alone</li>\n<li>Multiple competing frameworks can confuse newcomers</li>\n<li>May sacrifice predictive performance for causal understanding</li>\n</ol>\n<p>For practitioners, causal inference is essential when decisions about\ninterventions are needed, though it requires careful consideration of\nassumptions and domain knowledge. It&#39;s particularly valuable in fields like\nmedicine, policy-making, and business strategy where understanding cause-effect\nrelationships is crucial.</p>\n<h2 id=\"machine-learning-algorithms-learning-from-data\">Machine Learning: Algorithms Learning from Data</h2>\n<p><em>Machine learning approaches problems by having computers learn algorithms from\ndata, focusing on task performance rather than theoretical underpinning.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Computer-first approach to learning from data</li>\n<li>External evaluation based on task performance</li>\n<li>Less constrained by statistical assumptions</li>\n<li>Includes supervised, unsupervised, reinforcement, and deep learning</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Task-oriented and pragmatic approach</li>\n<li>Highly automatable</li>\n<li>Well-suited for building digital products</li>\n<li>Strong industry adoption and career opportunities</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Less principled than statistical approaches</li>\n<li>Many competing approaches can be overwhelming</li>\n<li>Models often prioritise performance over interpretability</li>\n<li>Usually requires substantial data and computation</li>\n</ol>\n<p>For practitioners, machine learning offers powerful tools for automation and\nprediction, particularly valuable in industry settings. It&#39;s especially useful\nwhen theoretical understanding is less important than practical performance.</p>\n<h3 id=\"supervised-learning-the-art-of-prediction\">Supervised Learning: The Art of Prediction</h3>\n<p><em>Supervised learning frames everything as a prediction problem, using labelled\ndata to learn mappings from inputs to outputs.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Learning is optimisation and search in hypothesis space</li>\n<li>Models evaluated on unseen data, not training data</li>\n<li>Focuses on generalising to new cases</li>\n<li>Highly automatable and competition-friendly</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Clear evaluation metrics</li>\n<li>Highly automatable</li>\n<li>Strong performance on prediction tasks</li>\n<li>Well-defined optimisation objectives</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Requires labelled data</li>\n<li>Models often black-box (uninterpretable)</li>\n<li>Not hypothesis-driven</li>\n<li>May miss causal relationships</li>\n<li>Can fail in unexpected ways when patterns change</li>\n</ol>\n<p>For practitioners, supervised learning excels in prediction tasks where good\nlabelled data exists and interpretability isn&#39;t crucial. It&#39;s particularly\nvaluable in industry settings for automation and decision support.</p>\n<h3 id=\"unsupervised-learning-discovering-hidden-patterns\">Unsupervised Learning: Discovering Hidden Patterns</h3>\n<p><em>This mindset focuses on finding inherent structures in data without labelled\noutcomes, making it ideal for exploratory analysis and pattern discovery.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Discovers patterns in data distributions</li>\n<li>Includes clustering, dimensionality reduction, anomaly detection</li>\n<li>No ground truth for validation</li>\n<li>More open-ended than supervised learning</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Finds patterns other approaches might miss</li>\n<li>Excellent for initial data exploration</li>\n<li>Flexible for undefined problems</li>\n<li>Can reveal natural groupings in data</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Hard to validate results objectively</li>\n<li>Feature weighting is often arbitrary</li>\n<li>Suffers from curse of dimensionality[^2]</li>\n<li>No guarantee of finding meaningful patterns</li>\n</ol>\n<p>For practitioners, unsupervised learning is valuable for initial data\nexploration and when labelled data isn&#39;t available. It&#39;s particularly useful in\ncustomer segmentation, anomaly detection, and dimension reduction.</p>\n<h3 id=\"reinforcement-learning-learning-through-interaction\">Reinforcement Learning: Learning Through Interaction</h3>\n<p><em>This mindset models an agent interacting with an environment, making decisions\nand learning from rewards.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Agent learns by taking actions and receiving rewards</li>\n<li>Handles delayed and sparse rewards</li>\n<li>Balances exploration and exploitation</li>\n<li>Creates its own training data through interaction</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Models dynamic real-world interactions</li>\n<li>Excellent for sequential decision-making</li>\n<li>Can discover novel strategies</li>\n<li>Learns through direct experience</li>\n<li>Combines well with deep learning</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Not all problems fit agent-environment framework</li>\n<li>Often unstable or difficult to train</li>\n<li>May perform poorly in real-world conditions</li>\n<li>Requires careful reward design</li>\n<li>Complex implementation choices</li>\n</ol>\n<p>For practitioners, reinforcement learning is valuable for problems involving\nsequential decisions or control, particularly in robotics, game playing, and\nresource management.</p>\n<h3 id=\"deep-learning-end-to-end-neural-networks\">Deep Learning: End-to-End Neural Networks</h3>\n<p><em>This mindset approaches problems through deep neural networks, letting the\nmodel learn both features and relationships.</em></p>\n<p>Key Aspects:</p>\n<ul>\n<li>Models tasks end-to-end through neural networks</li>\n<li>Learns hierarchical representations automatically</li>\n<li>Highly modular architecture design</li>\n<li>Benefits from transfer learning and pre-trained models</li>\n</ul>\n<p>Primary Strengths:</p>\n<ol>\n<li>Excels at complex data (images, text, speech)</li>\n<li>Learns useful feature representations</li>\n<li>Highly modular and customisable</li>\n<li>Strong tooling and community support</li>\n<li>Can handle multiple inputs/outputs seamlessly</li>\n</ol>\n<p>Notable Limitations:</p>\n<ol>\n<li>Underperforms on tabular data versus tree methods</li>\n<li>Requires large amounts of data</li>\n<li>Computationally intensive</li>\n<li>Hard to train and tune effectively</li>\n<li>Results can be difficult to interpret</li>\n</ol>\n<p>For practitioners, deep learning is essential for complex data types but may be\noverkill for simpler problems. Most valuable in computer vision, natural\nlanguage processing, and other complex pattern recognition tasks.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p><em><strong>aka Choosing Your Modelling Path</strong></em></p>\n<p>For developing T-shaped expertise in modelling, the practitioner&#39;s choice should\nalign with their primary domain while maintaining broader awareness. Here&#39;s how\nto approach this decision:</p>\n<ul>\n<li><p><em>Scientific Research</em> demands Statistical Modelling for its rigorous\nuncertainty quantification and established peer review frameworks.</p>\n</li>\n<li><p><em>Business Predictions</em> benefit most from Supervised Learning, optimising\nprediction accuracy while enabling automation and scalability.</p>\n</li>\n<li><p><em>Complex Data</em> (images/text) requires Deep Learning to handle unstructured\ndata and learn hierarchical features effectively.</p>\n</li>\n<li><p><em>Interventions/Policies</em> need Causal Inference to distinguish correlation from\ncausation and understand intervention effects.</p>\n</li>\n<li><p><em>Control Systems</em> thrive with Reinforcement Learning for sequential decisions\nand environment interaction.</p>\n</li>\n</ul>\n<p>For practical applications, certain combinations prove particularly effective:</p>\n<ul>\n<li><p><em>Industry/Business</em> combines Supervised Learning with Unsupervised Learning,\nenabling accurate predictions while discovering valuable patterns in customer\ndata.</p>\n</li>\n<li><p><em>Research</em> pairs Statistical Modelling with Machine Learning, balancing\nacademic rigour with modern capabilities.</p>\n</li>\n<li><p><em>Product Development</em> merges Deep Learning with Supervised Learning for\nend-to-end features with clear metrics.</p>\n</li>\n<li><p><em>Medical Diagnostics</em> unites Supervised Learning with Statistical Modelling,\ncrucial for evidence-based decisions with proper uncertainty quantification.</p>\n</li>\n</ul>\n<p>The choice should be based on the practitioner&#39;s domain requirements,\ncomputational resources, interpretability needs, and available time for mastery.\n<em>Remember:</em> Mastery of one mindset with broad awareness surpasses superficial\nknowledge of many.</p>\n<hr>\n<p>[^1]: Because Bayesian models treat everything as probability distributions\n    (rather than fixed values), any predictions or conclusions automatically\n    include their associated uncertainty. For example, if you predict someone&#39;s\n    future income using multiple uncertain factors, the final prediction comes\n    as a range of possibilities with their probabilities, rather than just a\n    single number.</p>\n<p>[^2]: Here is a\n    <a href=\"https://bsky.app/profile/chrisalbon.com/post/3lbendflq2w2n\">nice digital flashcard</a>\n    by Chris Albon, on the concept of <em>curse of dimensionality</em></p>\n"
  },
  {
    "title": "💡 TIL: Pydantic, Python's Data Validation Guard",
    "date": "2024-11-26T00:00:00.000Z",
    "tags": [
      "til",
      "data-validation",
      "python",
      "type-checking",
      "data-modeling",
      "code-quality",
      "pydantic",
      "error-handling"
    ],
    "url": "/posts/TIL-pydantic.html",
    "content": "<p><strong>TL;DR:</strong> Pydantic transforms Python&#39;s type hints from passive documentation\ninto active runtime validators, automatically converting and validating data\ntypes while providing intelligent error handling-significantly reducing\nboilerplate code and catching potential errors at system boundaries for more\nreliable API development, configuration management, and data processing\npipelines.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Today I started using <a href=\"https://docs.pydantic.dev/latest/\">Pydantic</a>, a Python\nlibrary that handles data validation through Python type annotations. Pydantic\nbrings runtime type checking and data validation that catches errors before they\ncause mysterious bugs in an application. It uses type hints to validate data at\nruntime, automatically converting and validating data types, preventing bugs,\nand reducing boilerplate code. It&#39;s essential for robust API development,\nconfiguration management, and data processing pipelines.</p>\n<h2 id=\"understanding-pydantic-and-its-value\">Understanding Pydantic and Its Value</h2>\n<p>Pydantic leverages Python&#39;s type hints to validate data structures. It converts\nyour type hints from mere documentation into active runtime checks, ensuring\ndata consistency throughout your application. Here are Pydantic&#39;s key features:</p>\n<h3 id=\"type-enforcement\">Type Enforcement</h3>\n<pre><code class=\"language-python\">from pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n    email: str\n\n# This raises a ValidationError\nuser = User(name=&quot;John&quot;, age=&quot;not_a_number&quot;, email=&quot;john@example.com&quot;)\n</code></pre>\n<h3 id=\"automatic-type-coercion\">Automatic Type Coercion</h3>\n<pre><code class=\"language-python\">class Order(BaseModel):\n    quantity: int\n    price: float\n\n# Pydantic automatically converts valid strings to numbers\norder = Order(quantity=&quot;3&quot;, price=&quot;9.99&quot;)\nprint(order.quantity)  # 3 (int)\nprint(order.price)    # 9.99 (float)\n</code></pre>\n<h3 id=\"real-world-benefits\">Real-World Benefits</h3>\n<ul>\n<li><strong>API Development</strong>: Validates incoming JSON data automatically</li>\n<li><strong>Configuration Management</strong>: Ensures config files meet your specifications</li>\n<li><strong>Database Operations</strong>: Validates data before insertion</li>\n<li><strong>Data Parsing</strong>: Converts between JSON, dictionaries, and model instances\nseamlessly</li>\n</ul>\n<h3 id=\"why-it-matters\">Why It Matters</h3>\n<ol>\n<li><strong>Error Prevention</strong>: Catches data issues at system boundaries</li>\n<li><strong>Clean Code</strong>: Reduces validation boilerplate</li>\n<li><strong>Self-Documenting</strong>: Type hints serve as both validation rules and\ndocumentation</li>\n<li><strong>Performance</strong>: Compiled validation code runs efficiently</li>\n</ol>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Pydantic transforms Python&#39;s type hints from passive documentation into active\ndata validators, significantly reducing runtime errors and improving code\nreliability.</p>\n"
  },
  {
    "title": "🏺 Historical Evolution of AI",
    "date": "2024-11-23T00:00:00.000Z",
    "tags": [
      "ai",
      "evolution",
      "llm",
      "symbolic",
      "neural-network",
      "data-science"
    ],
    "url": "/posts/ai-evolution.html",
    "content": "<p><strong>TL;DR:</strong> This historical overview traces AI&#39;s evolution through four major\nparadigms: from symbolic reasoning and expert systems (1950s-1970s), through\nneural networks and Bayesian approaches (1980s-1990s), to the Big Data\nrevolution (2000s-2010s), culminating in today&#39;s integrated systems that combine\nmultiple philosophical approaches-suggesting future progress requires unifying\nthese diverse methodologies rather than choosing between them.</p>\n<!--more-->\n\n<h1 id=\"ais-historical-evolution\">AI&#39;s Historical Evolution</h1>\n<h2 id=\"introduction\">Introduction</h2>\n<p>The field of Artificial Intelligence has undergone several paradigm shifts since\nits inception, each representing a distinct approach to creating intelligent\nsystems. Drawing from Pedro Domingos&#39; framework in\n<a href=\"https://en.wikipedia.org/wiki/The_Master_Algorithm\">The Master Algorithm</a> we\ncan trace how different schools of thought have shaped our understanding and\nimplementation of AI technologies.</p>\n<h2 id=\"historical-evolution\">Historical Evolution</h2>\n<h3 id=\"early-foundations-symbolic-ai-and-expert-systems-1950s-1970s\">Early Foundations: Symbolic AI and Expert Systems (1950s-1970s)</h3>\n<p>The pioneers of AI began with symbolic reasoning, believing intelligence could\nbe reduced to symbol manipulation. This <em>symbolist</em> approach offered explicit\nreasoning chains and interpretability but struggled with real-world complexity.\nExpert Systems followed, successfully applying rule-based reasoning to narrow\ndomains while revealing the challenges of scaling knowledge-based systems.</p>\n<h3 id=\"the-rise-of-neural-approaches-1980s-1990s\">The Rise of Neural Approaches (1980s-1990s)</h3>\n<p>The <em>connectionist</em> movement emerged with neural networks, drawing inspiration\nfrom biological systems. This era introduced pattern recognition capabilities\nand learning from examples. Simultaneously, the <em>Bayesian</em> school brought\nstatistical methods to the forefront, offering principled approaches to handling\nuncertainty but requiring significant data and computational resources.</p>\n<h3 id=\"the-data-revolution-2000s-2010s\">The Data Revolution (2000s-2010s)</h3>\n<p>Big Data and Deep Learning foundations emerged as the <em>analogiser</em> school gained\nprominence. This period saw the convergence of massive datasets, computational\npower, and sophisticated architectures. Deep Learning breakthrough demonstrated\nthe power of automatic feature learning, though at the cost of increased\ncomputational demands and reduced interpretability.</p>\n<h3 id=\"contemporary-ai-the-era-of-integration-2020s\">Contemporary AI: The Era of Integration (2020s)</h3>\n<p>Current AI systems, particularly large language models, represent a synthesis of\nmultiple schools. They combine symbolic reasoning[^1], neural architectures[^2],\nand statistical learning[^3], achieving impressive generative capabilities and\nfew-shot learning. However, they face challenges in resource requirements,\nreliability, and alignment with human values. A nicely distilled overview of\nwhat is today&#39;s AI, comes from an\n<a href=\"https://xcancel.com/karpathy/status/1864033537479135369\">Andrej Karpathy Tweet</a>.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The evolution of AI reveals a field shaped by competing philosophies, each\ncontributing essential insights. As Domingos argues, the future likely lies not\nin the dominance of any single approach but in their unification. While recent\nadvances demonstrate the potential of synthesising different methods,\nsignificant challenges remain in creating truly intelligent systems that are\nboth powerful and reliable.</p>\n<p>The path forward requires building on these foundations while addressing core\nchallenges in efficiency, interpretability, and alignment. Rather than choosing\nbetween different schools of thought, the field must continue to integrate their\nstrengths while mitigating their individual weaknesses.</p>\n<hr>\n<p>[^1]: Symbolic reasoning in modern AI manifests through attention mechanisms and\n    transformers&#39; ability to process structured input like code or mathematical\n    expressions. While not explicitly rule-based like early AI, these systems\n    can learn and apply symbolic patterns.</p>\n<p>[^2]: Neural architectures in contemporary AI primarily use the transformer\n    architecture, where self-attention layers process information in parallel,\n    allowing the model to weigh the importance of different inputs contextually.</p>\n<p>[^3]: Statistical learning appears in the form of probabilistic token prediction\n    and the use of large-scale statistical patterns learned during training.\n    Models learn probability distributions over sequences, enabling them to\n    generate coherent outputs.</p>\n"
  },
  {
    "title": "🔄 Considering Iterative Refinement Over Unit Testing",
    "date": "2024-11-22T00:00:00.000Z",
    "tags": [
      "fast-ai",
      "answer-ai",
      "iterative-refinement",
      "doctests",
      "best-practices",
      "llm",
      "dialogue-engineering",
      "code-quality"
    ],
    "url": "/posts/iterative-refinement.html",
    "content": "<p><strong>TL;DR:</strong> Drawing inspiration from Norvig, Howard, and Sanderson, this article\nadvocates for iterative refinement over traditional unit testing, emphasising\ntechniques like doctests that keep verification close to code-reducing\nmaintenance burden whilst improving reliability by focusing on actual usage\npatterns rather than rigid test-driven development that can lead to outdated\ntests and ossified code structures.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>In the realm of software development and related fields, three influential\nfigures -Peter Norvig (former Director of Research at Google), Jeremy Howard\n(founder of fast.ai), and Grant Sanderson (creator of 3Blue1Brown)- demonstrate\nthe power of iterative refinement over rigid test-driven development. Their\napproaches, while applied in different domains, share common principles that\nchallenge traditional development practices.</p>\n<h2 id=\"iterative-refinement\">Iterative Refinement</h2>\n<h3 id=\"peter-norvigs-software-development\">Peter Norvig&#39;s Software Development</h3>\n<p>Norvig&#39;s approach, demonstrated in both his\n<a href=\"https://norvig.com/docex.html\">original <code>docex</code> module</a> and his\n<a href=\"https://norvig.com/spell-correct.html\">spell corrector implementation</a>,\nemphasises tests that are tightly coupled with the code they verify. Before\nPython&#39;s doctests[^1] were officially supported, he created the <code>docex</code> module\nspecifically to write tests in docstrings using a concise syntax like</p>\n<pre><code class=\"language-python\">def factorial(n):\n    &quot;&quot;&quot;Return the factorial of n, an exact integer &gt;= 0.\n       &gt;&gt;&gt; [factorial(n) for n in range(6)]\n       [1, 1, 2, 6, 24, 120]\n       It must also not be ridiculously large:\n       &gt;&gt;&gt; factorial(1e100)\n       Traceback (most recent call last):\n       ...\n       OverflowError: n too large\n    &quot;&quot;&quot;\n    ...\n\nif __name__ == &quot;__main__&quot;:\n    import doctest\n    doctest.testmod()\n</code></pre>\n<pre><code class=\"language-console\">$ python fact.py -v\nTrying:\n    [factorial(n) for n in range(6)]\nExpecting:\n    [1, 1, 2, 6, 24, 120]\nok\nTrying:\n    factorial(1e100)\nExpecting:\n    Traceback (most recent call last):\n        ...\n    OverflowError: n too large\nok\n2 items passed all tests:\n   1 test in __main__\n   6 tests in __main__.factorial\n7 tests in 2 items.\n7 passed.\nTest passed.\n$\n</code></pre>\n<p>Even in his spell corrector, Norvig uses simple functions with in-line test\ncases rather than separate test files. This approach keeps tests close to the\ncode they verify, making them part of the living documentation rather than\nseparate artefacts that can drift out of sync.<br><em>Update: While randomly skimming through PyTorch code, it was good to stumble\nacross examples of\n<a href=\"https://github.com/pytorch/pytorch/blob/main/torch/autograd/grad_mode.py\">code containing doctests</a>.</em></p>\n<h3 id=\"jeremy-howards-machine-learning-development\">Jeremy Howard&#39;s Machine Learning Development</h3>\n<p>Howard&#39;s methodology, evidenced in fast.ai&#39;s development and his book &quot;Deep\nLearning for Coders&quot; advocates for rapid prototyping in notebooks. His emphasis\nlies in getting end-to-end solutions working quickly, then iteratively improving\nthem based on actual usage patterns. In his latest course\n<a href=\"https://solveit.fast.ai/\">SolveIt</a>, Howard extends this iterative philosophy to\n[Dialogue Engineering]({{ site.baseurl }}{% link\n_posts/2024-11-15-dialogue-engineering.md %}), i.e. using Large Language Models\nin an iterative conversation to develop solutions, demonstrating how modern AI\ncan be integrated into the development workflow while maintaining the principles\nof continuous refinement.</p>\n<h3 id=\"grant-sandersons-visual-mathematics\">Grant Sanderson&#39;s Visual Mathematics</h3>\n<p>This iterative philosophy extends to mathematical animations. In Grant\nSanderson&#39;s <a href=\"https://www.youtube.com/watch?v=rbu7Zu5X1zI\">How I animate</a> video,\nhe demonstrates how he builds visualisations incrementally, starting with basic\nshapes and gradually refining them while continuously previewing the results.\nThis approach allows for creative exploration while maintaining momentum.</p>\n<h3 id=\"the-problem-with-traditional-testing\">The Problem with Traditional Testing</h3>\n<p>Traditional unit testing often fragments development workflow by requiring\nseparate test maintenance and can lead to ossified code structures. When tests\naren&#39;t exercised regularly, they become outdated, creating false confidence.\nThis is particularly problematic in rapidly evolving domains like AI, where\ninterfaces and requirements frequently change.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Instead of extensive unit test suites, it&#39;s worth considering:</p>\n<ol>\n<li>Writing working code first</li>\n<li>Using doctests for critical functions</li>\n<li>Relying on end-to-end validation</li>\n<li>Refactoring based on actual usage patterns</li>\n<li>Keeping tests focused on stable interfaces</li>\n</ol>\n<p>This approach reduces maintenance burden while ensuring code remains reliable\nwhere it matters most, that is in production.</p>\n<p>&quot;<em>Programs must be written for people to read, and only incidentally for\nmachines to execute</em>&quot; - Abelson &amp; Sussman. The same applies to tests.</p>\n<hr>\n<p>[^1]: Python has supported\n    <a href=\"https://docs.python.org/3/library/doctest.html\">doctests</a> natively since\n    v2.6.9</p>\n"
  },
  {
    "title": "シ Back to Basics: A Modern, Minimal Python Toolchain",
    "date": "2024-11-21T00:00:00.000Z",
    "tags": [
      "python",
      "type-checking",
      "code-quality",
      "github-actions",
      "ci-cd",
      "cross-platform",
      "minimal",
      "toolchain"
    ],
    "url": "/posts/bring-it-back-to-basics.html",
    "content": "<p><strong>TL;DR:</strong> This article presents a streamlined Python toolchain that reduces\ncognitive load while maintaining the language&#39;s data science capabilities,\nfeaturing Rust-based tools like uv (package manager) and Ruff\n(linter/formatter), along with pyright for type checking-all configured through\na single pyproject.toml file and complemented by essential libraries for data\nprocessing, visualisation, and AI development.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Python&#39;s ecosystem for Data Science and AI is unmatched in its depth and\nmaturity. Yet, its fragmented tooling landscape often leads to decision\nparalysis and opinions galore: virtualenv or venv? pip or conda? black or\nflake8? These choices, while providing flexibility, can create unnecessary\ncognitive load and often foster dogmatic opinions about &quot;the right way&quot; to do\nthings. After exploring alternative stacks, I&#39;m returning to Python. Not least\nbecause it&#39;s perfect, but because it&#39;s productive. The challenge isn&#39;t Python&#39;s\ncapabilities; it&#39;s the abundance and complexity of its tooling. This article\npresents a carefully curated, minimal toolkit that leverages Python&#39;s ecosystem\nwhile avoiding its common setup pitfalls.</p>\n<h2 id=\"motivation\">Motivation</h2>\n<p>The appeal of integrated toolchains like Deno 2.0 is undeniable. Zero setup,\nimmediate productivity, and a cohesive development experience. My recent\nexploration of alternative stacks revealed the value of unified tools that just\nwork. While JavaScript&#39;s ecosystem for Data Science and AI is growing rapidly,\nit still lacks the depth and maturity of Python&#39;s scientific computing stack.<br>This exploration led to an important realisation: aside from an expansive Data\nand AI ecosystem, Python development can be achieved with a streamlined workflow\nthat increases productivity and decreases complexity. Rather than accepting the\ncognitive overhead of multiple competing tools, I decided to create my own\ncompact toolchain that meets most Data Science and AI requirements with\nminimalism, simplicity, and clarity in mind. The goal isn&#39;t to prescribe another\n&quot;right way&quot; of doing things, but rather to demonstrate how a carefully chosen\nset of modern tools can create a development experience that rivals the\nintegrated approaches of newer platforms while leveraging Python&#39;s mature\necosystem.</p>\n<h2 id=\"my-approach\">My Approach</h2>\n<h3 id=\"local-development\">Local Development</h3>\n<p>My toolchain starts with the following foundational choices that eliminate\ncommon Python setup headaches:</p>\n<ol start=\"0\">\n<li><p><a href=\"https://peps.python.org/pep-0008/\">PEP8</a>: Let&#39;s start with a style guide, so\nthat the team is on the same page</p>\n</li>\n<li><p><a href=\"https://docs.astral.sh/uv/\">uv</a>: A blazing-fast Python package and project\nmanager, written in Rust. It replaces pip, pip-tools, pipx, poetry, pyenv,\ntwine, virtualenv, and more, providing:</p>\n<ul>\n<li>Consistent dependency resolution</li>\n<li>Lightning-fast package installations</li>\n<li>Built-in virtual environment management</li>\n<li>Direct integration with <code>pyproject.toml</code></li>\n</ul>\n</li>\n<li><p><a href=\"https://packaging.python.org/en/latest/guides/writing-pyproject-toml/\"><code>pyproject.toml</code></a>:\nThe single source of truth for project configuration. For example:</p>\n</li>\n</ol>\n<pre><code class=\"language-toml\">    [project]\n    name = &quot;my-ds-project&quot;\n    version = &quot;0.1.0&quot;\n    dependencies = [\n        &quot;polars&quot;,\n        &quot;tensorflow&quot;,\n        &quot;plotly&quot;\n    ]\n\n    [tool.ruff]\n    line-length = 90 \n    select = [&quot;E&quot;, &quot;F&quot;, &quot;I&quot;]\n\n    # Required only if you use pytest for unit testing\n    [tool.pytest.ini_options]\n    testpaths = [&quot;tests&quot;]\n</code></pre>\n<ol start=\"3\">\n<li><p><a href=\"https://docs.astral.sh/ruff/\">Ruff</a>: A Rust-based tool that combines\nformatting and linting, replacing the need for black, flake8, isort etc.:</p>\n<ul>\n<li>Single-tool code quality enforcement</li>\n<li>Configurable through <code>pyproject.toml</code></li>\n<li>Significantly faster than Python-based alternatives</li>\n</ul>\n</li>\n<li><p><a href=\"https://github.com/microsoft/pyright\">pyright</a>: Static Type Checker for\nPython</p>\n<ul>\n<li>Static type checker</li>\n<li><a href=\"https://htmlpreview.github.io/?https://github.com/python/typing/blob/main/conformance/results/results.html\">Standards</a>\ncompliant</li>\n<li><a href=\"https://microsoft.github.io/pyright/#/configuration?id=sample-pyprojecttoml-file\">Configurable</a>\nwithin <code>pyproject.toml</code></li>\n</ul>\n</li>\n<li><p>[iterative refinement]({{ site.baseurl }}{% link\n_posts/2024-11-22-iterative-refinement.md %}): An approach that tightly\ncouples (doc)tests with code, ensuring\n<a href=\"https://www.merriam-webster.com/thesaurus/up-to-dateness\">up-to-dateness</a><br><del><a href=\"https://docs.pytest.org/en/stable/\">pytest</a>: Handles testing with minimal\nboilerplate and rich assertions</del></p>\n</li>\n</ol>\n<h3 id=\"cross-platform-distribution\">Cross-Platform Distribution</h3>\n<ol>\n<li>PyInstaller for creating stand-alone executables</li>\n<li>GitHub Actions workflow for automated builds:</li>\n</ol>\n<pre><code class=\"language-yaml\">- name: Build executables\n  run: |\n    pyinstaller --onefile src/main.py\n</code></pre>\n<ol start=\"3\">\n<li>Local cross-compilation using <a href=\"https://podman.io/\">Podman</a>:</li>\n</ol>\n<pre><code class=\"language-Dockerfile\">FROM python:3.13-slim\nCOPY . /app\nWORKDIR /app\nRUN pip install pyinstaller\nCMD pyinstaller --onefile src/main.py\n</code></pre>\n<h3 id=\"data-science\">Data Science</h3>\n<p>A carefully selected set of powerful libraries that minimize overlap:</p>\n<ul>\n<li><a href=\"https://pola.rs/\">Polars</a>: Fast DataFrame operations with a cohesive API.\n<a href=\"https://xcancel.com/charliermarsh/status/1860388882015223835\">Why?</a></li>\n</ul>\n<pre><code class=\"language-python\">    import polars as pl\n\n    def analyse_customer_behavior(path: str):\n        return (\n            pl.scan_parquet(path)\n            .with_columns([\n                pl.col(&quot;purchase_date&quot;).str.to_datetime(),\n                (pl.col(&quot;amount&quot;) * pl.col(&quot;quantity&quot;)).alias(&quot;total_spend&quot;)\n            ])\n            .group_by([\n                pl.col(&quot;customer_id&quot;),\n                pl.col(&quot;purchase_date&quot;).dt.month().alias(&quot;month&quot;)\n            ])\n            .agg([\n                pl.col(&quot;total_spend&quot;).sum().alias(&quot;monthly_spend&quot;),\n                pl.col(&quot;product_id&quot;).n_unique().alias(&quot;unique_products&quot;),\n                pl.col(&quot;purchase_date&quot;).count().alias(&quot;purchase_frequency&quot;)\n            ])\n            .sort([&quot;customer_id&quot;, &quot;month&quot;])\n            .collect()\n        )\n</code></pre>\n<ul>\n<li><a href=\"https://www.tensorflow.org/\">TensorFlow 2</a>: Deep learning when needed</li>\n</ul>\n<pre><code class=\"language-python\">    import tensorflow as tf\n    mnist = tf.keras.datasets.mnist\n\n    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n    x_train, x_test = x_train / 255.0, x_test / 255.0\n\n    model = tf.keras.models.Sequential([\n      tf.keras.layers.Flatten(input_shape=(28, 28)),\n      tf.keras.layers.Dense(128, activation=&#39;relu&#39;),\n      tf.keras.layers.Dropout(0.2),\n      tf.keras.layers.Dense(10, activation=&#39;softmax&#39;)\n    ])\n\n    model.compile(optimiser=&#39;adam&#39;,\n      loss=&#39;sparse_categorical_crossentropy&#39;,\n      metrics=[&#39;accuracy&#39;])\n\n    model.fit(x_train, y_train, epochs=5)\n    model.evaluate(x_test, y_test)\n</code></pre>\n<ul>\n<li><a href=\"https://xgboost.ai/\">XGBoost</a>: Gradient boosting for structured data</li>\n</ul>\n<pre><code class=\"language-python\">from xgboost import XGBClassifier\n# read data\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\ndata = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(data[&#39;data&#39;], data[&#39;target&#39;], test_size=.2)\n# create model instance\nbst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective=&#39;binary:logistic&#39;)\n# fit model\nbst.fit(X_train, y_train)\n# make predictions\npreds = bst.predict(X_test)\n</code></pre>\n<ul>\n<li><a href=\"https://plotly.com/python/\">Plotly</a>: Interactive visualizations</li>\n</ul>\n<pre><code class=\"language-python\">import plotly.express as px\ndf = px.data.iris()\nfig = px.scatter(df, x=&quot;sepal_width&quot;, y=&quot;sepal_length&quot;, color=&quot;species&quot;, symbol=&quot;species&quot;)\nfig.show()\n</code></pre>\n<ul>\n<li><a href=\"https://mlflow.org\">MLFlow</a>: Managing the Machine Learning Lifecycle</li>\n</ul>\n<center>\n    <img src=\"https://raw.githubusercontent.com/ai-mindset/ai-mindset.github.io/refs/heads/main/images/40_MLFlow.png\"/>\n</center><br />\n\n<h3 id=\"ai-engineering\">AI Engineering</h3>\n<p>With hybrid solutions becoming more prevalent nowadays, we can use a combination\nof tools.</p>\n<ul>\n<li><a href=\"https://ollama.com/\">Ollama</a>: Local model deployment and inference</li>\n</ul>\n<pre><code class=\"language-python\">    import ollama\n\n    def technical_advisor():\n        messages = [\n            {\n                &quot;role&quot;: &quot;system&quot;,\n                &quot;content&quot;: &quot;You are a technical advisor specializing in Python architecture.&quot;\n            },\n            {\n                &quot;role&quot;: &quot;user&quot;,\n                &quot;content&quot;: &quot;What&#39;s the best way to handle database migrations?&quot;\n            }\n        ]\n        \n        response = ollama.chat(model=&#39;llama2&#39;, messages=messages)\n        messages.append(response[&#39;message&#39;])\n        \n        # Follow-up question with context\n        messages.append({\n            &quot;role&quot;: &quot;user&quot;,\n            &quot;content&quot;: &quot;How would that work with SQLAlchemy specifically?&quot;\n        })\n        \n        return ollama.chat(model=&#39;llama2&#39;, messages=messages)\n</code></pre>\n<ul>\n<li><a href=\"https://docs.llamaindex.ai/\">LlamaIndex</a>: RAG pipeline construction using\nlocal LLMs or external APIs</li>\n</ul>\n<pre><code class=\"language-python\">    from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n    from llama_index.core.node_parser import SentenceSplitter\n    from llama_index.core.retrievers import VectorIndexRetriever\n    from llama_index.core.query_engine import RetrieverQueryEngine\n\n    def create_custom_rag():\n        # Load and parse documents\n        documents = SimpleDirectoryReader(&quot;technical_docs&quot;).load_data()\n        parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n        nodes = parser.get_nodes_from_documents(documents)\n        \n        # Create index with custom settings\n        index = VectorStoreIndex(nodes)\n        \n        # Custom retriever with similarity threshold\n        retriever = VectorIndexRetriever(\n            index=index,\n            similarity_top_k=3,\n            filters=lambda x: float(x.get_score()) &gt; 0.7\n        )\n        \n        # Create query engine with custom retriever\n        query_engine = RetrieverQueryEngine(retriever=retriever)\n        return query_engine\n</code></pre>\n<ul>\n<li><a href=\"https://www.mongodb.com/\">MongoDB</a>: A distributed document DB that supports\nvector storage and graph operations</li>\n</ul>\n<pre><code class=\"language-python\">    from pymongo import MongoClient\n    import numpy as np\n\n    def vector_search(text_embedding: np.ndarray, threshold: float = 0.8):\n        client = MongoClient(&quot;mongodb://localhost:27017/&quot;)\n        db = client.vector_db\n        \n        pipeline = [\n            {\n                &quot;$search&quot;: {\n                    &quot;index&quot;: &quot;vector_index&quot;,\n                    &quot;knnBeta&quot;: {\n                        &quot;vector&quot;: text_embedding.tolist(),\n                        &quot;path&quot;: &quot;embedding&quot;,\n                        &quot;k&quot;: 5\n                    }\n                }\n            },\n            {\n                &quot;$match&quot;: {\n                    &quot;score&quot;: {&quot;$gt&quot;: threshold}\n                }\n            },\n            {\n                &quot;$project&quot;: {\n                    &quot;_id&quot;: 0,\n                    &quot;text&quot;: 1,\n                    &quot;score&quot;: {&quot;$meta&quot;: &quot;searchScore&quot;}\n                }\n            }\n        ]\n        \n        return list(db.documents.aggregate(pipeline))\n</code></pre>\n<p><em>Update: Looking into <a href=\"https://weaviate.io/\">Weaviate</a> as an all-in-one DB\nsolution.</em></p>\n<p>This stack provides everything needed for modern Data Science and AI work while\nmaintaining clarity and minimising tool overlap.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Returning to Python with this minimal, modern toolchain has proven to be a\npragmatic choice. The combination of uv, Ruff, and Pytest creates a more unified\ndevelopment workflow, while retaining access to Python&#39;s mature scientific\ncomputing ecosystem.</p>\n<p>Key benefits of this approach:</p>\n<ol>\n<li><strong>Reduced Cognitive Load</strong>: One tool per task eliminates decision fatigue</li>\n<li><strong>Modern Performance</strong>: Rust-based tools (uv, Ruff) provide near-instant\nfeedback</li>\n<li><strong>Simplified Configuration</strong>: Single <code>pyproject.toml</code> as source of truth</li>\n<li><strong>Production Ready</strong>: Direct path from development to cross-platform\ndeployment</li>\n<li><strong>Full Feature Set</strong>: Complete Data Science and AI capabilities without bloat</li>\n<li><strong>Flexible AI Stack</strong>: Seamless integration between local models (Ollama),\nRAG pipelines (LlamaIndex), and vector storage (MongoDB)</li>\n<li><strong>Production AI</strong>: Easy transition from experimentation to production AI\nsystems with consistent tooling</li>\n</ol>\n<p>While Python&#39;s ecosystem will likely remain fragmented, we don&#39;t have to accept\nthe complexity. By carefully choosing modern tools that prioritise speed,\nsimplicity, and clarity, we can create a development environment that&#39;s both\npowerful and pleasant to use.</p>\n<p>The beauty of this approach lies not in its prescriptiveness, but in its\nprinciples: <em>minimize tooling</em>, <em>maximise capability</em>, and <em>maintain clarity</em>.\nWhether you adopt this exact stack or use it as inspiration for your own, the\ngoal remains the same: bring the focus back to solving problems rather than\nmanaging tools.</p>\n"
  },
  {
    "title": "💡 TIL: TF-IDF vs BM25",
    "date": "2024-11-20T00:00:00.000Z",
    "tags": [
      "til",
      "tf-idf",
      "bm25",
      "text-ranking",
      "nlp"
    ],
    "url": "/posts/TIL-BM25-TFIDF.html",
    "content": "<p><strong>TL;DR:</strong> While TF-IDF ranks documents based on term frequency weighted by\nrarity across a corpus, BM25 improves upon this foundation by adding term\nfrequency saturation and document length normalisation. Choose TF-IDF for\nsimpler tasks with uniformly-sized documents, but prefer BM25 for search engines\nhandling varied document lengths where its sophisticated algorithm delivers\nsuperior retrieval performance despite requiring more complex implementation and\nparameter tuning.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>When building search engines or document retrieval systems, two algorithms often\ncome up: <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">TF-IDF</a> and\n<a href=\"https://en.wikipedia.org/wiki/Okapi_BM25\">Okapi BM25</a>. While both aim to rank\ndocuments by relevance, they differ significantly in their approach and\neffectiveness. Today, I learned the key differences between these techniques and\nwhen to use each one.</p>\n<h2 id=\"tf-idf-the-classic-approach\">TF-IDF: The Classic Approach</h2>\n<p>TF-IDF (Term Frequency-Inverse Document Frequency) ranks documents based on how\nfrequently terms appear in a document, weighted by how rare those terms are\nacross all documents. It&#39;s straightforward: if a word appears often in a\ndocument but is rare across the corpus, it&#39;s probably important[^1]. $idf$ is\ncalculated as follows:</p>\n<p>$$idf(t) = \\log\\frac{N}{n_t}$$</p>\n<p>where:<br>$N$ : Total number of documents in corpus<br>$n_t$ : Number of documents containing term $t$</p>\n<p>TF-IDF is derived by the following calculation:</p>\n<p>$$TF\\text{-}IDF(t,d) = tf(t,d) \\cdot idf(t)$$</p>\n<p>where:<br>$tf(t,d)$ : Frequency of term $t$ in document $d$</p>\n<h3 id=\"advantages\">Advantages</h3>\n<ul>\n<li>Simple to understand and implement</li>\n<li>Computationally efficient</li>\n<li>Works well for documents of similar length</li>\n<li>Great for basic document classification</li>\n</ul>\n<h3 id=\"disadvantages\">Disadvantages</h3>\n<ul>\n<li>No term frequency saturation (more occurrences always mean higher scores)</li>\n<li>Doesn&#39;t handle varying document lengths well</li>\n<li>Can overemphasise common terms in long documents</li>\n</ul>\n<h2 id=\"bm25-the-modern-evolution\">BM25: The Modern Evolution</h2>\n<p>BM25 (Best Match 25) builds upon TF-IDF&#39;s foundation but adds two crucial\nimprovements: term frequency saturation and document length normalisation. Note\nhow the $idf_{BM25}$ component differs from TF-IDF&#39;s:</p>\n<p>$$idf_{BM25}(t) = \\log\\frac{N - n_t + 0.5}{n_t + 0.5}$$</p>\n<p>This modification provides smoother IDF weights and better handles edge cases.</p>\n<p>$$BM25(t,d) = \\frac{tf(t,d) \\cdot (k_1 + 1)}{tf(t,d) + k_1 \\cdot (1 - b + b \\cdot \\frac{|d|}{avgdl})} \\cdot idf_{BM25}$$</p>\n<p>where:<br>$tf(t,d)$ : Frequency of term $t$ in document $d$<br>$|d|$ : Length of document $d$ (in words)<br>$avgdl$ : Average document length in corpus<br>$k_1$ : Term frequency saturation parameter (typically 1.2-2.0)<br>$b$ : Length normalisation parameter (typically 0.75)<br>$N$ : Total number of documents in corpus<br>$n_t$ : Number of documents containing term $t$</p>\n<h3 id=\"advantages-1\">Advantages</h3>\n<ul>\n<li>Better handles varying document lengths</li>\n<li>Prevents term frequency from dominating scores</li>\n<li>More nuanced relevance rankings</li>\n<li>Industry standard for search engines</li>\n</ul>\n<h3 id=\"disadvantages-1\">Disadvantages</h3>\n<ul>\n<li>More complex implementation</li>\n<li>Requires parameter tuning</li>\n<li>Slightly higher computational cost</li>\n<li>Less interpretable than TF-IDF</li>\n</ul>\n<h2 id=\"which-to-choose\">Which to Choose?</h2>\n<h3 id=\"choose-tf-idf-when\">Choose TF-IDF when:</h3>\n<ul>\n<li>Building basic document classification systems</li>\n<li>Working with uniformly-sized documents</li>\n<li>Needing interpretable results</li>\n<li>Prioritising implementation simplicity</li>\n</ul>\n<h3 id=\"choose-bm25-when\">Choose BM25 when:</h3>\n<ul>\n<li>Building a search engine</li>\n<li>Handling documents of varying lengths</li>\n<li>Requiring state-of-the-art retrieval performance</li>\n<li>Working with user queries</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>While TF-IDF remains valuable for simpler tasks and educational purposes, BM25\nis generally superior for serious search applications. The choice between them\noften comes down to the trade-off between simplicity and sophistication. For\nmodern search engines, BM25 is the clear winner, but TF-IDF&#39;s simplicity makes\nit perfect for learning and basic applications.</p>\n<p>Remember: the best algorithm is the one that meets your specific needs. Don&#39;t\nautomatically reach for BM25 just because it&#39;s more advanced – sometimes,\nsimpler is better.</p>\n<p>[^1]: This is why TF-IDF is effective at identifying characteristic terms in\n    documents. It automatically downweights common words like &quot;the&quot;, &quot;and&quot;, &quot;is&quot;\n    while highlighting distinctive terms that appear frequently in specific\n    documents.</p>\n"
  },
  {
    "title": "🆙 Level Up With Dialogue Engineering",
    "date": "2024-11-15T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "dialogue-engineering",
      "prompt",
      "iterative-refinement",
      "rag"
    ],
    "url": "/posts/dialogue-engineering.html",
    "content": "<p><strong>TL;DR:</strong> Dialogue Engineering transforms AI interactions by replacing one-shot\nprompts with structured, multi-turn conversations that break complex tasks into\nmanageable steps: setting scenarios, gathering information, creating structured\noutlines, generating content iteratively, and refining conclusions. This\nsystematic approach dramatically improves productivity across research,\nbusiness, and content creation while maintaining human oversight to address AI\nlimitations like accuracy and contextual understanding.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Dialogue Engineering is transforming how we interact with AI[^1]. Rather than\nrelying on one-shot prompts, it&#39;s an iterative approach where we engage in\nstructured, multi-turn conversations with LLMs (Large Language Models) to\nachieve complex goals. While I first encountered the term through Jeremy\nHoward[^2][^3], the concept has deeper roots in human-AI interaction research.\nThough Howard popularised it recently through fast.ai and answer.ai, the concept\nhas been discussed since 1986[^4]. Dialogue engineering dramatically improves\nproductivity by breaking down complex tasks, maintaining context across\ninteractions, and guiding AI through iterative refinement. This systematic\napproach helps produce better results while reducing the cognitive load of\nprompt crafting. A nice overview of Dialogue Engineering comes from the Medium\narticle\n<a href=\"https://medium.com/@fabioc/dialog-engineering-ai-as-your-research-assistant-616a625e9853\">Dialog Engineering: AI as Your Research Assistant</a>.<br>Below, I&#39;ll summarise what I inferred from that article.</p>\n<h2 id=\"how-dialogue-engineering-works\">How Dialogue Engineering Works</h2>\n<ol>\n<li><strong>Setting the Scenario</strong><br>This first step involves defining clear objectives and research questions\nbefore engaging with AI. Rather than diving into broad topics, we frame\nspecific goals and provide relevant context. For example, when starting a\nresearch project, we outline exactly what we need to investigate and any\nimportant background information the AI should consider.<br><em>Best Practice:</em> Be clear and specific about goals, provide relevant\nbackground information to help AI understand context.</li>\n<li><strong>Gathering Information</strong><br>Once the scenario is set, we guide the AI in collecting and organising\nrelevant data. This could involve creating annotated bibliographies,\nsummarising key sources, or compiling research findings. The AI helps\nstructure this information in a way that&#39;s useful for the next steps.<br><em>Best Practice:</em> Request structured formats like annotated bibliographies,\nask for citations and evidence to ensure accuracy.</li>\n<li><strong>Structuring the Outline</strong><br>Before diving into content creation, we work with the AI to develop a clear\nroadmap. This outline breaks down the task into logical sections, ensuring a\ncoherent flow and manageable chunks of work.<br><em>Best Practice:</em> Break the task into clear sections, ensure logical\nconnections between parts that reflect overall goals.</li>\n<li><strong>Generating Content Iteratively</strong><br>With the outline in place, we tackle each section individually through\niterative refinement. Rather than expecting perfect content immediately, we\nprovide feedback and guide the AI to improve its outputs progressively.<br><em>Best Practice:</em> Work on sections individually to maintain focus, use\nfeedback loops to guide AI toward more specific, accurate outputs.</li>\n<li><strong>Conclusion and Introduction Refinement</strong><br>The final step involves revisiting the opening and closing sections once the\nmain content is complete. This ensures these crucial parts accurately reflect\nand synthesise the entire piece.<br><em>Best Practice:</em> Write introduction last to accurately reflect content, craft\nconclusion by synthesising main takeaways from each section.</li>\n</ol>\n<p>Throughout all steps, I maintain active oversight, checking for accuracy and\nproviding clear feedback. This systematic approach has dramatically improved my\nproductivity while ensuring high-quality outputs.</p>\n<h2 id=\"practical-applications\">Practical Applications</h2>\n<p>Here are the key areas where dialogue engineering proves particularly valuable:</p>\n<ul>\n<li><strong>Academic Research</strong><br>Researchers can leverage dialogue engineering to synthesise vast amounts of\ninformation, structure complex arguments, and ensure accurate citations. The\niterative approach is particularly useful for literature reviews and thesis\ndevelopment.<br><em>Example:</em> A researcher prompts AI to generate an annotated bibliography on\nAI-driven diagnostics, focusing on recent studies, then iteratively refines\nthe summaries and findings.</li>\n<li><strong>Business Strategy and Reporting</strong><br>For corporate applications, dialogue engineering helps generate market\nreports, analyse trends, and produce comprehensive strategy documents. This\nsystematic approach ensures consistency while maintaining analytical depth.<br><em>Example:</em> Business analysts use iterative prompts to draft sections of market\nreports, starting with &quot;<em>Generate a section on e-commerce trends focusing on\nAI-driven personalisation</em>&quot; then refining based on specific data points.</li>\n<li><strong>Report Automation</strong><br>Dialogue Engineering excels at automating recurring business reports, such as\nquarterly financial reviews or performance summaries. The structured approach\nensures consistency while allowing for customisation.<br><em>Example:</em> Teams automate quarterly reports by structuring templates with AI,\nfeeding relevant data, and using iterative refinement to maintain accuracy and\nfreshness.</li>\n<li><strong>Content Creation and Media</strong><br>Content creators can streamline the production of articles, blogs, and\nmultimedia scripts through structured dialogue with AI. This approach\nparticularly shines in drafting and revising content iteratively.<br><em>Example:</em> Writers use dialogue engineering to draft introductory paragraphs,\nthen iterate with prompts for more engaging language or additional examples.</li>\n<li><strong>Technical Writing and Documentation</strong><br>In fields requiring precise technical documentation, dialogue engineering\nhelps ensure clarity, accuracy, and consistency across complex documents.<br><em>Example:</em> Software engineers use dialogue engineering to draft technical\ndocumentation for new features, prompting &quot;<em>Draft a technical overview of the\nnew user authentication feature</em>&quot; then refining for clarity and technical\naccuracy.</li>\n</ul>\n<p>Each of these applications benefits from dialogue engineering&#39;s structured,\niterative approach, leading to more efficient workflows and higher-quality\noutputs.</p>\n<h2 id=\"best-practices\">Best Practices</h2>\n<p>Key best practices include:</p>\n<ul>\n<li><strong>Precision in Prompts</strong><br>Craft prompts that are neither too vague nor overly specific. Focus on clear,\nwell-structured queries that guide AI towards relevant outputs. <em>Example:</em>\nInstead of &quot;<em>Tell me about AI in healthcare</em>&quot; use &quot;<em>What are the latest\nadvancements in AI-driven diagnostics in healthcare, particularly in image\nrecognition?</em>&quot;</li>\n<li><strong>Iterative Refinement</strong><br>Build on each interaction, using feedback to improve outputs gradually rather\nthan expecting perfection immediately. <em>Example:</em> Start with a draft section,\nthen refine with follow-up prompts like &quot;<em>Expand on the use of dialogue\nengineering in business reporting, specifically market trend analysis.</em>&quot;</li>\n<li><strong>Leverage Feedback Loops</strong><br>Maintain continuous cycles of prompting, feedback, and refinement to improve\noutput quality over time. <em>Example:</em> When creating an outline, start broad,\nthen use feedback to add specific sections on practical examples in different\ndomains.</li>\n<li><strong>Source and Citation Checking</strong><br>Verify AI-generated sources and citations manually, as AI models lack\nreal-time access to databases. <em>Example:</em> Cross-reference any cited statistics\nor research papers with trusted external sources before including them in\nfinal outputs.</li>\n<li><strong>Structure Before Diving In</strong><br>Create clear outlines or plans before generating detailed content to ensure\nlogical flow and completeness. <em>Example:</em> Start with a structured outline for\na Medium article, then develop each section iteratively.</li>\n<li><strong>Mind Token Limits</strong><br>Break down long content into manageable chunks to work within AI model token\nlimits.<br><em>Example:</em> Generate long-form content section by section, refining each piece\nbefore moving to the next.</li>\n</ul>\n<p>However, we should be aware of the limitations (and challenges) of Dialogue\nEngineering too.</p>\n<h2 id=\"understanding-the-limitations\">Understanding the Limitations</h2>\n<p>While these best practices enhance the use of dialogue engineering, it&#39;s\nessential to acknowledge its constraints and challenges. Like any powerful tool,\ndialogue engineering comes with limitations that require careful consideration\nand management. Here&#39;s what we need to keep in mind:</p>\n<h3 id=\"key-limitations-and-challenges\">Key Limitations and Challenges</h3>\n<p>The foremost concern when using generative AI is <em>accuracy</em> and\n<em>hallucinations</em>. LLMs can sometimes generate plausible-sounding but false\ninformation, necessitating rigorous fact-checking processes. This is\nparticularly critical in professional contexts where accuracy is paramount.\n<em>Ethical implications</em> also demand attention. While AI can streamline work\nprocesses, maintaining authenticity and proper attribution is crucial. This\nconnects directly to the need for <em>consistent human oversight</em>, that is users\nmust <em>actively review outputs</em>, <em>ensure quality control</em>, and <em>make ethical\njudgements</em> about the content&#39;s appropriateness and accuracy. AI&#39;s current\n<em>limitations in understanding context and nuance</em> present another challenge.\nModels may struggle with subtle distinctions or produce oversimplified\nexplanations, especially in specialised fields. Technical constraints,\nparticularly token limits and handling complex, multi-layered reasoning tasks,\nfurther necessitate careful planning and task breakdown. These limitations\nunderscore a crucial point: dialogue engineering works best as a <em>collaborative\ntool</em> that <em>enhances</em>, rather than replaces, human expertise and judgement.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Dialogue Engineering represents a significant evolution in human-AI interaction,\nmoving beyond simple prompt engineering to create a dynamic, iterative approach.\nThrough structured conversations and systematic refinement, it enables us to\ntackle complex tasks more efficiently across academic, business, and creative\ndomains. While the technique requires careful attention to limitations like AI\nhallucinations and demands consistent human oversight, its power lies in\ntreating AI as a collaborative partner rather than a one-shot tool. By following\nbest practices and understanding its constraints, dialogue engineering becomes a\nforce multiplier for productivity, helping us create better outputs while\nmaintaining human expertise at the core of the process. This balance of\nsystematic interaction and human judgement makes dialogue engineering a valuable\nframework for anyone looking to maximise the potential of AI tools in their\nworkflow.</p>\n<hr>\n<p>[^1]: AI is an umbrella term that has meant different things over the years.\n    Since 2022, it has become a synonym of Generative AI. Here&#39;s a short AI\n    timeline:\n    <a href=\"https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence\">Symbolic AI</a>\n    (1950-60s), <a href=\"https://en.wikipedia.org/wiki/Expert_system\">Expert Systems</a>\n    (1970s), <a href=\"https://en.wikipedia.org/wiki/Neural_network\">Neural Networks</a> and\n    <a href=\"https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning\">Knowledge Representation</a>\n    (1980s), <a href=\"https://en.wikipedia.org/wiki/Machine_learning\">Machine Learning</a>\n    and <a href=\"https://en.wikipedia.org/wiki/Statistics\">Statistical Methods</a> (1990s),\n    <a href=\"https://en.wikipedia.org/wiki/Big_data\">Big Data</a> and Deep Learning\n    foundations (2000s),\n    <a href=\"https://en.wikipedia.org/wiki/Deep_learning\">Deep Learning</a> (2010s),\n    <a href=\"https://en.wikipedia.org/wiki/Generative_artificial_intelligence\">Generative AI</a>\n    and\n    <a href=\"https://en.wikipedia.org/wiki/Large_language_model\">Large Language Models</a>\n    (2020s)</p>\n<p>[^2]: <a href=\"https://www.youtube.com/watch?v=qO-YqJm0Q1U&t=16\">Answer.ai &amp; AI Magic with Jeremy Howard</a></p>\n<p>[^3]: <a href=\"https://www.answer.ai/posts/2024-11-07-solveit.html\">How To Solve It With Code</a></p>\n<p>[^4]: <em>Foundations of dialog engineering: the development of human-computer\n    interaction. Part II</em>\n    <a href=\"https://www.sciencedirect.com/science/article/pii/S0020737386800438\">(Gaines et al., 1986)</a></p>\n"
  },
  {
    "title": "🖥 The On-Prem Comeback (aka Cloud Repatriation)",
    "date": "2024-11-14T00:00:00.000Z",
    "tags": [
      "cloud",
      "on-prem"
    ],
    "url": "/posts/cloud-repatriation.html",
    "content": "<p><strong>TL;DR:</strong> Cloud repatriation-the strategic migration of applications and data\nfrom public clouds back to on-premises infrastructure-is gaining traction as\norganisations seek better cost control, performance, data privacy, and reduced\nvendor lock-in, with companies like 37signals projecting significant savings\nwhilst maintaining a balanced hybrid approach rather than abandoning cloud\nentirely.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>More recently, a notable shift is emerging in how organisations approach their\ncloud infrastructure. Some companies are beginning to move their applications\nand data away from public cloud providers like AWS, GCP and Azure. This marks a\nshift in the computing landscape.</p>\n<h2 id=\"what-does-cloud-repatriation-mean\">What Does Cloud Repatriation Mean?</h2>\n<p>Cloud repatriation refers to the process of moving applications, services and\ndata from public cloud environments back to on-premises data centres, private\nclouds or hybrid set-ups. This reverse migration represents a pivot from the\n&quot;cloud-first&quot; mindset that has dominated in the last few years.</p>\n<h2 id=\"why-its-happening\">Why It&#39;s Happening</h2>\n<p>Several key factors are driving this trend. For larger companies, cost is a real\nconsideration, with scale-ups such as 37signals projecting savings of £2 million\np.a. by leaving AWS. Performance issues and rising cloud costs have led major\norganisations like GEICO to reconsider their cloud strategy after experiencing\n2.5x increases in their bills alongside reliability challenges. <em>Data privacy</em>,\n<em>compliance</em> requirements and the desire to avoid <em>vendor lock-in</em> are also\nsignificant motivators aside from growing costs. Many organisations are finding\nthat running certain workloads on-premises or in hybrid environments offers\nbetter control over their infrastructure and data. See\n<a href=\"https://www.youtube.com/watch?v=kyJJeik9loU\">optimising infrastructure for AI</a>\nfor a nice overview on cloud&lt;-&gt;on-prem.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Cloud repatriation isn&#39;t about completely abandoning public clouds but rather\nabout finding the right balance. For organisations with predictable workloads\nand sufficient technical expertise, a strategic combination of on-premises,\nprivate cloud and public cloud infrastructure might prove more effective than a\npublic-cloud-only approach.</p>\n"
  },
  {
    "title": "🐢 Slow Down And Grow Smart, Not Fast",
    "date": "2024-11-14T00:00:00.000Z",
    "tags": [
      "slow-down",
      "advantage",
      "best-practices",
      "company-culture",
      "productivity",
      "decision-making",
      "business-value",
      "real-value"
    ],
    "url": "/posts/slow-down.html",
    "content": "<p><strong>TL;DR:</strong> Sustainable business growth prioritises measured expansion over\nexplosive scaling, focusing on leadership development, strategic capital\ndeployment, organisational health through clear alignment, and strong\ncommunication-creating resilient companies where profitability, employee\nretention, and team autonomy become key metrics of success rather than breakneck\nspeed.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>In an industry obsessed with &quot;move fast and break things,&quot; some companies prove\nthat measured growth creates lasting success. I was happy to listen to the\n<a href=\"https://saasscalingsecrets.buzzsprout.com/2172375/episodes/15926541-why-slower-growth-could-be-your-fast-track-to-success-with-roan-lavery-ceo-of-freeagent\">CEO of a successful company</a>\nrecounting their success, which was largely thanks to slow and steady growth.</p>\n<h2 id=\"sustainable-development-over-explosive-growth\">Sustainable Development Over Explosive Growth</h2>\n<p>It&#39;s safe to say that consistency and growing at a controlled pace is a recipe\nfor a successful sustainable [insert word here]. This strategy applies to most\nthings in life, in my view. Here is what caught my attention from this\ndiscussion:</p>\n<ul>\n<li>The company grew at a pace that allowed leaders to develop alongside the\nbusiness</li>\n<li>They focused on reaching profitability within 18-24 months after each funding\nround</li>\n<li>They raised capital from a position of strength, not necessity</li>\n<li>Capital was then deployed strategically rather than burning through runway</li>\n</ul>\n<h2 id=\"building-strong-foundations\">Building Strong Foundations</h2>\n<p>Organisational health was centred around <em>clear alignment</em> through frameworks\nlike <a href=\"https://www.tablegroup.com/product/the-advantage/\">The Advantage</a>[^1].\nCompany values have been <em>embedded into daily processes</em>. Goals are <em>integrated\ninto regular team workflows</em>. Finally, maintaining <em>strong communication across\nall levels</em> has been key to the company&#39;s success.</p>\n<h2 id=\"culture-and-retention\">Culture And Retention</h2>\n<p>Success metrics go beyond financial growth. Key employees have stayed with the\ncompany <em>long-term</em>. Teams have maintained <em>autonomy</em> while staying <em>accountable\nthrough balanced scorecards</em>. Leadership has focused on creating <em>clarity</em> and\n<em>empowering teams</em>. Regular reinforcement of values has been achieved through\n<em><a href=\"https://youtube.com/watch?v=Og7NzaVpceE\">good onboarding</a></em>[^2], and daily\noperations.</p>\n<h2 id=\"key-takeaways\">Key Takeaways</h2>\n<ul>\n<li><strong>Match growth to capability</strong>: Ensure your organisation can sustainably\nsupport its growth rate</li>\n<li><strong>Focus on fundamentals</strong>: Build strong processes and systems that scale\ngradually</li>\n<li><strong>Invest in people</strong>: Give teams time and resources to develop alongside the\ncompany</li>\n<li><strong>Deploy capital wisely</strong>: Prioritise sustainable growth over rapid cash burn</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p><strong>This is the TL;DR really:</strong> Building a successful tech company doesn&#39;t require\nbreakneck speed or unsustainable growth. Smart, measured expansion with a focus\non people and processes creates resilient businesses that stand the test of\ntime.</p>\n<hr>\n<p>[^1]: &quot;<em>[the author] makes an overwhelming case that organisational health will\n    surpass all other disciplines in business as the greatest opportunity for\n    improvement and competitive advantage.</em>&quot;</p>\n<p>[^2]: Characteristics of a good onboarding program: a) One thing at a time, b)\n    Lots of practice (with feedback), c) Attain proficiency, then move forward,\n    d) The learner follows rather than guides, e) &quot;Minimum Productive\n    Competency&quot;</p>\n"
  },
  {
    "title": "💪 The Advantage",
    "date": "2024-11-14T00:00:00.000Z",
    "tags": [
      "slow-down",
      "advantage"
    ],
    "url": "/posts/the-advantage.html",
    "content": "<p><strong>TL;DR:</strong> Patrick Lencioni&#39;s &quot;The Advantage&quot; argues that organisational\nhealth-built through leadership teams founded on trust, clear communication, and\naligned values-is the single greatest competitive advantage companies can\nachieve, outweighing traditional &quot;smart&quot; business strategies and metrics for\nsustainable long-term success.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Recently I listened to a\n<a href=\"https://saasscalingsecrets.buzzsprout.com/2172375/episodes/15926541-why-slower-growth-could-be-your-fast-track-to-success-with-roan-lavery-ceo-of-freeagent\">successful company&#39;s CEO interview</a>,\nwhere he explained how growing sustainably contributed to the company&#39;s success.\nThe CEO said that their growth strategy was inspired by\n<a href=\"https://www.tablegroup.com/pat/\">Patrick Lencioni</a>&#39;s book\n<a href=\"https://www.tablegroup.com/product/the-advantage/\">The Advantage</a>. I found the\nbook&#39;s premise very interesting, hence I&#39;ll attempt to summarise important\npoints as a note to self.</p>\n<h2 id=\"central-thesis\">Central Thesis</h2>\n<p>Organisational health is the <em>single greatest competitive advantage</em> a company\ncan achieve, yet it&#39;s often overlooked in favour of &quot;smart&quot; business decisions.</p>\n<h2 id=\"key-components-of-organisational-health\">Key components of organisational health</h2>\n<h3 id=\"leadership-team-structure\">Leadership team structure</h3>\n<ul>\n<li>Optimal size: 3-10 people</li>\n<li>Built on trust and vulnerability</li>\n<li>Values collective success over individual achievement</li>\n</ul>\n<h3 id=\"core-principles\">Core principles</h3>\n<ul>\n<li>Trust and vulnerability as foundations</li>\n<li>Accountability at all levels</li>\n<li>Commitment to collective goals</li>\n<li>Clear communication and expectations</li>\n</ul>\n<h3 id=\"operational-excellence\">Operational excellence</h3>\n<ul>\n<li>Regular, focused meetings with specific purposes</li>\n<li>Clear distinction between strategic and tactical discussions</li>\n<li>Emphasis on debate and healthy conflict resolution</li>\n<li>Continuous progress monitoring</li>\n</ul>\n<h3 id=\"people-and-culture\">People and culture</h3>\n<ul>\n<li>Hire for cultural fit rather than training after hiring</li>\n<li>Focus on values alignment in recruitment</li>\n<li>Reward behaviour that aligns with organisational values</li>\n<li>Foster an environment of open communication</li>\n</ul>\n<h3 id=\"critical-success-factors\">Critical success factors</h3>\n<ul>\n<li>Minimal internal politics</li>\n<li>High clarity in communication</li>\n<li>Clear decision-making processes</li>\n<li>Low employee turnover</li>\n<li>High morale and productivity</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The book&#39;s fundamental message is that creating a healthy organisation through\nstrong leadership, clear communication, and aligned values is more important\nthan traditional business metrics for long-term success. It&#39;s not about being\nthe &quot;smartest&quot; in the market, but about creating the healthiest internal\nenvironment.</p>\n"
  },
  {
    "title": "✅ On-boarding that works",
    "date": "2024-11-14T00:00:00.000Z",
    "tags": [
      "onboarding",
      "best-practices",
      "productivity",
      "company-culture",
      "decision-making",
      "talent",
      "learning",
      "efficiency"
    ],
    "url": "/posts/good-onboarding.html",
    "content": "<p><strong>TL;DR:</strong> Effective onboarding programmes dramatically reduce the productivity\ngap for new hires by applying evidence-based learning principles: breaking\nskills into manageable components, providing structured learning paths,\ntargeting 80% proficiency before moving on, defining minimum productive\ncompetency, using checklists with frequent feedback, and focusing on mechanical\ncompetency - all based on how human memory and learning actually work,\ntransforming the costly standard approach where developers take months to become\nfully productive.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>I recently watched a lively presentation titled\n<a href=\"https://www.youtube.com/watch?v=Og7NzaVpceE\">opinionated onboarding</a>, that\ndiscussed the dramatic impact of onboarding practices on new staff and business\nsuccess. The speaker, drawing from his experience, articulated that poor\nonboarding is actively harming companies while good onboarding can transform\nteam productivity and retention. Having experienced both ends of the spectrum\nmyself, I strongly agree with the presenter&#39;s central thesis. Companies can&#39;t\nafford to waste months getting new staff up to speed, yet that&#39;s what&#39;s\nhappening very frequently. The costs are staggering, both in lost productivity\nand squandered talent.</p>\n<h2 id=\"the-problem\">The Problem</h2>\n<p>New employees face an overwhelming cognitive burden as they simultaneously\nnavigate multiple learning curves: mastering the tech stack, deciphering an\nunfamiliar codebase, adapting to team workflows, understanding the business\ndomain, and learning organisational structures. Many companies worsen this\nsituation through ineffective approaches, either expecting staff to self-direct\ntheir learning with vague instructions like &quot;go learn X and tell us when you&#39;re\ndone&quot;, or by immediately assigning them tickets without proper context or\nsupport. This inefficiency comes at a significant cost though. According to the\npresenter, the average software developer staying at a company for only 20\nmonths, taking 6 months to become productive means losing nearly a third of\ntheir effective tenure[^1]. Rather than fixing their onboarding processes, many\ncompanies respond by exclusively hiring senior professionals who can &quot;hit the\nground running&quot; - an approach that not only limits their talent pool but proves\nunrealistic even for experienced hires.</p>\n<h2 id=\"effective-on-boarding-strategies\">Effective On-boarding Strategies</h2>\n<p>According to the presenter, research and experience show that effective\nonboarding follows clear cognitive science principles. Rather than overwhelming\nnew hires with broad, unfocused (occasionally learning) objectives, successful\nonboarding programs recognise how human learning actually works and structure\ntheir approach accordingly. Two key principles emerge as foundational to any\neffective onboarding strategy:</p>\n<ol>\n<li>Focus on building specific capabilities rather than general &quot;understanding&quot;</li>\n<li>Account for cognitive limitations:<ul>\n<li>People can only hold ~4 concepts in working memory</li>\n<li>New concepts take more mental space than familiar ones</li>\n<li>Skills must be practiced close to when they&#39;re learned</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"best-practices\">Best Practices</h3>\n<p>These principles translate into concrete best practices that any organisation\ncan implement. While the specific skills and technologies may vary between\nteams, successful onboarding programs share common structural elements that\nmaximize learning efficiency while minimizing cognitive overload:</p>\n<ul>\n<li><strong>Break down complex skills into smaller, manageable components</strong>. For\nexample, rather than asking someone to &quot;learn LiveView&quot;[^2] break it down into\nspecific tasks like creating forms, handling events, or managing state</li>\n<li><strong>Provide structured learning paths rather than self-directed exploration</strong>.\nWhen new hires must decide what to learn next, they waste valuable mental\ncapacity on planning rather than learning. A clear, predefined path eliminates\nthis overhead</li>\n<li><strong>Aim for 80% proficiency before moving to next skill</strong>. This threshold\nensures sufficient mastery while avoiding diminishing returns from pursuing\nperfection</li>\n<li><strong>Define minimum productive competency for the role</strong>. Not every skill needs\nto be mastered immediately - identify what&#39;s truly needed for day-one\nproductivity and focus there first</li>\n<li><strong>Use checklists and frequent practice with feedback</strong>. Clear checkpoints\nprovide motivation and progress tracking, while regular feedback prevents\nlearners from developing incorrect habits</li>\n<li><strong>Focus on mechanical competency to reduce cognitive load</strong>. When basic\noperations become automatic, developers can focus their mental energy on\nsolving more complex problems</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Effective onboarding is not just a nice-to-have, it&#39;s a competitive necessity in\ntoday&#39;s software industry. While poor onboarding practices continue to cost\ncompanies valuable time and talent, the path to improvement is clear. By\n<em>breaking down</em> complex skills, providing <em>structured learning</em> paths, and\n<em>respecting</em> cognitive limitations, organisations can dramatically reduce the\ntime it takes for new hires to become productive team members. This investment\nin structured onboarding not only accelerates developer productivity but also\nexpands hiring possibilities, allowing companies to tap into a broader talent\npool. The choice is simple: continue losing months of productivity to\nineffective onboarding, or implement these evidence-based practices to build\nstronger, more capable engineering teams.</p>\n<hr>\n<p>[^1]: Since the presentation focused on software developers, I use it here as a\n    proxy for various technical positions including AI Engineering, Data Science\n    and others. Also, the tenure statistic may be skewed towards the U.S.\n    market, however it&#39;s true that many employees job hop in pursuit of a higher\n    salary or a better job altogether</p>\n<p>[^2]: <a href=\"https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html\">Phoenix LiveView</a>\n    &quot;<em>is a process that received events, updates its state and renders updates\n    to a page as diffs</em>&quot;</p>\n"
  },
  {
    "title": "💡 TIL: vLLM Is A High-Performance Engine For LLM Serving",
    "date": "2024-11-13T00:00:00.000Z",
    "tags": [
      "til",
      "llm",
      "ai",
      "python",
      "on-prem",
      "performance"
    ],
    "url": "/posts/TIL-vLLM.html",
    "content": "<p><strong>TL;DR:</strong> vLLM revolutionises LLM deployment through its PagedAttention\nalgorithm, which applies virtual memory principles to key-value caches, enabling\nmore efficient memory management and significantly improving throughput for\nresource-constrained environments whilst supporting popular open-source models.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>As a Data Scientist / AI Engineer exploring local-first solutions[^1][^2],\ndeploying Large Language Models (LLMs) presents significant resource management\nchallenges. vLLM emerges as a breakthrough solution that fundamentally\nreimagines how we deploy and utilise these resource-intensive models[^4].</p>\n<h2 id=\"what-is-vllm\">What Is vLLM?</h2>\n<p>vLLM is an open-source serving engine that optimises LLM deployment through\nvirtualisation techniques[^4]. At its core, vLLM introduces PagedAttention, a\nnovel attention algorithm that improves memory utilisation through paged memory\nmanagement[^3]. Similar to how operating systems manage virtual memory,\nPagedAttention segments the key-value memory into non-continuous pages, enabling\nmore efficient memory usage and request handling.</p>\n<p>Key features[^4]:</p>\n<ul>\n<li>Efficient memory management through PagedAttention</li>\n<li>Continuous batching for request handling</li>\n<li>Support for popular open-source models (Llama, Mistral, Falcon)</li>\n</ul>\n<h2 id=\"implementation\">Implementation</h2>\n<p>Here&#39;s a simple example of using vLLM:</p>\n<pre><code class=\"language-python\">from vllm import LLM, SamplingParams\n\n# Initialise the model\nllm = LLM(model=&quot;meta-llama/Llama-3.1-8B&quot;)\n\n# Define sampling parameters\nsampling_params = SamplingParams(\n    temperature=0.7,\n    max_tokens=128\n)\n\n# Generate text\noutputs = llm.generate([&quot;Your prompt goes here&quot;], sampling_params)\n</code></pre>\n<h2 id=\"applications\">Applications</h2>\n<p>According to industry analysis[^4], vLLM&#39;s applications span multiple domains:</p>\n<ol>\n<li><p>Natural Language Processing</p>\n<ul>\n<li>Enhances chatbots and sentiment analysis</li>\n<li>Improves language translation services</li>\n</ul>\n</li>\n<li><p>Healthcare</p>\n<ul>\n<li>Enables secure patient data analysis</li>\n<li>Assists in medical diagnostics</li>\n</ul>\n</li>\n<li><p>Financial Services</p>\n<ul>\n<li>Powers fraud detection systems</li>\n<li>Enhances automated customer service</li>\n</ul>\n</li>\n<li><p>Education</p>\n<ul>\n<li>Facilitates intelligent tutoring systems</li>\n<li>Enables automated assessment tools</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"best-practices-for-implementation4\">Best Practices for Implementation[^4]</h2>\n<p>For optimal vLLM deployment:</p>\n<ul>\n<li>Implement model optimisation techniques</li>\n<li>Utilise containerisation for scalable deployment</li>\n<li>Maintain robust monitoring systems</li>\n<li>Regular performance optimisation</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>vLLM represents a significant advancement in LLM serving technology[^3],\noffering an efficient, scalable solution for resource-constrained environments.\nIts innovative approach to memory management through PagedAttention and broad\napplicability across industries makes it an essential tool for modern AI\ndevelopment.</p>\n<hr>\n<p>[^1]: <a href=\"https://www.puppet.com/blog/cloud-repatriation\">Cloud Repatriation: Examples, Unpacking 2024 Trends &amp; Tips for Reverse Migration</a></p>\n<p>[^2]: <a href=\"https://thenewstack.io/why-companies-are-ditching-the-cloud-the-rise-of-cloud-repatriation/\">Why Companies Are Ditching the Cloud: The Rise of Cloud Repatriation</a></p>\n<p>[^3]: Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.H., Gonzalez,\n    J.E., Zhang, H., and Stoica, I. (2023). &quot;Efficient Memory Management for\n    Large Language Model Serving with PagedAttention.&quot;\n    <a href=\"https://arxiv.org/abs/2309.06180\">arXiv:2309.06180</a>.</p>\n<p>[^4]: <a href=\"https://aijobs.net/insights/vllm-explained/\">vLLM Explained</a></p>\n"
  },
  {
    "title": "🔀 Cross-Platform Builds In Python",
    "date": "2024-11-11T00:00:00.000Z",
    "tags": [
      "python",
      "github-actions",
      "ci-cd",
      "cross-platform",
      "deno",
      "typescript"
    ],
    "url": "/posts/py-cross-compile.html",
    "content": "<p><strong>TL;DR:</strong> Creating cross-platform Python application packages requires CI/CD\nsolutions like GitHub Actions since tools like PyInstaller can&#39;t natively build\nfor multiple platforms; alternatives like Julia and Elixir offer promising but\nstill-maturing packaging options, while Deno emerges as an appealing alternative\nwith its straightforward cross-platform packaging capabilities, lightweight\nfootprint, and growing data ecosystem - though Python remains dominant for data\nanalysis despite its packaging limitations.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>In the last couple of years I&#39;ve spoken to 3-4 people who had needed a bespoke\ndata analysis tool that could be used locally, with privacy in mind. In some\ncases they&#39;d need to work in a sandboxed environment for security reasons, other\ntimes they had IP protection concerns. A desktop app or a CLI tool[^1] seemed to\nfit the bill in all those cases.<br>In the last decade+, Data and Python have become a synonym.\n<a href=\"https://pyinstaller.org/\">PyInstaller</a> seemed like an obvious choice. However,\nPyInstaller cannot cross-compile code as it stands. Being a <em>Linux</em> user,\noffering to help <em>Windows</em> users, meant I should find a workaround e.g.\nleveraging GitHub Actions for cross-compilation.</p>\n<h2 id=\"taking-the-scenic-route\">Taking The Scenic Route</h2>\n<p>Out of curiosity, I decided to have a poke around a couple of different\nlanguages and ecosystems that could teach me a few things while helping me\nunderstand what is a viable alternative to Python.</p>\n<h3 id=\"julia\">Julia</h3>\n<p>The <a href=\"https://julialang.org/\">Julia</a> programming language has caught my eye\nsince 2014. It partially reminded me of MATLAB, that I used for my PhD.\nFamiliarity aside, it&#39;s a great language to develop with. It&#39;s fast,\ninteractive, with the best REPL I&#39;ve encountered, highly promising overall\nespecially so after the release of Julia v1.9.<br>To my understanding Julia, being a JIT compiled language, wasn&#39;t designed for\nstatic compilation per se. Community efforts have enabled the generation of\ncompiled packages, with <a href=\"https://binarybuilder.org/\">BinaryBuilder</a>,\n<a href=\"https://julialang.github.io/PackageCompiler.jl\">PackageCompiler</a> and\n<a href=\"https://github.com/tshort/StaticCompiler.jl\">StaticCompiler</a> being the most\nwell known compilation tools available at the time of writing. From an\nintermediate Julia user&#39;s point of view, I&#39;ve found that compilation results may\nvary. Also, to the best of my knowledge most compilation tools actually\n<em>package</em> code rather than statically compile it, which may expose valuable IP.\nTherefore, I concluded that Julia <em>probably</em> isn&#39;t as easy to compile as I\ninitially thought (and hoped).</p>\n<h3 id=\"elixir\">Elixir</h3>\n<p><a href=\"https://elixir-lang.org/\">Elixir</a> is a fantastic hosted functional language,\nrunning on the tried and tested\n<a href=\"https://en.wikipedia.org/wiki/BEAM_(Erlang_virtual_machine)\">BEAM (Erlang VM)</a>.\nOne of the many things Elixir has going for it, is its strong drive towards\n<em>good</em> documentation. The language&#39;s REPL is also excellent. All in all, Elixir\nis rapidly evolving and it&#39;s worth experimenting with.<br>Starting from 2021, <a href=\"https://github.com/elixir-nx\">Numerical Elixir (Nx)</a> has\nprogressed by leaps and bounds. The Nx community has managed to produce\nexcellent libraries, with <a href=\"https://livebook.dev/\">Livebook</a> being the best\nliterate programming environment I&#39;ve ever used. As far as data applications are\nconcerned, Elixir will become a <em>very strong</em> contender, it&#39;s well worth keeping\na close eye on the language.<br>As for cross-compilation, to my understanding\n<a href=\"https://hex.pm/packages/burrito\">Burrito</a> is the only tool that allows for\npackaged Elixir code to be truly portable albeit producing sizeable executables.\nBurrito is still WIP, not a guaranteed solution for the time being but a\nnoteworthy tool that&#39;s improving fast.<br>Being doubtful as to whether this tech stack could meet all my current needs,\nbeside being a niche language in Data and AI, led me to search for another tech\nstack for fun and profit.</p>\n<h3 id=\"deno-typescript\">Deno (TypeScript)</h3>\n<p>More recently, especially given many AI Engineering APIs are written both in\nPython <em>and</em> TypeScript, I started using Deno. The idea is to leverage Deno for\nall my computational needs, since it&#39;s an all-in-one, straightforward to use\nruntime. Installation and setup were a breeze, Deno comes with <em>all</em> the tools a\ndeveloper requires (formatter, linter, testing suite, package manager etc.), it\nplays very nicely with Vim, it&#39;s lightweight, secure, compatible with NPM\npackages and the list goes on. Importantly, it can easily cross-compile\nexecutables. The data and AI ecosystem is not yet as mature as that of Python.\nHowever, if someone is willing to put in the effort, I&#39;ve found that it&#39;s well\nworth the investment. This is why I am betting on Deno for my Data Science and\nAI needs.</p>\n<h2 id=\"what-about-python-cross-compilation\">What About Python Cross-Compilation?</h2>\n<p><strong>How hard could it be?</strong> 🤔<br><strong>TL;DR:</strong> it&#39;s an involved process that requires access to a CI/CD platform\nsuch as GitHub Actions. Once a pipeline is in place, it&#39;s a straightforward\nprocess that requires internet access and registering to a hosting service such\nas GitHub.</p>\n<p>I wrote two pipelines, one for generating a\n<a href=\"https://github.com/ai-mindset/py-cross-compile/blob/main/.github/workflows/unix-build.yml\">Unix build</a>\nand one for\n<a href=\"https://github.com/ai-mindset/py-cross-compile/blob/main/.github/workflows/win-build.yml\">Windows</a>.\nThe result is pretty decent, however the cumbersome process and reliance on\nthird party tech (GitHub Actions in this case) strengthened my conviction that\nDeno and TypeScript are worth investing in, for a more complete solution. The\nJS/DS Data[^2] ecosystem is not as mature yet but it&#39;s evolving pretty fast.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The process of cross-compiling a simple Python app was pretty instructive. The\nmain downside I see is the reliance on a hosting service and a CI/CD platform.\nFrankly, having access to a hosting service and using CI/CD is almost a given in\nmy line of work. Still, it&#39;s nowhere near as straightforward as running\n<code>$ deno compile main.ts</code> I am considering attempting the same using\n<a href=\"https://podman.io/\">Podman</a>, since\n<a href=\"https://learn.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/container-base-images\">Windows</a>,\n<a href=\"https://github.com/sickcodes/Docker-OSX\">macOS</a> and\n<a href=\"https://hub.docker.com/_/ubuntu/\">Linux</a> containers are available. Stay tuned\nfor updates!</p>\n<hr>\n<p>[^1]: One might argue that running statically compiled executables in a\n    sandboxed environment is a security risk. Static malware analysis tools\n    exist for this exact reason</p>\n<p>[^2]: To be fair, the Data ecosystem is pretty decent and continuously\n    improving. It&#39;s the ML and statistical ecosystem and specifically the lack\n    of a native Scikit-learn and Scipy-like packages that&#39;s still somewhat\n    lacking</p>\n"
  },
  {
    "title": "💡 TIL: 1.58-bit LLMs Match Full Performance @ 98.6% Energy Reduction",
    "date": "2024-10-30T00:00:00.000Z",
    "tags": [
      "til",
      "llm",
      "performance",
      "energy-reduction"
    ],
    "url": "/posts/TIL-1bitLLM.html",
    "content": "<p><strong>TL;DR:</strong> Ternary-weighted LLMs using only {-1, 0, 1} values (1.58 bits) can\nmatch full-precision performance while delivering dramatic efficiency\nimprovements: 2.71x faster inference, 2.55x lower memory usage, and 71.4x lower\nenergy consumption for matrix operations at 3B parameter scale.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>Back in February 2024, a preprint titled\n<a href=\"https://arxiv.org/abs/2402.17764\">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a>\nwas released. Lots of people picked up on it, simply search for <em>1.58-bits</em> on\nYouTube for instance, however it escaped me due to\n<a href=\"https://xcancel.com/AdamMGrant/status/1851348990589354464\">a busy time at work</a>.\nIt was only when I stumbled across this preprint again recently, that I realised\nwhat a fantastic idea it is to\n<a href=\"https://www.youtube.com/watch?v=wCDGiys-nLA\">substitute multiplication with addition or subtraction</a>.</p>\n<h2 id=\"contributions\">Contributions</h2>\n<p>The TL;DR is that all LLM weights can be ternary i.e. {-1, 0, 1}. Ternary\nweights are 1.58-bits. Activations are 8-bits. This highly quantised model\nmatches full-precision performance at 3B parameter scale.<br>This highly quantised model exhibits 2.71x faster inference, 2.55x lower memory\nusage at 3B scale, 71.4x lower energy consumption for matrix multiplication\noperations. Benefits increase with model scale e.g. 4.1x speed-up at 70B\nparameters, 8.9x higher throughput, 11x larger batch size.</p>\n<h2 id=\"what-does-158-bits-mean\">What Does 1.58-bits Mean?</h2>\n<p>Carnegie Mellon University has a great reference on the\n<a href=\"https://www.cs.cmu.edu/~dst/Tutorials/Info-Theory/\">basics of Information theory</a>.\nLearning how to measure information content for a ternary system {-1, 0, 1}, we\nnotice that:<br>Each value {-1, 0, 1} has an equal probability $P = \\frac{1}{3}$ for each state.\nThe information content is $-(P \\log_2{P})$ summed over all states</p>\n<p>$$\n-(\\frac{1}{3} \\log_2(\\frac{1}{3}) + \\frac{1}{3} \\log_2(\\frac{1}{3}) + \\frac{1}{3} \\log_2(\\frac{1}{3}))\n= -(3 × (\\frac{1}{3} \\log_2(\\frac{1}{3})))\n= -\\log_2(\\frac{1}{3})\n\\approx 1.58496... bits\n$$</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>LLMs can achieve comparable performance to full-precision models while using\nonly three weight values {-1, 0, 1}, achieving up to 71.4x lower energy\nconsumption for matrix operations and 3.55x lower memory usage at 3B scale. This\nbreakthrough suggests a new direction for efficient LLM deployment, particularly\npromising for edge devices and mobile applications, while also opening\nopportunities for specialized hardware optimized for 1-bit operations.</p>\n"
  },
  {
    "title": "🗃️ RAG Is Here To Stay",
    "date": "2024-10-29T00:00:00.000Z",
    "tags": [
      "rag",
      "llm",
      "ai",
      "performance"
    ],
    "url": "/posts/rag-is-here-to-stay.html",
    "content": "<p><strong>TL;DR:</strong> Despite larger LLM context windows, Retrieval-Augmented Generation\n(RAG) remains essential for information curation, data provenance, and\novercoming the &quot;lost in the middle&quot; effect where models struggle with\ninformation placed centrally in long contexts-making careful retrieval\nstrategies more valuable than simply dumping large amounts of text into expanded\ncontext windows.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>This morning I noticed that\n<a href=\"https://xcancel.com/simonw/status/1850928417363149049\">Simon Willison shared some views on RAG</a>,\n<a href=\"https://xcancel.com/burkov/status/1851159933913280647\">Andryi Burkov criticised</a>\npeople who claim that RAG is obsolete, and other RAG-related discussions taking\nplace sparked by recent longer LLM context windows. Below I&#39;m sharing some\nthoughts based on personal experience.</p>\n<h2 id=\"rag\">RAG</h2>\n<p>RAG is not simply a workaround to context limits, it&#39;s a way to carefully curate\ninformation and data. It enables provenance and visibility of the data flowing\nthrough an LLM pipeline -compared to fine-tuning which bakes knowledge into the\nmodel itself. Importantly, RAG is not a synonym of embeddings. Embedding text is\na fantastic way to enable semantic search, especially if it is done in a smart\nway (word, sentence, paragraph, or document) given project needs.<br>I have successfully reused existing infrastructure to provide one of the largest\ncompanies in the world with the ability to quickly retrieve information through\nQ &amp; A. To achieve this, in the context of simplicity and leveraging existing\ninfrastructure, I opted against adding moving parts like a Vector DB. Instead, I\nused plain JSON objects and an agentic system to meet the client&#39;s needs. It\nworked very well, with feedback from higher management being &quot;<em><strong>thank you</strong>,\nthis is mind-blowing</em>&quot;.<br>A nice overview of RAG comes from\n<a href=\"https://www.latent.space/p/llamaindex\">Jerry Liu&#39;s interview on Latent Space</a>.<br><em>Update: a useful open-source tool for\n<a href=\"https://github.com/Brandon-c-tech/RAG-logger\">RAGLogging</a> just came out.</em></p>\n<h2 id=\"u-shaped-performance\">U-Shaped Performance</h2>\n<p>One LLM behaviour that should be considered, before regarding RAG obsolete, is\ntheir tendency to attend to information from the beginning and end of the\ncontext window. See\n<a href=\"https://arxiv.org/abs/2307.03172\">Lost in the Middle: How Language Models Use Long Contexts</a>\nfor an empirical analysis.<br>The paper concludes</p>\n<blockquote>\n<p>We empirically study how language models use long input contexts via a series\nof controlled experiments. We show that language model performance degrades\nsignificantly when changing the position of relevant information, indicating\nthat models struggle to robustly access and use information in long input\ncontexts. In particular, performance is often lowest when models must use\ninformation in the middle of long input contexts.We conduct a preliminary\ninvestigation of the role of (i) model architecture, (ii) query-aware\ncontextualisation, and (iii) instruction fine-tuning to better understand how\nthey affect how language models use context. Finally, we conclude with a\npractical case study of open-domain question answering,finding that the\nperformance of language model readers saturates far before retriever recall.\nOur results and analysis provide a better understanding of how language models\nuse their input context and provides new evaluation protocols for future\nlong-context models.<br>In other words, simply dumping loads of text or embeddings into an LLM with a\nbig context window -say 2M tokens- won&#39;t yield great results. There&#39;s more to\nit than brute forcing.</p>\n</blockquote>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Extending context length, as appealing as it may sound, neither simplifies nor\nsolves the issue of creating a good quality AI system that is enriched by large\ntext corpora. It seems that when it comes to larger data volumes,\n<a href=\"https://www.youtube.com/watch?v=5e1Wzbr8wGU\">semantic search augmented with Graph search</a>\ncould be a more robust, albeit more involved, approach. Solid prompt engineering\napproaches, including\n<a href=\"https://www.promptingguide.ai/techniques/cot\">Chain-of-Thought</a>,\n<a href=\"https://www.promptingguide.ai/techniques/fewshot\">Few-shot prompting</a> etc. are\nalso powerful tools to keep in our toolbox.</p>\n"
  },
  {
    "title": "💡 TIL: Useful Nuggets from AI Engineers",
    "date": "2024-10-25T00:00:00.000Z",
    "tags": [
      "til",
      "ai",
      "llm"
    ],
    "url": "/posts/TIL-useful-AI-nuggets.html",
    "content": "<p><strong>TL;DR:</strong> Leading AI researchers emphasise that success in the field now\ndepends more on engineering skills and adaptability than academic credentials,\nwith the most valuable skills being prioritisation, communication, project\nselection, and willingness to manually inspect data-suggesting that choosing the\nright problem can multiply output by 10-100x more effectively than simply\nworking longer hours.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>As I was going through some\n<a href=\"https://web.stanford.edu/class/cs25/\">CS25: Transformers United V4</a> lectures\nfrom Stanford, I stumbled across some pertinent and useful quotes from guest\nlecturers.</p>\n<h2 id=\"📖\">📖</h2>\n<h3 id=\"best-ai-skillset\"><a href=\"https://xcancel.com/_jasonwei/status/1631017964286922753?cxt=HHwWgsDS-c2MxaItAAAA\">Best AI skillset</a></h3>\n<blockquote>\n<p>Best AI skillset in 2018: PhD + long publication record in a specific area<br>Best AI skillset in 2023: strong engineering abilities + adapting quickly to\nnew directions without sunk cost fallacy</p>\n</blockquote>\n<h3 id=\"advice-on-choosing-a-topic\"><a href=\"https://xcancel.com/_jasonwei/status/1514327894746574851\">Advice on choosing a topic</a></h3>\n<blockquote>\n<p>[...] the project you choose defines the upper-bound for your success.</p>\n</blockquote>\n<h3 id=\"study-the-change-itself\"><a href=\"https://docs.google.com/presentation/d/1u05yQQaw4QXLVYGLI6o3YoFHv6eC3YN8GvWD8JMumpE\">Study the change itself</a></h3>\n<blockquote>\n<p>AI is advancing so fast it is hard to keep up. People spend a lot of time and\nenergy catching up with the latest developments. But not enough attention goes\nto the old things. It is more important to <em>study the change itself</em></p>\n</blockquote>\n<h3 id=\"why-im-100-transparent-with-my-manager\"><a href=\"https://xcancel.com/_jasonwei/status/1699860824053911558\">Why I&#39;m 100% transparent with my manager</a></h3>\n<blockquote>\n<p>I try to open this [performance] conversation [with my line manager] by asking\n&quot;what can I do better&quot;.<br>I tend to use my 1-1s to talk about bigger picture stuff. [...] since that&#39;s\nwhere managers can help the most. Of course, all this [honesty with your\nmanager] going well is conditional on working in a healthy company and having\na decent manager. [...] do you want to keep working for someone who doesn&#39;t\nask for feedback, or who doesn&#39;t take your problems seriously?</p>\n</blockquote>\n<h3 id=\"my-strengths-are-communication-and-prioritization\"><a href=\"https://xcancel.com/_jasonwei/status/1689346627428036608\">My strengths are communication and prioritization</a></h3>\n<blockquote>\n<p>[...] a friend recently asked me what were the best skills I had. [...] I said\nprioritization and communication. These skills are relatively general but\nhappen to be very important for AI research.</p>\n</blockquote>\n<h3 id=\"many-great-managers-do-ic-individual-contributor-work\"><a href=\"https://xcancel.com/_jasonwei/status/1701665241652945283\">Many great managers do IC (Individual Contributor) work</a></h3>\n<blockquote>\n<p>It seems to be not a coincidence that some of the strongest leaders in AI who\nmanage large teams frequently do very low-level technical work.</p>\n</blockquote>\n<h3 id=\"manually-inspect-data\"><a href=\"https://xcancel.com/_jasonwei/status/1708921475829481683\">Manually inspect data</a></h3>\n<blockquote>\n<p>[...] great AI researchers are willing to manually inspect lots of data. And\nmore than that, they build infrastructure that allows them to manually inspect\ndata quickly. Though not glamorous, manually examining data gives valuable\nintuitions about the problem.</p>\n</blockquote>\n<h3 id=\"read-informal-write-ups\"><a href=\"https://xcancel.com/_jasonwei/status/1731780538405716078\">Read informal write-ups</a></h3>\n<blockquote>\n<p>[...] I like to look at the process of how they [great researchers] got there.</p>\n</blockquote>\n<h3 id=\"advice-from-bryan-johnson\"><a href=\"https://xcancel.com/_jasonwei/status/1766692847078756557\">Advice from Bryan Johnson</a></h3>\n<blockquote>\n<p>[...] having good health enables clear thinking, which is by far the biggest\nleverage in AI. [...] While it&#39;s possible to double output by working twice as\nmany hours, choosing a better project has the potential to 10x or even 100x\noutput.</p>\n</blockquote>\n"
  },
  {
    "title": "🔁 GitHub Actions for yt-dlp-hq",
    "date": "2024-10-08T00:00:00.000Z",
    "tags": [
      "github-actions",
      "ci-cd",
      "yt-dlp",
      "deno",
      "typescript",
      "cross-platform"
    ],
    "url": "/posts/gh-actions-ytdlphq.html",
    "content": "<p><strong>TL;DR:</strong> This article details the development of a Deno-based tool for\ndownloading high-quality videos with audio using yt-dlp, highlighting unexpected\nchallenges with GitHub Actions where compiled executables became unusable after\nrelease-ultimately solved by compressing executables into .tar files, preserving\nfunctionality whilst revealing potential limitations in GitHub&#39;s release\nmechanisms.</p>\n<!--more-->\n\n<p><strong><a href=\"https://xkcd.com/1205/\">Was it worth my time</a>?</strong></p>\n<h2 id=\"introduction\">Introduction</h2>\n<p>There are times where I need to use my computer offline, e.g. when I&#39;m\ntravelling. Having to stay offline is a good opportunity for me to study some\nlectures of interest, without distractions. For that, I need offline access to\nthe videos I&#39;m interested in.<br><a href=\"https://github.com/yt-dlp/yt-dlp\">yt-dlp</a> is a great open-source project that\nallows the user to download audio and/or video from a wide array of platforms\nincluding YouTube. Recently, I noticed that it&#39;s no longer as straightforward to\ndownload a video with audio, using <code>yt-dlp</code>. One workaround is to download the\naudio and video streams separately, and merge them using\n<a href=\"https://ffmpeg.org/\">FFmpeg</a>. This was a good opportunity to write a small\nautomation project in a language I&#39;m interested in.</p>\n<h3 id=\"yt-dlp-and-youtube\"><code>yt-dlp</code> And YouTube</h3>\n<p>Here&#39;s an example that motivates implementing this project. Imagine I&#39;d like to\ndownload a video from the excellent\n<a href=\"https://www.youtube.com/channel/UCKWaEZ-_VweaEx1j62do_vQ\">IBM Technology</a>\nYouTube channel, for instance\n<a href=\"https://www.youtube.com/watch?v=F8NKVhkZZWI\">What are AI Agents</a>. Listing the\nvideo&#39;s available formats, returns the following table</p>\n<pre><code class=\"language-console\">$ yt-dlp -F https://www.youtube.com/watch\\?v\\=F8NKVhkZZWI\n[youtube] Extracting URL: https://www.youtube.com/watch?v=F8NKVhkZZWI\n[youtube] F8NKVhkZZWI: Downloading webpage\n[youtube] F8NKVhkZZWI: Downloading ios player API JSON\n[youtube] F8NKVhkZZWI: Downloading web creator player API JSON\n[youtube] F8NKVhkZZWI: Downloading m3u8 information\n[info] Available formats for F8NKVhkZZWI:\nID      EXT   RESOLUTION FPS CH │   FILESIZE   TBR PROTO │ VCODEC          VBR ACODEC      ABR ASR MORE INFO\n─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\nsb3     mhtml 48x27        0    │                  mhtml │ images                                  storyboard\nsb2     mhtml 80x45        0    │                  mhtml │ images                                  storyboard\nsb1     mhtml 160x90       0    │                  mhtml │ images                                  storyboard\nsb0     mhtml 320x180      0    │                  mhtml │ images                                  storyboard\n233     mp4   audio only        │                  m3u8  │ audio only          unknown             [en] Default\n234     mp4   audio only        │                  m3u8  │ audio only          unknown             [en] Default\n139-drc m4a   audio only      2 │    4.35MiB   49k https │ audio only          mp4a.40.5   49k 22k [en] low, DRC, m4a_dash\n139     m4a   audio only      2 │    4.35MiB   49k https │ audio only          mp4a.40.5   49k 22k [en] low, m4a_dash\n249     webm  audio only      2 │    4.37MiB   49k https │ audio only          opus        49k 48k [en] low, webm_dash\n250     webm  audio only      2 │    5.27MiB   59k https │ audio only          opus        59k 48k [en] low, webm_dash\n140-drc m4a   audio only      2 │   11.55MiB  129k https │ audio only          mp4a.40.2  129k 44k [en] medium, DRC, m4a_dash\n140     m4a   audio only      2 │   11.55MiB  129k https │ audio only          mp4a.40.2  129k 44k [en] medium, m4a_dash\n251     webm  audio only      2 │    9.45MiB  106k https │ audio only          opus       106k 48k [en] medium, webm_dash\n602     mp4   256x144     15    │ ~  7.26MiB   81k m3u8  │ vp09.00.10.08   81k video only\n394     mp4   256x144     30    │    4.36MiB   49k https │ av01.0.00M.08   49k video only          144p, mp4_dash\n269     mp4   256x144     30    │ ~ 11.26MiB  126k m3u8  │ avc1.4D400C    126k video only\n160     mp4   256x144     30    │    2.97MiB   33k https │ avc1.4D400C     33k video only          144p, mp4_dash\n603     mp4   256x144     30    │ ~ 13.63MiB  153k m3u8  │ vp09.00.11.08  153k video only\n278     webm  256x144     30    │    7.23MiB   81k https │ vp09.00.11.08   81k video only          144p, webm_dash\n395     mp4   426x240     30    │    5.90MiB   66k https │ av01.0.00M.08   66k video only          240p, mp4_dash\n229     mp4   426x240     30    │ ~ 14.99MiB  168k m3u8  │ avc1.4D4015    168k video only\n133     mp4   426x240     30    │    4.49MiB   50k https │ avc1.4D4015     50k video only          240p, mp4_dash\n604     mp4   426x240     30    │ ~ 21.46MiB  241k m3u8  │ vp09.00.20.08  241k video only\n242     webm  426x240     30    │    7.38MiB   83k https │ vp09.00.20.08   83k video only          240p, webm_dash\n396     mp4   640x360     30    │   10.27MiB  115k https │ av01.0.01M.08  115k video only          360p, mp4_dash\n230     mp4   640x360     30    │ ~ 29.86MiB  335k m3u8  │ avc1.4D401E    335k video only\n134     mp4   640x360     30    │    7.76MiB   87k https │ avc1.4D401E     87k video only          360p, mp4_dash\n18      mp4   640x360     30  2 │   32.38MiB  363k https │ avc1.42001E         mp4a.40.2       44k [en] 360p\n605     mp4   640x360     30    │ ~ 39.56MiB  444k m3u8  │ vp09.00.21.08  444k video only\n243     webm  640x360     30    │   12.52MiB  140k https │ vp09.00.21.08  140k video only          360p, webm_dash\n397     mp4   854x480     30    │   17.06MiB  191k https │ av01.0.04M.08  191k video only          480p, mp4_dash\n231     mp4   854x480     30    │ ~ 37.90MiB  425k m3u8  │ avc1.4D401F    425k video only\n135     mp4   854x480     30    │   11.28MiB  126k https │ avc1.4D401F    126k video only          480p, mp4_dash\n606     mp4   854x480     30    │ ~ 50.26MiB  564k m3u8  │ vp09.00.30.08  564k video only\n244     webm  854x480     30    │   17.41MiB  195k https │ vp09.00.30.08  195k video only          480p, webm_dash\n398     mp4   1280x720    30    │   31.31MiB  351k https │ av01.0.05M.08  351k video only          720p, mp4_dash\n232     mp4   1280x720    30    │ ~ 57.85MiB  649k m3u8  │ avc1.4D401F    649k video only\n136     mp4   1280x720    30    │   20.16MiB  226k https │ avc1.4D401F    226k video only          720p, mp4_dash\n609     mp4   1280x720    30    │ ~ 72.26MiB  810k m3u8  │ vp09.00.31.08  810k video only\n247     webm  1280x720    30    │   27.69MiB  310k https │ vp09.00.31.08  310k video only          720p, webm_dash\n399     mp4   1920x1080   30    │   63.06MiB  707k https │ av01.0.08M.08  707k video only          1080p, mp4_dash\n270     mp4   1920x1080   30    │ ~193.27MiB 2167k m3u8  │ avc1.640028   2167k video only\n137     mp4   1920x1080   30    │   96.17MiB 1078k https │ avc1.640028   1078k video only          1080p, mp4_dash\n614     mp4   1920x1080   30    │ ~164.68MiB 1847k m3u8  │ vp09.00.40.08 1847k video only\n248     webm  1920x1080   30    │   91.43MiB 1025k https │ vp09.00.40.08 1025k video only          1080p, webm_dash\n616     mp4   1920x1080   30    │ ~322.53MiB 3617k m3u8  │ vp09.00.40.08 3617k video only          Premium\n</code></pre>\n<p>It looks like the only ID containing a video <em>with</em> audio, is <code>18</code>, i.e. a 420p,\n640x360 video according to my media player. This might be sufficient for a video\nlike the above, but such low resolution would make it almost impossible to read\ncode or smaller writing.</p>\n<h2 id=\"my-solution\">My Solution</h2>\n<p>Given I have started leveraging Deno for my needs, I wrote a small tool called\n<a href=\"https://github.com/ai-mindset/yt-dlp-hq\">yt-dlp-hq</a>. It&#39;s certainly basic, with\nlots of room for improvement. However it does exactly what I need and I&#39;m\nrelatively happy with the result, pending some improvements[^1].<br>Deno is great for cross-compilation. Also, GitHub Actions can be a good method\nfor automating testing, running, compiling etc.[^2] Is it though? Let&#39;s see.</p>\n<h2 id=\"my-ci-pipeline\">My CI Pipeline</h2>\n<p>I started off by using <a href=\"https://nektosact.com/introduction.html\">act</a>, a very\nnice tool that allows for testing pipelines locally. The main downside I found\nwas that for an intermediate Docker user with little <code>act</code> experience, sometimes\nGitHub Actions don&#39;t behave the same way locally as they would online. Also, I\nlike <a href=\"https://podman.io/\">podman</a> considerably better, since it&#39;s daemonless and\nnot as resource-hungry among others.<br>Putting <code>act</code> aside, I focused on setting up a\n<a href=\"https://github.com/ai-mindset/yt-dlp-hq/blob/main/.github/workflows/ci.yml\">pipeline</a>\nthat&#39;d work well enough with every new PR opened against <code>main</code> aside from\nothers.<br>The pipeline ran successfully, where in theory it built and released <code>yt-dlp-hq</code>\nexecutables. However, when I downloaded the corresponding executable for my OS\nand CPU architecture, it did not run. When I locally built the same set of\nexecutables, running <code>deno task build</code>, the executable for my OS &amp; arch worked\nas expected. This made me wonder whether I&#39;m doing something wrong, if it&#39;s a\nGitHub Action intricacy or some other issue I needed to resolve. Inspired by\nMedicine, I tried approaching the issue through differential diagnosis, which to\nmy understanding works by excluding other causes in order to hone in on the\nactual medical condition. I.e. I first created a release directory locally. I\nthen manually created a release on GitHub. To my dismay, the executable I\nmanually uploaded didn&#39;t run when I downloaded it back from GitHub. This made me\nwonder if there is a conversion involved when a pipeline generates executables\nor the user uploads them manually for release. Spoiler alert: I still don&#39;t know\nif that&#39;s the case, but I suspect that GitHub indeed doesn&#39;t save executables\nwithout some change taking place during upload.</p>\n<h3 id=\"fixing-executables-github-release\">Fixing Executables GitHub Release</h3>\n<p>Initially, I changed the following setting on my repository:<br><em>&quot;Settings -&gt; Actions -&gt; General -&gt; Workflow permissions&quot;</em> select <em>&quot;Read and\nwrite permissions&quot;</em>. Then, I experimented with compressing each generated\nexecutable into a .tar file. This did the trick. Simply compressing an\nexecutable is enough to maintain its function. Thus, the way to install\n<code>yt-dlp-hq</code> takes one extra step.<br>For example, if you&#39;re a Linux user on an Intel-based machine, here&#39;s how you\ncan use my tool</p>\n<pre><code class=\"language-console\">$ curl -L -O https://github.com/ai-mindset/yt-dlp-hq/releases/download/1.0.0/yt-dlp-hq-intel-linux.tar &amp;&amp; tar xvf yt-dlp-hq-intel-linux.tar &amp;&amp; cd release\n$ ./yt-dlp-hq-intel-linux https://www.youtube.com/watch?v=dQw4w9WgXcQ\n</code></pre>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>I&#39;m glad I learned something more about GitHub and Actions, its idiosyncrasies\nand abilities. It took me a couple days, which made me consider the benefits of\n<a href=\"https://xkcd.com/1319/\">automation</a>. Being more minimalistic, I tend to opt for\nsimple automation when possible <a href=\"https://xkcd.com/1205/\"><em>if</em> it&#39;s worth it</a>. To\nquote <a href=\"https://en.wikiquote.org/wiki/Alan_Perlis\">Alan Perlis</a>, &quot;<em>Simplicity\ndoes not precede complexity, but follows it</em>&quot;.</p>\n<hr>\n<p>[^1]: Some improvements I&#39;m planning include unit testing, automatic audio &amp;\n    video ID selection and possibly automatic FFmpeg installation when it&#39;s not\n    available in <code>$PATH</code>.</p>\n<p>[^2]: A <a href=\"https://julialang.org/\">Juiia</a> enthusiast introduced me to\n    <a href=\"https://woodpecker-ci.org/\">Woodpecker CI</a> and\n    <a href=\"https://codeberg.org/\">Codeberg</a>. I&#39;m definitely considering switching,\n    following my recent GitHub Actions experience 🤔</p>\n"
  },
  {
    "title": "📖 Python To TypeScript Cheatsheet",
    "date": "2024-09-06T00:00:00.000Z",
    "tags": [
      "python",
      "typescript",
      "cheatsheet"
    ],
    "url": "/posts/python-typescript-cheatsheet.html",
    "content": "<p><strong>TL;DR:</strong> This compact reference guide provides side-by-side comparisons of\nPython and TypeScript syntax for common programming constructs including\nvariable declarations, functions, classes, control flow structures, and error\nhandling-serving as a quick reference for Python developers exploring TypeScript\nwithin the context of Deno development.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>I&#39;ve been curious as to how Python and TypeScript compare at a high level, for\nsomeone new to TypeScript. Below is a chatsheet I put together with the help of\nClaude 3.5 Sonnet. It covers basic syntax, it&#39;s by no means complete or\nexhaustive. However it gives a first taste of the similarities and differences\nbetween the two languages. The reason I am looking into TypeScript is explained\nin my <a href=\"../deno/\">Deno article</a>.</p>\n<h2 id=\"variables-and-data-types\">Variables And Data Types</h2>\n<table>\n<thead>\n<tr>\n<th>Concept</th>\n<th>Python</th>\n<th>TypeScript</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Int</td>\n<td><code>x = 5</code></td>\n<td><code>let x: number = 5;</code></td>\n</tr>\n<tr>\n<td>Float</td>\n<td><code>y = 3.14</code></td>\n<td><code>let y: number = 3.14;</code></td>\n</tr>\n<tr>\n<td>Str</td>\n<td><code>name = &quot;John&quot;</code></td>\n<td><code>let name: string = &quot;John&quot;;</code></td>\n</tr>\n<tr>\n<td>Bool</td>\n<td><code>is_valid = True</code></td>\n<td><code>let isValid: boolean = true;</code></td>\n</tr>\n<tr>\n<td>List</td>\n<td><code>numbers = [1, 2, 3]</code></td>\n<td><code>let numbers: number[] = [1, 2, 3];</code></td>\n</tr>\n<tr>\n<td>Dict</td>\n<td><code>person = {&quot;name&quot;: &quot;John&quot;, &quot;age&quot;: 30}</code></td>\n<td><code>let person: { name: string; age: number } = { name: &quot;John&quot;, age: 30 };</code></td>\n</tr>\n</tbody></table>\n<h2 id=\"functions\">Functions</h2>\n<table>\n<thead>\n<tr>\n<th>Concept</th>\n<th>Python</th>\n<th>TypeScript</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Func def</td>\n<td><code>def greet(name: str) -&gt; str:</code></td>\n<td><code>function greet(name: string): string {</code></td>\n</tr>\n<tr>\n<td>Func return</td>\n<td><code>return f&quot;Hello, {name}!&quot;</code></td>\n<td><code>return `Hello, ${name}!`;</code></td>\n</tr>\n<tr>\n<td>Lambda</td>\n<td><code>lambda x: x * 2</code></td>\n<td><code>(x: number): number =&gt; x * 2</code></td>\n</tr>\n</tbody></table>\n<h2 id=\"classes\">Classes</h2>\n<table>\n<thead>\n<tr>\n<th>Concept</th>\n<th>Python</th>\n<th>TypeScript</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Class def</td>\n<td><code>class Person:</code></td>\n<td><code>class Person {</code></td>\n</tr>\n<tr>\n<td>Constructor</td>\n<td><code>def __init__(self, name: str, age: int):</code></td>\n<td><code>constructor(name: string, age: number) {</code></td>\n</tr>\n<tr>\n<td>Instance vars</td>\n<td><code>self.name = name</code></td>\n<td><code>this.name = name;</code></td>\n</tr>\n<tr>\n<td></td>\n<td><code>self.age = age</code></td>\n<td><code>this.age = age;</code></td>\n</tr>\n<tr>\n<td>Method def</td>\n<td><code>def greet(self) -&gt; str:</code></td>\n<td><code>greet(): string {</code></td>\n</tr>\n<tr>\n<td>Method return</td>\n<td><code>return f&quot;Hello, I&#39;m {self.name}!&quot;</code></td>\n<td><code>return `Hello, I&#39;m ${this.name}!`;</code></td>\n</tr>\n</tbody></table>\n<h2 id=\"control-flow\">Control Flow</h2>\n<table>\n<thead>\n<tr>\n<th>Concept</th>\n<th>Python</th>\n<th>TypeScript</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>If</td>\n<td><code>if x &gt; 0:</code></td>\n<td><code>if (x &gt; 0) {</code></td>\n</tr>\n<tr>\n<td>Else if</td>\n<td><code>elif x &lt; 0:</code></td>\n<td><code>} else if (x &lt; 0) {</code></td>\n</tr>\n<tr>\n<td>Else</td>\n<td><code>else:</code></td>\n<td><code>} else {</code></td>\n</tr>\n<tr>\n<td>For loop</td>\n<td><code>for i in range(5):</code></td>\n<td><code>for (let i = 0; i &lt; 5; i++) {</code></td>\n</tr>\n<tr>\n<td>While loop</td>\n<td><code>while x &gt; 0:</code></td>\n<td><code>while (x &gt; 0) {</code></td>\n</tr>\n</tbody></table>\n<h2 id=\"error-handling\">Error Handling</h2>\n<table>\n<thead>\n<tr>\n<th>Concept</th>\n<th>Python</th>\n<th>TypeScript</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Try</td>\n<td><code>try:</code></td>\n<td><code>try {</code></td>\n</tr>\n<tr>\n<td>Except/Catch</td>\n<td><code>except ZeroDivisionError:</code></td>\n<td><code>} catch (error) {</code></td>\n</tr>\n<tr>\n<td>Finally</td>\n<td><code>finally:</code></td>\n<td><code>} finally {</code></td>\n</tr>\n</tbody></table>\n"
  },
  {
    "title": "🏗️ Modern Data Science and AI Engineering with Deno 2.0",
    "date": "2024-09-05T00:00:00.000Z",
    "tags": [
      "ai",
      "llm",
      "cross-platform",
      "data-processing",
      "data-science",
      "deno",
      "machine-learning",
      "minimal",
      "polars",
      "production",
      "deployment",
      "toolchain",
      "typescript",
      "security",
      "zero-config"
    ],
    "url": "/posts/deno.html",
    "content": "<p><strong>TL;DR:</strong> Deno 2.0 offers a compelling alternative to Python for AI and data\nscience workflows by providing zero-configuration TypeScript support, native\nsecurity features, cross-compilation capabilities, and an ecosystem of essential\ntools-addressing Python&#39;s environment management complexities and deployment\nfrictions whilst enabling production-ready development from proof-of-concept\nthrough to single-binary distribution.</p>\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>The landscape of Data Science and AI engineering is at a critical inflection\npoint. While Python has dominated data science and machine learning, its\nfragmented ecosystem and deployment complexities increasingly impede production\nsystems. I&#39;ve already touched on [my solution to Python&#39;s fragmentation]({{\nsite.baseurl }}{% link _posts/2024-11-21-bring-it-back-to-basics.md %}).<br>Deno 2.0 emerges as a compelling solution to these challenges, bringing together\na number of technologies that cover most computing requirements across a very\nwide range of domains. Having said that, JavaScript (JS) and its superset\nTypeScript (TS) are\n<a href=\"https://www.youtube.com/watch?v=aXOChLn5ZdQ\">far from perfect languages</a>[^1],\nbut this discussion is outside the scope of this blog post.</p>\n<p>The key factors driving change are:</p>\n<ul>\n<li>Python environment management complexity</li>\n<li>Production security requirements</li>\n<li>Deployment workflow friction</li>\n<li>Need for type safety in large-scale AI applications</li>\n</ul>\n<h2 id=\"the-deno-advantage-and-ecosystem\">The Deno Advantage and Ecosystem</h2>\n<h3 id=\"core-capabilities\">Core Capabilities</h3>\n<p>Deno 2.0 provides a comprehensive, zero-configuration solution with:</p>\n<ul>\n<li>Native TS support</li>\n<li>First-class security features</li>\n<li>Cross-compilation through <code>deno compile</code></li>\n<li>Built-in development tools</li>\n</ul>\n<p>As Ryan Dahl emphasized in a recent\n<a href=\"https://www.youtube.com/watch?v=tZBCq8Ijkgw\">Syntax podcast episode</a>: &quot;<em>Deno\nworks really great as a single file. It&#39;s really great for scripting, [...] you\ncan just put some imports in and start working from a single file. And that is\nactually exactly what you want from notebooks</em>&quot;. This aligns with recent work by\n<a href=\"https://www.answer.ai/\">Answer.AI</a>&#39;s\n<a href=\"https://www.alexisgallagher.com/\">Alexis Gallagher</a> on\n<a href=\"https://youtube.com/watch?v=t6-Uup-Alfs\">single-script Python development</a>.</p>\n<h3 id=\"ai-and-data-processing-tools\">AI and Data Processing Tools</h3>\n<p>The ecosystem provides direct parallels to Python&#39;s essential tools:</p>\n<p>Data Processing:</p>\n<ul>\n<li><a href=\"https://pola-rs.github.io/nodejs-polars/\">nodejs-polars</a> for high-performance\nDataFrame operations</li>\n<li><a href=\"https://observablehq.com/plot/\">Observable Plot</a> for modern visualisation</li>\n</ul>\n<p>Machine Learning:</p>\n<ul>\n<li><a href=\"https://js.langchain.com/\">LangChain.js</a> and\n<a href=\"https://ts.llamaindex.ai/\">LlamaIndex.ts</a> for LLM applications</li>\n<li><a href=\"https://huggingface.co/docs/transformers.js/index\">Transformers.js</a> for\nHugging Face integration</li>\n<li><a href=\"https://www.tensorflow.org/js\">TensorFlow.js</a> and\n<a href=\"https://github.com/nuanio/xgboost-node\">XGBoost-node</a> for ML tasks</li>\n</ul>\n<p>Infrastructure:</p>\n<ul>\n<li>Native\n<a href=\"https://docs.deno.com/runtime/manual/basics/connecting_to_databases/#postgres\">Postgres</a>\nand\n<a href=\"https://docs.deno.com/runtime/manual/basics/connecting_to_databases/#mongodb\">MongoDB</a>\nsupport</li>\n<li><a href=\"https://github.com/qdrant/qdrant-js\">Qdrant JS</a> for vector storage</li>\n<li><a href=\"https://stdlib.io/docs/api/latest\">STDLib</a> for extended functionality</li>\n<li><a href=\"https://github.com/axa-group/nlp.js/\">NLP.js</a> and\n<a href=\"https://github.com/spencermountain/compromise\">compromise</a> for NLP</li>\n</ul>\n<h2 id=\"a-pragmatic-decision-framework\">A Pragmatic Decision Framework</h2>\n<p>Modern AI systems need to balance rapid experimentation with production-ready\nstability. Through experimentation, I&#39;ve found that a TS-based approach using\nDeno provides an elegant solution to both needs.</p>\n<h3 id=\"production-first-design\">Production-First Design</h3>\n<p>For a start, a zero-configuration Deno-based environment makes it easy to\nproduce code spanning proof of concept (POC) to production. This gives the user\nnative security, cross-compilation capabilities and simple single-binary\ndistribution, eliminating many traditional deployment headaches. While Python\nremains popular for Data Science and AI research, Deno with simple TS[^2] has\nbeen able to handle most of my computational equally well, in a lightweight and\nproductive way.</p>\n<h3 id=\"practical-implementation-guide\">Practical Implementation Guide</h3>\n<p>Transitioning to this kind of all-in-one Deno-driven architecture can start by\nutilising tools like LangChain.js or LlamaIndex.ts for LLM applications. Data\nprocessing can be handled efficiently through nodejs-polars, while Observable\nPlots provides powerful visualisation.<br>Emphasising simplicity, we can use REST/GraphQL to handle service communication,\nwith shared data stores and container-based deployment maintaining clear service\nboundaries. This approach supports both monolithic and microservice\narchitectures, based on project needs.</p>\n<h3 id=\"development-best-practices\">Development Best Practices</h3>\n<p>[Iterative refinement development]({{ site.baseurl }}{% link\n_posts/2024-11-22-iterative-refinement.md %}) remains an equally productive\napproach. Strong typing helps with development and code robustness, while\ncorrectly used async/await patterns ensure system responsiveness. This approach\nenables rapid prototyping without sacrificing production readiness.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Leveraging Deno with TS as a replacement for Python is a possible, viable and\nusually more lightweight alternative for developing more maintainable, secure\nand production-ready Data and AI systems. Deno&#39;s zero-config setup, extensive\ntooling, security focus and stability address key pain points I have encountered\nin my Python development journey.<br>The Deno ecosystem has reached maturity, making it a viable and often superior\nalternative -in my experience- to traditional Python-based approaches for modern\nAI engineering workflows.</p>\n<hr>\n<p>[^1]: There are noteworthy -sadly not as widely used- languages such as\n    <a href=\"https://clojure.org/\">Clojure</a> and <a href=\"https://racket-lang.org/\">Racket</a>,\n    backed by\n    <a href=\"https://en.wikipedia.org/wiki/Lisp_(programming_language)\">computer science research</a>,\n    that pioneered concepts like iterative refinement (aka REPL-driven\n    development) among others.</p>\n<p>[^2]: &quot;simple&quot; in this context refers to leveraging types but avoiding more\n    involved TypeScript ideas.</p>\n"
  }
]